# 数据处理流程说明（小白版）

## 📚 总体概述

你的数据处理就像是把一堆乱七八糟的原料（原始数据），经过一系列清洗、整理、组合，最终做成一道可以上桌的菜（模型可以直接用的干净数据）。

整个过程分为4个步骤，每个步骤对应一个Python文件。

---

## 🔍 步骤一：etl.py - 把散乱的原料整理成半成品

### 这个文件做了什么？

想象一下，你有：
- **几千个股票价格文件**：每个股票的每天价格数据都在单独的文件里
- **一个23GB的超级大新闻文件**：里面包含了所有股票的新闻，但混在一起

`etl.py` 的任务就是：
1. **合并股价文件**：把几千个分散的文件合并成一个统一的大文件
2. **提取有用新闻**：从那23GB的大文件中，只提取你需要的时间范围和股票代码的新闻

### 具体怎么做的？

#### 任务1：合并股价数据
```
1. 扫描目录下所有的CSV文件（比如 AAPL.csv, MSFT.csv 等等）
2. 对每个文件：
   - 统一列名大小写（有些文件是 Date，有些是 date，统一成 Date）
   - 从文件名提取股票代码（AAPL.csv → Ticker = "AAPL"）
   - 只保留2019-2023年的数据（过滤掉其他年份）
   - 只保留标准列（Date, Ticker, Open, Close, High, Low, Volume）
3. 把所有处理好的数据合并成一个文件：Stock_Prices.csv
```

**结果**：一个包含所有股票价格的统一文件

#### 任务2：清洗新闻数据
```
23GB的文件太大了，一次性读入内存会爆炸！
所以采用"分块读取"的方式：
1. 每次只读10万行（而不是全部读入）
2. 对每一块：
   - 只保留2019-2023年的新闻
   - 只保留有对应股价数据的股票新闻（如果一个股票没有价格数据，它的新闻也不要了）
   - 只保留关键信息（Date, Ticker, Headline, Publisher）- 不要全文内容，节省空间
3. 把所有处理好的块合并成一个文件：Stock_News.csv
```

**结果**：一个清洗后的新闻文件，只包含有用信息

---

## 📊 步骤二：align.py - 把半成品组合成完整菜品

### 这个文件做了什么？

现在你有三个半成品：
- **股价数据**（Stock_Prices.csv）
- **大盘指数数据**（SP500_Index.csv）- 整个市场的表现
- **新闻数据**（Stock_News.csv）

`align.py` 的任务是：
1. **把它们合并在一起**：把股价、大盘、新闻按日期和股票代码对齐
2. **处理缺失值**：如果某天没有新闻，就填空字符串；如果某天没有大盘数据，就用前一天的填充
3. **计算波动率**：用数学公式计算每个股票的价格波动程度

---

### 💡 为什么要计算波动率？（重要！）

**波动率是什么？**
波动率就像是股票的"心跳率"——它告诉你这只股票的价格跳得有多厉害。

**举个生活例子**：
- 正常心跳：60-100次/分钟 → 正常波动率
- 剧烈运动时：120-150次/分钟 → 高波动率（市场动荡时）
- 睡觉时：40-60次/分钟 → 低波动率（市场平静时）

**在股票中**：
- **低波动率** = 价格变化很平稳，今天涨1%，明天跌0.5%，后天涨0.8%（波动小）
- **高波动率** = 价格变化很剧烈，今天涨10%，明天跌8%，后天涨15%（波动大）

**为什么要计算它？两个重要目的：**

#### 目的1：作为模型的一个特征（让模型"看"得更清楚）

就像医生看病要量体温、血压、心跳一样，模型预测股价也需要"看"波动率这个指标。

**为什么有用？**
- 如果一个股票最近波动很大，可能说明有重要消息（比如公司发布重大公告）
- 波动率高的股票，未来可能继续波动；波动率低的股票，可能继续保持平稳
- 模型可以学习到：**"哦，这个股票最近波动率很高，我需要更加关注它"**

#### 目的2：决定用哪种"算法"处理（你的模型的特殊功能）

你的模型有一个特殊的设计：**量子-经典混合架构**。

简单理解就是：
- **经典算法** = 普通电脑算法（速度快，适合处理常规情况）
- **量子算法** = 特殊算法（更复杂，适合处理复杂情况）

**波动率在这里的作用就像"智能开关"**：

```
如果波动率低（市场平静）→ 用经典算法处理（够用了，速度快）
如果波动率高（市场动荡）→ 用量子算法处理（需要更强的计算能力）
```

**为什么这样设计？**
- 平静的市场 → 情况比较简单 → 用普通算法就够了
- 动荡的市场 → 情况很复杂 → 需要量子算法才能"理解"清楚

这就像：
- **走路**（低波动）→ 用普通导航就够了
- **开F1赛车**（高波动）→ 需要专业的赛车导航系统

**你的代码中是这样实现的：**
```python
# 如果波动率 > 某个阈值
if 波动率 > 阈值:
    使用量子算法处理  # 处理复杂的、高风险的样本
else:
    使用经典算法处理  # 处理普通的、低风险的样本
```

**总结**：波动率不仅告诉模型"市场现在有多乱"，还告诉模型"应该用多强的算法来处理这个样本"。

### 具体怎么做的？

```
1. 读取三个数据文件
2. 新闻数据预处理：
   - 如果同一天同一个股票有多条新闻，把它们合并成一条（用 | 分隔）
3. 数据合并（就像Excel的VLOOKUP）：
   - 以股价数据为主表
   - 把大盘数据按日期合并进来
   - 把新闻数据按日期和股票代码合并进来
4. 缺失值处理：
   - 大盘数据缺失 → 用前一天的值填充（前向填充）
   - 新闻缺失 → 填空字符串 ""
5. 计算波动率：
   - 计算每日的对数收益率：log(今天收盘价/昨天收盘价)
   - 计算20天滚动波动率：用过去20天的收益率标准差来衡量波动
```

**结果**：一个完整的数据文件 `Final_Model_Data.csv`，包含：
- 股价信息（Open, Close, High, Low, Volume）
- 大盘信息（Market_Close, Market_Vol）
- 新闻信息（News_Text）
- 计算的指标（Log_Ret, Volatility_20d）

---

## 🛡️ 步骤三：dataset.py - 最后的安全检查和标准化（训练时自动调用）

### 这个文件做了什么？

`dataset.py` 是**数据集类**，在运行 `train_gnn.py` 时会被自动调用，不用单独运行。它主要做两件事：
1. **安全检查**：找出并处理异常值（比如收益率超过100%的明显错误数据）
2. **标准化**：把不同特征的数值范围统一（就像把所有温度都转换成摄氏度）

### 具体怎么做的？

```
1. 数据清洗（安全检查）：
   - 把收益率限制在 -100% 到 +100% 之间（超出这个范围肯定是错误数据）
   - 删除无穷大值（Inf）和缺失值（NaN）
   - 限制波动率的最大值（防止计算溢出）

2. 划分训练集和测试集：
   - 前80%的时间作为训练集（模型学习用）
   - 后20%的时间作为测试集（检验模型效果用）

3. 标准化：
   - 使用 StandardScaler 把每个特征都转换成"均值为0，标准差为1"的分布
   - 训练集：计算均值和标准差，然后转换
   - 测试集：使用训练集的均值和标准差（不能自己算！）

4. 构建滑动窗口：
   - 假设用过去30天的数据预测未来1天
   - 对每个股票，生成所有可能的30天窗口
   - 确保不跨股票（一个窗口里的数据必须来自同一只股票）
```

**结果**：可以直接喂给模型训练的数据集

---

## 📊 步骤四：build_graph.py - 从新闻构建公司关系图（LLM 动态图谱）

### 这个文件做了什么？

从 `Stock_News.csv` 里提取公司之间的关系（供应、竞争、合作等），生成邻接矩阵 `Graph_Adjacency.npy`，给后面的 GNN 用。

- **有 LLM API**：调用大模型从新闻中抽取「公司A–关系–公司B」
- **没有 API**：用规则「新闻里提到某只股票代码就建一条边」来建图，也能跑通

**结果**：`data/processed/Graph_Adjacency.npy`，形状 (股票数, 股票数)

---

## 🎯 总结：整个流程就是

```
原始数据（几千个文件 + 23GB大文件）
    ↓
[etl.py] 整理成统一的格式
    ↓
股价文件 + 新闻文件
    ↓
[align.py] 合并对齐 + 计算特征
    ↓
Final_Model_Data.csv
    ↓
[build_graph.py] 从新闻建公司关系图
    ↓
Graph_Adjacency.npy
    ↓
[train_gnn.py] 训练时自动用 dataset.py 做清洗、标准化、滑动窗口
    ↓
QL-MATCC-GNN 模型训练完成
```

**具体每一步怎么运行、按什么顺序，见项目根目录的《运行说明.md》。**

---

## 💡 关键技巧解释

1. **分块读取（Chunk Processing）**：对于超大文件（23GB），不一次性读入内存，而是分多次读取，避免内存爆炸。

2. **前向填充（Forward Fill）**：如果某天的数据缺失，用前一天的值填充。比如：1月1日有数据，1月2日缺失，1月3日有数据 → 1月2日就用1月1日的数据。

3. **标准化（Standardization）**：不同特征的数值范围差距很大（比如股价可能是100-200，成交量可能是几千万），标准化后都变成相似的尺度，模型更容易学习。

4. **滑动窗口（Sliding Window）**：用过去N天的数据预测未来。比如用1-30号的数据预测31号的，用2-31号的数据预测32号的，以此类推。
