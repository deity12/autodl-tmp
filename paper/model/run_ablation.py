# -*- coding: utf-8 -*-
"""
å…¨è‡ªåŠ¨æ¶ˆèå®éªŒè„šæœ¬ (Run Ablation Studies)
========================================================================
åŠŸèƒ½ï¼š
    ä¾æ¬¡è¿è¡Œ 5 ç»„å®éªŒï¼ˆ1ä¸ªå®Œæ•´æ¨¡å‹åŸºå‡† + 4ç»„æ¶ˆèå®éªŒï¼‰ï¼ŒéªŒè¯å„æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚
    ä¸ä¿®æ”¹åŸæœ‰çš„ train_gnn.pyï¼Œç‹¬ç«‹è¿è¡Œã€‚
    
    å®éªŒåˆ—è¡¨ï¼š
    1. Full Modelï¼šå®Œæ•´æ¨¡å‹åŸºå‡†çº¿ï¼ˆæ‰€æœ‰æ¨¡å—å¼€å¯ï¼‰
    2. w/o Quantumï¼šç§»é™¤é‡å­æ¨¡å—
    3. w/o Graphï¼šç§»é™¤å›¾ç¥ç»ç½‘ç»œ
    4. w/o MATCCï¼šç§»é™¤è¶‹åŠ¿è§£è€¦
    5. w/o Market Guidanceï¼šç§»é™¤å¸‚åœºå¼•å¯¼

è¾“å‡ºï¼š
    - model/ablation/curve_full_model.png (å®Œæ•´æ¨¡å‹åŸºå‡†çº¿)
    - model/ablation/curve_no_quantum.png
    - model/ablation/curve_no_graph.png
    - model/ablation/curve_no_matcc.png
    - model/ablation/curve_no_market_guidance.png
    - model/ablation/losses_full_model.json (å®Œæ•´æ¨¡å‹çš„ Loss æ•°å€¼åˆ—è¡¨)
    - model/ablation/losses_no_quantum.json (å„æ¶ˆèå®éªŒçš„ Loss æ•°å€¼åˆ—è¡¨)
    - model/ablation/losses_*.json (æ‰€æœ‰å®éªŒçš„ Loss æ•°å€¼åˆ—è¡¨ï¼Œç”¨äºåç»­å¯¹æ¯”åˆ†æ)
    - model/ablation/ablation_results_summary.csv (æ±‡æ€»è¡¨æ ¼)
    - model/ablation/ablation_results_comparison.png (å¯¹æ¯”å›¾è¡¨ï¼ŒåŒ…å«5æ¡æ›²çº¿)
    - model/ablation/best_model_*.pth (å„å®éªŒçš„æœ€ä½³æ¨¡å‹)

æ”¹è¿›ç‚¹ï¼š
    1. âœ… æ·»åŠ  Full Model åŸºå‡†å®éªŒï¼ˆç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼‰
    2. âœ… æ·»åŠ  w/o Market Guidance å®éªŒï¼ˆè®ºæ–‡ä¸­ç¬¬4ç»„æ¶ˆèï¼‰
    3. âœ… ä¿å­˜æ¯ä¸ªå®éªŒçš„æœ€ä½³æ¨¡å‹
    4. âœ… **ä¿å­˜æ¯ä¸ªå®éªŒçš„ Loss æ•°å€¼åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰**ï¼Œé¿å…é‡å¤è¿è¡Œ
    5. âœ… ç”Ÿæˆæ±‡æ€»å¯¹æ¯”è¡¨æ ¼å’Œå›¾è¡¨ï¼ˆFull Model ç”¨é‡‘è‰²çªå‡ºæ˜¾ç¤ºï¼‰
    6. âœ… è®°å½•æ›´è¯¦ç»†çš„æŒ‡æ ‡ï¼ˆæœ€ä½³epochã€æœ€ç»ˆlossç­‰ï¼‰
    7. âœ… è·¯å¾„æ£€æŸ¥å’Œé”™è¯¯å¤„ç†æ›´å®Œå–„
    8. âœ… æ‰€æœ‰ç»“æœä¿å­˜åœ¨ç‹¬ç«‹çš„ ablation/ ç›®å½•ï¼Œé¿å…æ··æ·†
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import json
from datetime import datetime
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr, spearmanr

# ================= 1. ç¯å¢ƒä¸è·¯å¾„é…ç½® =================
# ç¡®ä¿èƒ½å¯¼å…¥ä¸Šçº§ç›®å½•çš„æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(os.path.join(parent_dir, 'dataProcessed'))
sys.path.append(current_dir)

# æ•°æ®è·¯å¾„ï¼ˆå…¼å®¹ä¸åŒå¯èƒ½çš„è·¯å¾„ï¼‰
possible_graph_paths = [
    os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency.npy'),
    os.path.join(parent_dir, 'dataProcessed', 'Graph_Adjacency.npy'),
]
possible_csv_paths = [
    os.path.join(parent_dir, 'data', 'processed', 'Final_Model_Data.csv'),
    os.path.join(parent_dir, 'dataProcessed', 'Final_Model_Data.csv'),
]

GRAPH_PATH = None
CSV_PATH = None
for path in possible_graph_paths:
    if os.path.exists(path):
        GRAPH_PATH = path
        break
for path in possible_csv_paths:
    if os.path.exists(path):
        CSV_PATH = path
        break

if CSV_PATH is None:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° Final_Model_Data.csv")
    print("   å·²å°è¯•è·¯å¾„:")
    for path in possible_csv_paths:
        print(f"     - {path}")
    exit(1)

try:
    from dataset import FinancialDataset
    from model_gnn import QL_MATCC_GNN_Model
    print("âœ… æˆåŠŸå¯¼å…¥åŸºç¡€æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)

# ================= 2. ç»Ÿä¸€è¶…å‚æ•° (ä¿æŒä¸ Full Model ä¸€è‡´) =================
BASE_CONFIG = {
    'input_dim': 8,
    'n_embd': 512,
    'n_layers': 4,
    'n_qubits': 4,
    'gnn_embd': 128,
    'seq_len': 30,
    'batch_size': 3072,  # ä¿æŒä¸åŸè®­ç»ƒè„šæœ¬ä¸€è‡´
    'epochs': 6,        # æ¶ˆèå®éªŒå¯ä»¥é€‚å½“å‡å°‘ï¼Œä½†ä¸ºäº†å…¬å¹³å¯¹æ¯”ï¼Œå»ºè®®ä¿æŒç›¸åŒ
    'lr': 1e-4,
    'early_stop_patience': 3,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_workers': 12,
    'prefetch_factor': 8,
    'use_amp': True,
}

# ================= 3. ç»“æœå­˜å‚¨ç›®å½• =================
# åˆ›å»º ablation ç›®å½•ç”¨äºå­˜æ”¾æ‰€æœ‰æ¶ˆèå®éªŒç»“æœ
ABLATION_DIR = os.path.join(current_dir, 'ablation')
os.makedirs(ABLATION_DIR, exist_ok=True)
print(f"ğŸ“ æ¶ˆèå®éªŒç»“æœå°†ä¿å­˜åˆ°: {ABLATION_DIR}")

RESULTS = []  # å­˜å‚¨æ‰€æœ‰å®éªŒçš„ç»“æœ


def calculate_metrics(y_true, y_pred):
    """
    è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼ˆåŒ…æ‹¬é‡åŒ–é‡‘èæ ‡å‡†æŒ‡æ ‡ï¼‰
    
    å‚æ•°:
        y_true: çœŸå®å€¼ (numpy array)
        y_pred: é¢„æµ‹å€¼ (numpy array)
    
    è¿”å›:
        dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()
    
    # ========== 1. ç»Ÿè®¡è¯¯å·®ç±» ==========
    # MSE (å‡æ–¹è¯¯å·®) - è®­ç»ƒä¸»æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)
    
    # MAE (å¹³å‡ç»å¯¹è¯¯å·®) - å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
    mae = mean_absolute_error(y_true, y_pred)
    
    # RMSE (å‡æ–¹æ ¹è¯¯å·®) - ä¸ç›®æ ‡å˜é‡åŒå•ä½ï¼Œæ›´ç›´è§‚
    rmse = np.sqrt(mse)
    
    # RÂ² (å†³å®šç³»æ•°) - æ¨¡å‹æ‹Ÿåˆä¼˜åº¦
    r2 = r2_score(y_true, y_pred)
    
    # MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®) - ç›¸å¯¹è¯¯å·®
    mask = np.abs(y_true) > 1e-8
    if np.sum(mask) > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = None
    
    # ========== 2. æ–¹å‘é¢„æµ‹ç±» ==========
    # Directional Accuracy (æ–¹å‘å‡†ç¡®ç‡) - é‡‘èé¢„æµ‹æ ¸å¿ƒæŒ‡æ ‡
    # é¢„æµ‹æ¶¨è·Œæ–¹å‘çš„å‡†ç¡®ç‡ï¼Œå¯¹é‡‘èé¢„æµ‹éå¸¸é‡è¦
    true_direction = np.sign(y_true)
    pred_direction = np.sign(y_pred)
    directional_accuracy = np.mean(true_direction == pred_direction)
    
    # ========== 3. é‡åŒ–æŠ•èµ„ç±» ==========
    # IC (Information Coefficient) - ä¿¡æ¯ç³»æ•°
    # Pearson ç›¸å…³ç³»æ•°ï¼Œè¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„çº¿æ€§ç›¸å…³æ€§
    # è¿™æ˜¯é‡åŒ–é‡‘èé¢†åŸŸçš„é»„é‡‘æ ‡å‡†æŒ‡æ ‡
    try:
        ic, ic_pvalue = pearsonr(y_pred, y_true)
        ic = float(ic)
    except:
        ic = None
        ic_pvalue = None
    
    # RankIC (Rank Information Coefficient) - ç§©ä¿¡æ¯ç³»æ•°
    # Spearman ç§©ç›¸å…³ç³»æ•°ï¼Œè¡¡é‡é¢„æµ‹æ’åä¸çœŸå®æ’åçš„ç›¸å…³æ€§
    # æ¯” IC æ›´ç¨³å¥ï¼Œä¸å—å¼‚å¸¸å€¼å½±å“ï¼Œæ˜¯åŸºé‡‘å…¬å¸æœ€çœ‹é‡çš„æŒ‡æ ‡
    try:
        rank_ic, rank_ic_pvalue = spearmanr(y_pred, y_true)
        rank_ic = float(rank_ic)
    except:
        rank_ic = None
        rank_ic_pvalue = None
    
    # ä¼ ç»Ÿç›¸å…³ç³»æ•°ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
    try:
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        correlation = float(correlation)
    except:
        correlation = None
    
    return {
        # ç»Ÿè®¡è¯¯å·®ç±»
        'mse': float(mse),
        'mae': float(mae),
        'rmse': float(rmse),
        'r2': float(r2),
        'mape': float(mape) if mape is not None and not np.isnan(mape) else None,
        
        # æ–¹å‘é¢„æµ‹ç±»
        'directional_accuracy': float(directional_accuracy),
        
        # é‡åŒ–æŠ•èµ„ç±»
        'ic': ic,  # Information Coefficient (Pearson)
        'ic_pvalue': float(ic_pvalue) if ic_pvalue is not None else None,
        'rank_ic': rank_ic,  # Rank Information Coefficient (Spearman)
        'rank_ic_pvalue': float(rank_ic_pvalue) if rank_ic_pvalue is not None else None,
        
        # å…¼å®¹æ€§æŒ‡æ ‡
        'correlation': correlation,
    }


def run_experiment(exp_name, use_quantum=True, use_graph=True, use_matcc=True, use_market_guidance=True):
    """
    è¿è¡Œå•ä¸ªå®éªŒçš„æ ¸å¿ƒå‡½æ•°
    
    å‚æ•°:
        exp_name: å®éªŒåç§°ï¼ˆå¦‚ "no_quantum"ï¼‰
        use_quantum: æ˜¯å¦ä½¿ç”¨é‡å­æ¨¡å—
        use_graph: æ˜¯å¦ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œ
        use_matcc: æ˜¯å¦ä½¿ç”¨MATCCè¶‹åŠ¿è§£è€¦
        use_market_guidance: æ˜¯å¦ä½¿ç”¨å¸‚åœºå¼•å¯¼
    """
    print("\n" + "="*70)
    print(f"ğŸ§ª å¼€å§‹è¿è¡Œå®éªŒ: {exp_name}")
    print(f"   é…ç½®: Quantum={use_quantum}, Graph={use_graph}, MATCC={use_matcc}, MarketGuidance={use_market_guidance}")
    print("="*70)

    # ---------------- A. å‡†å¤‡å›¾è°± ----------------
    # å¦‚æœæ˜¯ w/o Graph å®éªŒï¼Œå¼ºåˆ¶ä½¿ç”¨å•ä½é˜µï¼ˆåˆ‡æ–­å›¾è¿æ¥ï¼‰
    if not use_graph:
        print("   âš ï¸ [æ¶ˆèè®¾ç½®] ç¦ç”¨å›¾ç¥ç»ç½‘ç»œ (ä½¿ç”¨å•ä½é˜µ)")
        df_t = pd.read_csv(CSV_PATH, usecols=['Ticker'])
        num_nodes = int(df_t['Ticker'].nunique())
        adj_matrix = np.eye(num_nodes, dtype=np.float32)
    else:
        if GRAPH_PATH and os.path.exists(GRAPH_PATH):
            adj_matrix = np.load(GRAPH_PATH)
            print(f"   âœ… åŠ è½½å›¾è°±: {GRAPH_PATH}, å½¢çŠ¶: {adj_matrix.shape}")
        else:
            # å…œåº•ï¼šä½¿ç”¨å•ä½é˜µ
            print(f"   âš ï¸ æœªæ‰¾åˆ°å›¾è°±æ–‡ä»¶ï¼Œä½¿ç”¨å•ä½é˜µ")
            df_t = pd.read_csv(CSV_PATH, usecols=['Ticker'])
            num_nodes = int(df_t['Ticker'].nunique())
            adj_matrix = np.eye(num_nodes, dtype=np.float32)
    
    num_nodes = adj_matrix.shape[0]

    # ---------------- B. å‡†å¤‡æ•°æ® ----------------
    # æ¯æ¬¡é‡æ–°åŠ è½½æ•°æ®ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
    train_dataset = FinancialDataset(CSV_PATH, seq_len=BASE_CONFIG['seq_len'], mode='train')
    test_dataset = FinancialDataset(CSV_PATH, seq_len=BASE_CONFIG['seq_len'], mode='test', scaler=train_dataset.scaler)
    
    train_loader = DataLoader(train_dataset, batch_size=BASE_CONFIG['batch_size'], shuffle=True, 
                              num_workers=BASE_CONFIG['num_workers'], pin_memory=True, 
                              prefetch_factor=BASE_CONFIG['prefetch_factor'],
                              persistent_workers=True if BASE_CONFIG['num_workers'] > 0 else False)
    test_loader = DataLoader(test_dataset, batch_size=BASE_CONFIG['batch_size'], shuffle=False, 
                             num_workers=BASE_CONFIG['num_workers'], pin_memory=True, 
                             prefetch_factor=BASE_CONFIG['prefetch_factor'],
                             persistent_workers=True if BASE_CONFIG['num_workers'] > 0 else False)

    # ---------------- C. åˆå§‹åŒ–æ¨¡å‹ ----------------
    model = QL_MATCC_GNN_Model(
        input_dim=BASE_CONFIG['input_dim'],
        n_embd=BASE_CONFIG['n_embd'],
        n_layers=BASE_CONFIG['n_layers'],
        n_qubits=BASE_CONFIG['n_qubits'],
        num_nodes=num_nodes,
        adj_matrix=adj_matrix,
        gnn_embd=BASE_CONFIG['gnn_embd'],
        # === å…³é”®ï¼šè¿™é‡Œä¼ å…¥æ¶ˆèå¼€å…³ ===
        use_quantum=use_quantum,
        use_matcc=use_matcc,
        use_market_guidance=use_market_guidance,
    ).to(BASE_CONFIG['device'])

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=BASE_CONFIG['lr'], betas=(0.9, 0.999), eps=1e-8)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6)
    scaler = torch.cuda.amp.GradScaler() if BASE_CONFIG['use_amp'] else None

    # ---------------- D. è®­ç»ƒå¾ªç¯ ----------------
    train_losses, val_losses = [], []
    best_val = float('inf')
    best_epoch = 0
    counter = 0

    for epoch in range(BASE_CONFIG['epochs']):
        model.train()
        train_loss = 0.0
        steps = 0
        
        # è®­ç»ƒ
        pbar = tqdm(train_loader, desc=f"[{exp_name}] Ep {epoch+1}/{BASE_CONFIG['epochs']}", ncols=100)
        for batch in pbar:
            x = batch['x'].to(BASE_CONFIG['device'], non_blocking=True)
            y = batch['y'].to(BASE_CONFIG['device'], non_blocking=True)
            vol = batch['vol'].to(BASE_CONFIG['device'], non_blocking=True)
            node_indices = batch.get('node_indices')
            if node_indices is not None:
                node_indices = node_indices.to(BASE_CONFIG['device'], non_blocking=True)

            optimizer.zero_grad()
            if scaler:
                with torch.cuda.amp.autocast():
                    preds = model(x, vol, node_indices=node_indices)
                    loss = criterion(preds, y)
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                preds = model(x, vol, node_indices=node_indices)
                loss = criterion(preds, y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
            
            train_loss += loss.item()
            steps += 1
            pbar.set_postfix(loss=f"{loss.item():.6f}")

        avg_train = train_loss / len(train_loader)
        train_losses.append(avg_train)

        # éªŒè¯ï¼ˆæ”¶é›†æ‰€æœ‰é¢„æµ‹å€¼å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰
        model.eval()
        val_loss = 0.0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(BASE_CONFIG['device'], non_blocking=True)
                y = batch['y'].to(BASE_CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(BASE_CONFIG['device'], non_blocking=True)
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(BASE_CONFIG['device'], non_blocking=True)
                
                if scaler:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                val_loss += criterion(preds, y).item()
                
                # æ”¶é›†é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼ˆç”¨äºè®¡ç®—å®Œæ•´æŒ‡æ ‡ï¼‰
                all_preds.append(preds.cpu().numpy())
                all_targets.append(y.cpu().numpy())
        
        avg_val = val_loss / len(test_loader)
        val_losses.append(avg_val)
        
        # è®¡ç®—å®Œæ•´è¯„ä¼°æŒ‡æ ‡ï¼ˆä»…åœ¨æœ€ä½³epochæ—¶è®¡ç®—ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        metrics = None
        if avg_val < best_val:
            all_preds_np = np.concatenate(all_preds, axis=0)
            all_targets_np = np.concatenate(all_targets, axis=0)
            metrics = calculate_metrics(all_targets_np, all_preds_np)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val)
        cur_lr = optimizer.param_groups[0]['lr']
        
        print(f"   Ep {epoch+1}: Train={avg_train:.6f}, Val={avg_val:.6f}, lr={cur_lr:.2e}")

        # æ—©åœå’Œä¿å­˜æœ€ä½³æ¨¡å‹
        best_metrics = None
        if avg_val < best_val:
            best_val = avg_val
            best_epoch = epoch + 1
            best_metrics = metrics  # ä¿å­˜æœ€ä½³epochçš„æŒ‡æ ‡
            counter = 0
            # ä¿å­˜æ¶ˆèå®éªŒçš„æœ€ä½³æ¨¡å‹åˆ° ablation ç›®å½•
            model_save_path = os.path.join(ABLATION_DIR, f'best_model_{exp_name}.pth')
            torch.save(model.state_dict(), model_save_path)
            
            # æ‰“å°å…³é”®æŒ‡æ ‡
            if metrics:
                print(f"   ğŸ’¾ Best model saved (Ep {best_epoch}, Val={best_val:.6f})")
                print(f"      Metrics: RÂ²={metrics['r2']:.4f}, MAE={metrics['mae']:.6f}, "
                      f"DirAcc={metrics['directional_accuracy']:.2%}", end="")
                if metrics.get('ic') is not None:
                    print(f", IC={metrics['ic']:.4f}", end="")
                if metrics.get('rank_ic') is not None:
                    print(f", RankIC={metrics['rank_ic']:.4f}", end="")
                print()
            else:
                print(f"   ğŸ’¾ Best model saved (Ep {best_epoch}, Val={best_val:.6f})")
        else:
            counter += 1
            if counter >= BASE_CONFIG['early_stop_patience']:
                print("   ğŸ›‘ Early stopping")
                break

    # ---------------- E. ç”»å›¾å¹¶ä¿å­˜ ----------------
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss', lw=2)
    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss', lw=2)
    if val_losses:
        plt.plot(best_epoch, best_val, 'g*', markersize=14, label=f'Best (Ep {best_epoch})')
    plt.title(f'Ablation Study: {exp_name}', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(ABLATION_DIR, f'curve_{exp_name}.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ç»“æœå·²ä¿å­˜: {save_path}")
    
    # ---------------- F. æœ€ç»ˆè¯„ä¼°ï¼ˆåœ¨æœ€ä½³æ¨¡å‹ä¸Šè®¡ç®—å®Œæ•´æŒ‡æ ‡ï¼‰---------------
    # é‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è®¡ç®—å®Œæ•´æŒ‡æ ‡
    if best_metrics is None:
        print("   âš ï¸ é‡æ–°è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡...")
        model.load_state_dict(torch.load(os.path.join(ABLATION_DIR, f'best_model_{exp_name}.pth')))
        model.eval()
        all_preds_final = []
        all_targets_final = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(BASE_CONFIG['device'], non_blocking=True)
                y = batch['y'].to(BASE_CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(BASE_CONFIG['device'], non_blocking=True)
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(BASE_CONFIG['device'], non_blocking=True)
                
                if scaler:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                all_preds_final.append(preds.cpu().numpy())
                all_targets_final.append(y.cpu().numpy())
        
        all_preds_final_np = np.concatenate(all_preds_final, axis=0)
        all_targets_final_np = np.concatenate(all_targets_final, axis=0)
        best_metrics = calculate_metrics(all_targets_final_np, all_preds_final_np)
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        if best_metrics:
            print(f"   ğŸ“Š Final Metrics: RÂ²={best_metrics['r2']:.4f}, "
                  f"DirAcc={best_metrics['directional_accuracy']:.2%}", end="")
            if best_metrics.get('ic') is not None:
                print(f", IC={best_metrics['ic']:.4f}", end="")
            if best_metrics.get('rank_ic') is not None:
                print(f", RankIC={best_metrics['rank_ic']:.4f}", end="")
            print()
    
    # ---------------- G. ä¿å­˜ Loss æ•°å€¼åˆ—è¡¨å’Œè¯„ä¼°æŒ‡æ ‡ ----------------
    loss_data_path = os.path.join(ABLATION_DIR, f'losses_{exp_name}.json')
    loss_data = {
        'experiment': exp_name,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val,
        'best_epoch': best_epoch,
        'total_epochs': len(train_losses),
        'use_quantum': use_quantum,
        'use_graph': use_graph,
        'use_matcc': use_matcc,
        'use_market_guidance': use_market_guidance,
        'metrics': best_metrics,  # æ·»åŠ å®Œæ•´è¯„ä¼°æŒ‡æ ‡
        'config': {
            'batch_size': BASE_CONFIG['batch_size'],
            'lr': BASE_CONFIG['lr'],
            'epochs': BASE_CONFIG['epochs'],
        }
    }
    with open(loss_data_path, 'w') as f:
        json.dump(loss_data, f, indent=2)
    print(f"âœ… Loss æ•°å€¼åˆ—è¡¨å’Œè¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {loss_data_path}")
    
    # ---------------- H. è®°å½•ç»“æœåˆ°æ±‡æ€»åˆ—è¡¨ ----------------
    final_train = train_losses[-1] if train_losses else float('nan')
    final_val = val_losses[-1] if val_losses else float('nan')
    
    result = {
        'experiment': exp_name,
        'best_val_loss': best_val,
        'best_epoch': best_epoch,
        'final_train_loss': final_train,
        'final_val_loss': final_val,
        'total_epochs': len(train_losses),
        'use_quantum': use_quantum,
        'use_graph': use_graph,
        'use_matcc': use_matcc,
        'use_market_guidance': use_market_guidance,
        # æ·»åŠ è¯„ä¼°æŒ‡æ ‡
        # ç»Ÿè®¡è¯¯å·®ç±»
        'mse': best_metrics['mse'] if best_metrics else None,
        'mae': best_metrics['mae'] if best_metrics else None,
        'rmse': best_metrics['rmse'] if best_metrics else None,
        'r2': best_metrics['r2'] if best_metrics else None,
        'mape': best_metrics['mape'] if best_metrics and best_metrics['mape'] is not None else None,
        
        # æ–¹å‘é¢„æµ‹ç±»
        'directional_accuracy': best_metrics['directional_accuracy'] if best_metrics else None,
        
        # é‡åŒ–æŠ•èµ„ç±»
        'ic': best_metrics['ic'] if best_metrics and best_metrics.get('ic') is not None else None,
        'ic_pvalue': best_metrics['ic_pvalue'] if best_metrics and best_metrics.get('ic_pvalue') is not None else None,
        'rank_ic': best_metrics['rank_ic'] if best_metrics and best_metrics.get('rank_ic') is not None else None,
        'rank_ic_pvalue': best_metrics['rank_ic_pvalue'] if best_metrics and best_metrics.get('rank_ic_pvalue') is not None else None,
        
        # å…¼å®¹æ€§æŒ‡æ ‡
        'correlation': best_metrics['correlation'] if best_metrics and best_metrics.get('correlation') is not None else None,
    }
    RESULTS.append(result)
    
    # æ¸…ç†æ˜¾å­˜
    del model, optimizer, scheduler
    if scaler:
        del scaler
    torch.cuda.empty_cache()


def save_summary_results():
    """ä¿å­˜æ±‡æ€»ç»“æœåˆ°CSVå’Œç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
    if not RESULTS:
        print("âš ï¸ æ²¡æœ‰å®éªŒç»“æœå¯ä¿å­˜")
        return
    
    # ä¿å­˜CSVåˆ° ablation ç›®å½•ï¼ˆåŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼‰
    df_results = pd.DataFrame(RESULTS)
    
    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼Œè®©é‡è¦æŒ‡æ ‡åœ¨å‰é¢
    column_order = [
        'experiment', 'best_val_loss', 
        # ç»Ÿè®¡è¯¯å·®ç±»
        'mse', 'mae', 'rmse', 'r2', 'mape',
        # æ–¹å‘é¢„æµ‹ç±»
        'directional_accuracy',
        # é‡åŒ–æŠ•èµ„ç±»
        'ic', 'ic_pvalue', 'rank_ic', 'rank_ic_pvalue',
        # å…¶ä»–
        'correlation', 'best_epoch', 'final_train_loss', 'final_val_loss', 
        'total_epochs', 'use_quantum', 'use_graph', 'use_matcc', 'use_market_guidance'
    ]
    # åªä¿ç•™å­˜åœ¨çš„åˆ—
    column_order = [col for col in column_order if col in df_results.columns]
    df_results = df_results[column_order]
    
    csv_path = os.path.join(ABLATION_DIR, 'ablation_results_summary.csv')
    df_results.to_csv(csv_path, index=False, float_format='%.6f')
    print(f"\nâœ… æ±‡æ€»ç»“æœå·²ä¿å­˜: {csv_path}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å·¦å›¾ï¼šæœ€ä½³éªŒè¯æŸå¤±å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    exp_names = [r['experiment'] for r in RESULTS]
    best_vals = [r['best_val_loss'] for r in RESULTS]
    
    # ä¸º Full Model ä½¿ç”¨ç‰¹æ®Šé¢œè‰²ï¼ˆé‡‘è‰²ï¼‰ï¼Œå…¶ä»–ç”¨ä¸åŒé¢œè‰²
    colors = []
    ablation_colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    ablation_idx = 0
    
    for name in exp_names:
        if name == 'full_model':
            colors.append('#FFD700')  # é‡‘è‰²ï¼Œçªå‡ºåŸºå‡†çº¿
        else:
            # ä¸ºæ¶ˆèå®éªŒåˆ†é…ä¸åŒé¢œè‰²
            if ablation_idx < len(ablation_colors):
                colors.append(ablation_colors[ablation_idx])
                ablation_idx += 1
            else:
                colors.append('#1f77b4')  # é»˜è®¤è“è‰²
    
    axes[0].bar(exp_names, best_vals, color=colors)
    axes[0].set_ylabel('Best Val Loss (MSE)', fontsize=12)
    axes[0].set_title('Ablation Study: Best Validation Loss', fontsize=14, fontweight='bold')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(best_vals):
        axes[0].text(i, v, f'{v:.6f}', ha='center', va='bottom', fontsize=9)
    
    # å³å›¾ï¼šæœ€ä½³éªŒè¯æŸå¤±å¯¹æ¯”ï¼ˆæ›´æ¸…æ™°çš„å±•ç¤ºï¼‰
    # æŒ‰æŸå¤±å€¼æ’åºï¼ŒFull Model åº”è¯¥æ˜¯æœ€ä½çš„
    sorted_results = sorted(RESULTS, key=lambda x: x['best_val_loss'])
    sorted_names = [r['experiment'] for r in sorted_results]
    sorted_vals = [r['best_val_loss'] for r in sorted_results]
    
    # ä¸º Full Model ä½¿ç”¨ç‰¹æ®Šé¢œè‰²
    bar_colors = []
    ablation_colors_h = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    ablation_idx_h = 0
    
    for name in sorted_names:
        if name == 'full_model':
            bar_colors.append('#FFD700')  # é‡‘è‰²
        else:
            # ä¸ºæ¶ˆèå®éªŒåˆ†é…ä¸åŒé¢œè‰²
            if ablation_idx_h < len(ablation_colors_h):
                bar_colors.append(ablation_colors_h[ablation_idx_h])
                ablation_idx_h += 1
            else:
                bar_colors.append('#1f77b4')  # é»˜è®¤è“è‰²
    
    axes[1].barh(sorted_names, sorted_vals, color=bar_colors)
    axes[1].set_xlabel('Best Val Loss (MSE)', fontsize=12)
    axes[1].set_title('Ablation Study: Loss Ranking', fontsize=14, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(sorted_vals):
        axes[1].text(v, i, f' {v:.6f}', va='center', fontsize=9)
    
    axes[1].set_xlabel('Epochs', fontsize=12)
    axes[1].set_ylabel('Loss (MSE)', fontsize=12)
    axes[1].legend(fontsize=9)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    comparison_path = os.path.join(ABLATION_DIR, 'ablation_results_comparison.png')
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {comparison_path}")
    
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print("\n" + "="*70)
    print("ğŸ“Š æ¶ˆèå®éªŒæ±‡æ€»ç»“æœ")
    print("="*70)
    print(df_results.to_string(index=False))
    print("="*70)


# ================= 4. ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨å…¨è‡ªåŠ¨æ¶ˆèå®éªŒæµç¨‹...")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {current_dir}")
    print(f"ğŸ“ ç»“æœç›®å½•: {ABLATION_DIR}")
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {CSV_PATH}")
    print(f"ğŸ”— å›¾è°±æ–‡ä»¶: {GRAPH_PATH if GRAPH_PATH else 'æœªæ‰¾åˆ°ï¼ˆå°†ä½¿ç”¨å•ä½é˜µï¼‰'}")
    print(f"ğŸ’» è®¾å¤‡: {BASE_CONFIG['device']}")
    
    start_time = datetime.now()
    
    # å®éªŒ 0: å®Œæ•´æ¨¡å‹åŸºå‡† (Full Model / Baseline)
    # æ‰€æœ‰å¼€å…³å…¨éƒ¨æ‰“å¼€ï¼Œä½œä¸ºå¯¹æ¯”çš„"å¤©èŠ±æ¿"ï¼Œç¡®ä¿åœ¨ç›¸åŒå®éªŒæ¡ä»¶ä¸‹å…¬å¹³å¯¹æ¯”
    print("\n" + "="*70)
    print("ğŸ“Œ é‡è¦æç¤ºï¼šé¦–å…ˆè¿è¡Œ Full Model ä½œä¸ºåŸºå‡†çº¿")
    print("   è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰å®éªŒåœ¨å®Œå…¨ç›¸åŒçš„æ¡ä»¶ä¸‹ï¼ˆepochsã€batch_sizeã€éšæœºç§å­ç­‰ï¼‰è¿›è¡Œå¯¹æ¯”")
    print("="*70)
    run_experiment(exp_name="full_model", 
                   use_quantum=True, use_graph=True, use_matcc=True, use_market_guidance=True)
    
    # å®éªŒ 1: æ— é‡å­æ¨¡å— (w/o Quantum)
    run_experiment(exp_name="no_quantum", 
                   use_quantum=False, use_graph=True, use_matcc=True, use_market_guidance=True)
    
    # å®éªŒ 2: æ— å›¾ç¥ç»ç½‘ç»œ (w/o Graph)
    run_experiment(exp_name="no_graph", 
                   use_quantum=True, use_graph=False, use_matcc=True, use_market_guidance=True)
    
    # å®éªŒ 3: æ— è¶‹åŠ¿è§£è€¦ (w/o MATCC)
    run_experiment(exp_name="no_matcc", 
                   use_quantum=True, use_graph=True, use_matcc=False, use_market_guidance=True)
    
    # å®éªŒ 4: æ— å¸‚åœºå¼•å¯¼ (w/o Market Guidance) - ã€æ–°å¢ã€‘
    run_experiment(exp_name="no_market_guidance", 
                   use_quantum=True, use_graph=True, use_matcc=True, use_market_guidance=False)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    save_summary_results()
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60  # åˆ†é’Ÿ
    
    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰å®éªŒå·²å®Œæˆï¼ï¼ˆ1ä¸ªåŸºå‡† + 4ä¸ªæ¶ˆèå®éªŒï¼‰")
    print(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f} åˆ†é’Ÿ")
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - ablation/curve_full_model.png (å®Œæ•´æ¨¡å‹åŸºå‡†)")
    print("   - ablation/curve_no_quantum.png")
    print("   - ablation/curve_no_graph.png")
    print("   - ablation/curve_no_matcc.png")
    print("   - ablation/curve_no_market_guidance.png")
    print("   - ablation/losses_*.json (æ¯ä¸ªå®éªŒçš„ Loss æ•°å€¼åˆ—è¡¨ï¼Œå…±5ä¸ª)")
    print("   - ablation/ablation_results_summary.csv")
    print("   - ablation/ablation_results_comparison.png (åŒ…å«5æ¡æ›²çº¿å¯¹æ¯”)")
    print("   - ablation/best_model_*.pth (æ¯ä¸ªå®éªŒçš„æœ€ä½³æ¨¡å‹ï¼Œå…±5ä¸ª)")
    print("="*70)
