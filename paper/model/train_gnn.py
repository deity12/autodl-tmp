# -*- coding: utf-8 -*-
"""
QL-MATCC-GNN æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆå¯¹åº”è®ºæ–‡å®Œæ•´æ¶æ„ï¼šLLM å›¾è°± + Quantum-RWKV + MATCC + GNNï¼‰
========================================================================
ç›¸å¯¹ train_autodl.py çš„ä¿®æ”¹ï¼š
    1. ä½¿ç”¨ QL_MATCC_GNN_Modelï¼ˆèåˆ GAT å›¾ç¥ç»ç½‘ç»œï¼‰
    2. åŠ è½½é‚»æ¥çŸ©é˜µ Graph_Adjacency.npyï¼ˆç”± dataProcessed/build_graph.py ç”Ÿæˆï¼‰
    3. å‰å‘æ—¶ä¼ å…¥ node_indices=Noneï¼ˆé€€åŒ–æ¨¡å¼ï¼šbatch å†…æ— å›¾ç»“æ„æ—¶ GNN ä»…è‡ªç¯ï¼‰
    4. æ¨¡å‹ä¸æ›²çº¿åˆ†åˆ«ä¿å­˜ä¸º best_model_gnn.pthã€training_curve_gnn.pngï¼Œé¿å…è¦†ç›–åŸç‰ˆ

è¿è¡Œé¡ºåºå»ºè®®ï¼š
    1. å…ˆè¿è¡Œ dataProcessed/build_graph.py ç”Ÿæˆ Graph_Adjacency.npy
    2. å†è¿è¡Œæœ¬è„šæœ¬ï¼špython model/train_gnn.py
"""

import sys
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr, spearmanr

# ================= 1. ç¯å¢ƒä¸è·¯å¾„ =================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(os.path.join(parent_dir, 'dataProcessed'))
sys.path.append(current_dir)

# é‚»æ¥çŸ©é˜µè·¯å¾„ï¼ˆä¸ build_graph è¾“å‡ºä¸€è‡´ï¼‰
GRAPH_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency.npy')

try:
    from dataset import FinancialDataset
    from model_gnn import QL_MATCC_GNN_Model
    print("âœ… æˆåŠŸå¯¼å…¥ datasetã€model_gnn æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)

# ================= 2. è¶…å‚æ•°ï¼ˆé’ˆå¯¹ RTX 3090 48GB ä¼˜åŒ–ï¼‰=================
CONFIG = {
    'csv_path': os.path.join(parent_dir, 'data', 'processed', 'Final_Model_Data.csv'),
    'input_dim': 8,
    # ã€æ˜¾å­˜ä¼˜åŒ–ã€‘æ¨¡å‹ç»´åº¦ï¼š48GB æ˜¾å­˜å…è®¸æ›´å¤§çš„åµŒå…¥ç»´åº¦ï¼Œæå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›
    'n_embd': 512,  # å¯å°è¯• 768 æˆ– 1024ï¼Œä½†éœ€é…åˆ batch_size è°ƒæ•´
    'n_layers': 4,
    'n_qubits': 4,
    # ã€æ˜¾å­˜ä¼˜åŒ–ã€‘GNNï¼šå›¾åµŒå…¥ç»´åº¦ï¼ˆGAT çš„ NÃ—NÃ—2*gnn_embdï¼‰
    # 48GB æ˜¾å­˜å…è®¸é€‚å½“å¢å¤§ï¼Œæå‡å›¾ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
    'gnn_embd': 128,  # ä» 64 æå‡åˆ° 128ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å­˜
    'seq_len': 30,
    # GNN çš„ GAT å±‚åœ¨ batch å†…åšé‚»å±…èšåˆï¼Œç†è®ºä¸ŠåŒä¸€ batch ä¸­ä¼šæ··åˆä¸åŒæ—¶åˆ»çš„æ ·æœ¬ã€‚
    # æœ¬é¡¹ç›®åœ¨å·¥ç¨‹ä¸Šé‡‡ç”¨â€œè¾ƒå¤§ batch sizeâ€ï¼ˆä¾‹å¦‚ 512ï¼‰çš„æŠ˜ä¸­ç­–ç•¥ï¼š
    #   1ï¼‰åˆ©ç”¨å¤§æ˜¾å­˜ï¼Œå°† batch æ‹‰å¤§ï¼Œä½¿å¾—æ¯ä¸ª batch ä¸­æ¥è‡ªåŒä¸€æ—¶é—´æ®µã€åŒä¸€è¡Œä¸šçš„æ ·æœ¬å æ¯”è¾ƒé«˜ï¼›
    #   2ï¼‰åœ¨å®è·µä¸­ï¼Œè¿™æ˜¯æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œå¸¸ç”¨çš„ Sampled Batch è¿‘ä¼¼è®­ç»ƒæ–¹å¼ï¼Œå¯åœ¨å¯æ¥å—çš„æ—¶é—´é”™ä½ä¸‹è·å¾—ç¨³å®šæ”¶ç›Šã€‚
    # è¿™ä¸€ç‚¹ä¼šåœ¨è®ºæ–‡å®éªŒè®¾è®¡éƒ¨åˆ†è¿›è¡Œè¯´æ˜ã€‚
    # ã€å…³é”®ä¼˜åŒ–ã€‘Batch Sizeï¼šRTX 3090 48GB æ˜¾å­˜å¯æ”¯æŒæ›´å¤§çš„ batch
    # å¤§ batch çš„ä¼˜åŠ¿ï¼š
    #   1ï¼‰æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œæ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
    #   2ï¼‰å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼ŒåŠ é€Ÿè®­ç»ƒ
    #   3ï¼‰å¯¹äº GNNï¼Œå¤§ batch ä¸­æ¥è‡ªåŒä¸€æ—¶é—´æ®µã€ç›¸å…³è‚¡ç¥¨çš„æ ·æœ¬å æ¯”æ›´é«˜ï¼Œå‡å°‘æ—¶é—´é”™ä½å™ªå£°
    #   4ï¼‰è¿™æ˜¯æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œå¸¸ç”¨çš„ Sampled Batch è¿‘ä¼¼è®­ç»ƒæ–¹å¼
    'batch_size': 3072,  # ä» 512 æå‡åˆ° 1024ï¼Œå……åˆ†åˆ©ç”¨ 48GB æ˜¾å­˜
    'epochs': 10,
    'lr': 1e-4,
    'early_stop_patience': 3,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    # ã€æ•°æ®åŠ è½½ä¼˜åŒ–ã€‘num_workersï¼šåŒ¹é… CPU æ ¸å¿ƒæ•°ï¼ˆ12æ ¸ï¼‰ï¼Œæœ€å¤§åŒ–æ•°æ®åŠ è½½å¹¶è¡Œåº¦
    # æ›´å¤š worker å¯ä»¥æå‰å‡†å¤‡å¥½æ•°æ®ï¼Œé¿å… GPU ç­‰å¾… CPU æ•°æ®é¢„å¤„ç†
    'num_workers': 12,  # ä» 8 æå‡åˆ° 12ï¼ŒåŒ¹é… CPU æ ¸å¿ƒæ•°
    # ã€æ•°æ®é¢„å–ä¼˜åŒ–ã€‘prefetch_factorï¼šæ¯ä¸ª worker é¢„å–çš„ batch æ•°é‡
    # å¢å¤§æ­¤å€¼å¯ä»¥è¿›ä¸€æ­¥å‡å°‘ GPU ç­‰å¾…æ—¶é—´ï¼Œä½†ä¼šå ç”¨æ›´å¤šå†…å­˜
    'prefetch_factor': 8,  # ä» 4 æå‡åˆ° 8ï¼Œæ›´æ¿€è¿›çš„æ•°æ®é¢„å–
    # ã€æ··åˆç²¾åº¦è®­ç»ƒã€‘å¯ç”¨ AMP å¯ä»¥ï¼š
    #   1ï¼‰å‡å°‘çº¦ 50% çš„æ˜¾å­˜å ç”¨ï¼ˆç»å…¸æ¨¡å—ç”¨ FP16ï¼‰
    #   2ï¼‰åŠ é€Ÿè®­ç»ƒï¼ˆRTX 3090 çš„ Tensor Core å¯¹ FP16 æœ‰ç¡¬ä»¶åŠ é€Ÿï¼‰
    #   3ï¼‰é‡å­æ¨¡å—ä¿æŒ FP32 ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
    'use_amp': True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    # ã€ç¼–è¯‘ä¼˜åŒ–ã€‘å¯ç”¨ torch.compile å¯ä»¥è¿›ä¸€æ­¥åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
    # ä½†é¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘æ—¶é—´ï¼Œä¸”å¯èƒ½ä¸æŸäº›è‡ªå®šä¹‰æ“ä½œä¸å…¼å®¹
    'use_compile': False,  # å¯é€‰ï¼šå¯ç”¨ JIT ç¼–è¯‘åŠ é€Ÿï¼ˆéœ€ PyTorch 2.0+ï¼‰
}


def main():
    print(f">>> Training on device: {CONFIG['device']}")
    if CONFIG['device'] == 'cuda':
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        print(f"   GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
        print(f"   Batch Size: {CONFIG['batch_size']} (å……åˆ†åˆ©ç”¨ {gpu_memory:.1f}GB æ˜¾å­˜)")
        print(f"   DataLoader Workers: {CONFIG['num_workers']} (åŒ¹é… CPU æ ¸å¿ƒæ•°)")

    # ================= 3. åŠ è½½é‚»æ¥çŸ©é˜µ =================
    if os.path.exists(GRAPH_PATH):
        adj_matrix = np.load(GRAPH_PATH)
        print(f">>> åŠ è½½å›¾è°±: {GRAPH_PATH}, å½¢çŠ¶: {adj_matrix.shape}")
    else:
        # è‹¥æœªç”Ÿæˆå›¾è°±ï¼Œç”¨å•ä½é˜µå…œåº•ï¼ˆGNN é€€åŒ–ä¸ºä»…è‡ªç¯ï¼‰
        df_t = pd.read_csv(CONFIG['csv_path'], usecols=['Ticker'])
        num_nodes = int(df_t['Ticker'].nunique())
        adj_matrix = np.eye(num_nodes, dtype=np.float32)
        print(f">>> æœªæ‰¾åˆ° {GRAPH_PATH}ï¼Œä½¿ç”¨å•ä½é˜µ (num_nodes={num_nodes})ã€‚å»ºè®®å…ˆè¿è¡Œ: python dataProcessed/build_graph.py")

    num_nodes = adj_matrix.shape[0]

    # ================= 4. æ•°æ®åŠ è½½ =================
    print("\n>>> Loading Datasets...")
    try:
        train_dataset = FinancialDataset(CONFIG['csv_path'], seq_len=CONFIG['seq_len'], mode='train')
        test_dataset = FinancialDataset(CONFIG['csv_path'], seq_len=CONFIG['seq_len'], mode='test', scaler=train_dataset.scaler)
        print(f"   Train: {len(train_dataset)}, Test: {len(test_dataset)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=True,
        num_workers=CONFIG.get('num_workers', 8),
        pin_memory=True,
        prefetch_factor=CONFIG.get('prefetch_factor', 4),
        persistent_workers=True if CONFIG.get('num_workers', 0) > 0 else False,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=CONFIG.get('num_workers', 8),
        pin_memory=True,
        prefetch_factor=CONFIG.get('prefetch_factor', 4),
        persistent_workers=True if CONFIG.get('num_workers', 0) > 0 else False,
    )

    # ================= 5. æ¨¡å‹åˆå§‹åŒ–ï¼ˆQL-MATCC-GNNï¼‰=================
    print("\n>>> Initializing QL_MATCC_GNN_Model...")
    model = QL_MATCC_GNN_Model(
        input_dim=CONFIG['input_dim'],
        n_embd=CONFIG['n_embd'],
        n_layers=CONFIG['n_layers'],
        n_qubits=CONFIG['n_qubits'],
        num_nodes=num_nodes,
        adj_matrix=adj_matrix,
        gnn_embd=CONFIG.get('gnn_embd', 64),
    ).to(CONFIG['device'])

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {total_params:,}")

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'], betas=(0.9, 0.999), eps=1e-8)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6)
    
    # ã€æ··åˆç²¾åº¦è®­ç»ƒã€‘åˆå§‹åŒ– GradScalerï¼ˆç”¨äº FP16 è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾ï¼‰
    # AMP ä¼šè‡ªåŠ¨å°†ç»å…¸æ¨¡å—è½¬æ¢ä¸º FP16ï¼Œé‡å­æ¨¡å—ä¿æŒ FP32
    use_amp = CONFIG.get('use_amp', False)
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    if use_amp:
        print("   âœ… å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)ï¼šç»å…¸æ¨¡å—ä½¿ç”¨ FP16ï¼Œé‡å­æ¨¡å—ä¿æŒ FP32")
    
    # ã€JIT ç¼–è¯‘ä¼˜åŒ–ã€‘å¯é€‰ï¼šä½¿ç”¨ torch.compile åŠ é€Ÿæ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
    # æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘æ—¶é—´ï¼Œä¸”å¯èƒ½ä¸æŸäº›è‡ªå®šä¹‰æ“ä½œï¼ˆå¦‚é‡å­çº¿è·¯ï¼‰ä¸å…¼å®¹
    if CONFIG.get('use_compile', False) and hasattr(torch, 'compile'):
        try:
            model = torch.compile(model, mode='reduce-overhead')
            print("   âœ… å·²å¯ç”¨ torch.compile åŠ é€Ÿ")
        except Exception as e:
            print(f"   âš ï¸ torch.compile å¯ç”¨å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨æœªç¼–è¯‘ç‰ˆæœ¬")

    # ================= 6. è®­ç»ƒå¾ªç¯ =================
    train_losses, val_losses = [], []
    best_val_loss = float('inf')
    best_metrics_epoch = None
    early_stop_counter = 0
    early_stop_patience = CONFIG['early_stop_patience']

    print("\n>>> Start Training (QL-MATCC-GNN)...")
    print(">>> Dataset å·²æä¾› node_indicesï¼Œå°†ç”¨å…¶ä»é‚»æ¥çŸ©é˜µåˆ‡ç‰‡ batch å†…å­å›¾åš GNN èšåˆã€‚")
    print("=" * 60)

    for epoch in range(CONFIG['epochs']):
        model.train()
        epoch_train_loss = 0.0
        num_batches = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']} [Train]", ncols=100)

        for batch_idx, batch in enumerate(progress_bar):
            x = batch['x'].to(CONFIG['device'], non_blocking=True)
            y = batch['y'].to(CONFIG['device'], non_blocking=True)
            vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
            # Dataset å·²è¿”å› node_indices (B,)ï¼Œç”¨äºä»å…¨å›¾é‚»æ¥çŸ©é˜µåˆ‡ç‰‡ batch å†…å­å›¾
            node_indices = batch.get('node_indices')
            if node_indices is not None:
                node_indices = node_indices.to(CONFIG['device'], non_blocking=True)

            optimizer.zero_grad()
            
            # ã€æ··åˆç²¾åº¦è®­ç»ƒã€‘ä½¿ç”¨ autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            # åœ¨ autocast å†…çš„æ“ä½œä¼šè‡ªåŠ¨é€‰æ‹© FP16/FP32ï¼ˆé‡å­æ¨¡å—ä¼šä¿æŒ FP32ï¼‰
            if use_amp:
                with torch.cuda.amp.autocast():
                    preds = model(x, vol, node_indices=node_indices)
                    loss = criterion(preds, y)
                # ä½¿ç”¨ scaler è¿›è¡Œåå‘ä¼ æ’­å’Œæ¢¯åº¦ç¼©æ”¾ï¼ˆé˜²æ­¢ FP16 ä¸‹æ¢¯åº¦ä¸‹æº¢ï¼‰
                scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ªï¼ˆåœ¨ scaler ç¼©æ”¾åçš„æ¢¯åº¦ä¸Šè¿›è¡Œï¼‰
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()  # æ›´æ–° scaler çš„ç¼©æ”¾å› å­
            else:
                # æ ‡å‡† FP32 è®­ç»ƒæµç¨‹
                preds = model(x, vol, node_indices=node_indices)
                loss = criterion(preds, y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nâŒ NaN/Inf at batch {batch_idx}")
                return

            epoch_train_loss += loss.item()
            num_batches += 1
            progress_bar.set_postfix(loss=f'{loss.item():.6f}', avg=f'{epoch_train_loss/num_batches:.6f}')

        avg_train = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train)

        # ---------- éªŒè¯ï¼ˆæ”¶é›†é¢„æµ‹å€¼å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰----------
        model.eval()
        epoch_val = 0.0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(CONFIG['device'], non_blocking=True)
                y = batch['y'].to(CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(CONFIG['device'], non_blocking=True)
                
                # ã€æ··åˆç²¾åº¦éªŒè¯ã€‘éªŒè¯æ—¶ä¹Ÿä½¿ç”¨ autocast ä»¥ä¿æŒä¸€è‡´æ€§
                if use_amp:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                epoch_val += criterion(preds, y).item()
                
                # æ”¶é›†é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼ˆç”¨äºè®¡ç®—å®Œæ•´æŒ‡æ ‡ï¼‰
                all_preds.append(preds.cpu().numpy())
                all_targets.append(y.cpu().numpy())
        
        avg_val = epoch_val / len(test_loader)
        val_losses.append(avg_val)
        
        # è®¡ç®—å®Œæ•´è¯„ä¼°æŒ‡æ ‡ï¼ˆä»…åœ¨æœ€ä½³epochæ—¶è®¡ç®—ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        if avg_val < best_val_loss:
            all_preds_np = np.concatenate(all_preds, axis=0)
            all_targets_np = np.concatenate(all_targets, axis=0)
            
            # è®¡ç®—æŒ‡æ ‡
            y_true = all_targets_np.flatten()
            y_pred = all_preds_np.flatten()
            
            # ========== 1. ç»Ÿè®¡è¯¯å·®ç±» ==========
            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_true, y_pred)
            
            # MAPE
            mask = np.abs(y_true) > 1e-8
            if np.sum(mask) > 0:
                mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
            else:
                mape = None
            
            # ========== 2. æ–¹å‘é¢„æµ‹ç±» ==========
            true_direction = np.sign(y_true)
            pred_direction = np.sign(y_pred)
            directional_accuracy = np.mean(true_direction == pred_direction)
            
            # ========== 3. é‡åŒ–æŠ•èµ„ç±» ==========
            # IC (Information Coefficient) - Pearson ç›¸å…³ç³»æ•°
            try:
                ic, ic_pvalue = pearsonr(y_pred, y_true)
                ic = float(ic)
            except:
                ic = None
                ic_pvalue = None
            
            # RankIC (Rank Information Coefficient) - Spearman ç§©ç›¸å…³ç³»æ•°
            try:
                rank_ic, rank_ic_pvalue = spearmanr(y_pred, y_true)
                rank_ic = float(rank_ic)
            except:
                rank_ic = None
                rank_ic_pvalue = None
            
            # ä¼ ç»Ÿç›¸å…³ç³»æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
            try:
                correlation = np.corrcoef(y_true, y_pred)[0, 1]
                correlation = float(correlation)
            except:
                correlation = None
            
            best_metrics = {
                # ç»Ÿè®¡è¯¯å·®ç±»
                'mse': float(mse),
                'mae': float(mae),
                'rmse': float(rmse),
                'r2': float(r2),
                'mape': float(mape) if mape is not None else None,
                
                # æ–¹å‘é¢„æµ‹ç±»
                'directional_accuracy': float(directional_accuracy),
                
                # é‡åŒ–æŠ•èµ„ç±»
                'ic': ic,
                'ic_pvalue': float(ic_pvalue) if ic_pvalue is not None else None,
                'rank_ic': rank_ic,
                'rank_ic_pvalue': float(rank_ic_pvalue) if rank_ic_pvalue is not None else None,
                
                # å…¼å®¹æ€§æŒ‡æ ‡
                'correlation': correlation,
            }
        else:
            best_metrics = None

        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(avg_val)
        cur_lr = optimizer.param_groups[0]['lr']

        print(f"\nEpoch {epoch+1}/{CONFIG['epochs']}: Train={avg_train:.6f}, Val={avg_val:.6f}, lr={cur_lr:.2e}" + (" â¬‡ï¸" if cur_lr != old_lr else ""))

        if avg_val < best_val_loss:
            best_val_loss = avg_val
            best_metrics_epoch = best_metrics  # ä¿å­˜æœ€ä½³epochçš„æŒ‡æ ‡
            save_path = os.path.join(current_dir, 'best_model_gnn.pth')
            torch.save(model.state_dict(), save_path)
            if best_metrics:
                print(f"  ğŸŒŸ Best model saved: {save_path}")
                print(f"     Metrics: RÂ²={best_metrics['r2']:.4f}, MAE={best_metrics['mae']:.6f}, "
                      f"DirAcc={best_metrics['directional_accuracy']:.2%}, Corr={best_metrics['correlation']:.4f}")
            else:
                print(f"  ğŸŒŸ Best model saved: {save_path}")
            early_stop_counter = 0
        else:
            early_stop_counter += 1

        if early_stop_counter >= early_stop_patience:
            print(f"\nğŸ›‘ Early stopping (best val loss: {best_val_loss:.6f})")
            break
        print("-" * 60)

    # ================= 7. æ›²çº¿ä¸æ€»ç»“ =================
    curve_path = os.path.join(current_dir, 'training_curve_gnn.png')
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss', lw=2)
    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss', lw=2)
    if val_losses:
        be = val_losses.index(best_val_loss) + 1
        plt.plot(be, best_val_loss, 'g*', markersize=14, label=f'Best (Epoch {be})')
    plt.title('QL-MATCC-GNN Training', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(curve_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f">>> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")
    
    # ================= 8. æœ€ç»ˆè¯„ä¼°ï¼ˆåœ¨æœ€ä½³æ¨¡å‹ä¸Šè®¡ç®—å®Œæ•´æŒ‡æ ‡ï¼‰=================
    # å¦‚æœæœ€ä½³epochæ—¶æ²¡æœ‰è®¡ç®—æŒ‡æ ‡ï¼Œç°åœ¨é‡æ–°è®¡ç®—
    if best_metrics_epoch is None:
        print(">>> é‡æ–°è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡...")
        model.load_state_dict(torch.load(os.path.join(current_dir, 'best_model_gnn.pth')))
        model.eval()
        all_preds_final = []
        all_targets_final = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(CONFIG['device'], non_blocking=True)
                y = batch['y'].to(CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(CONFIG['device'], non_blocking=True)
                
                if use_amp:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                all_preds_final.append(preds.cpu().numpy())
                all_targets_final.append(y.cpu().numpy())
        
        all_preds_final_np = np.concatenate(all_preds_final, axis=0)
        all_targets_final_np = np.concatenate(all_targets_final, axis=0)
        
        y_true = all_targets_final_np.flatten()
        y_pred = all_preds_final_np.flatten()
        
        # ========== 1. ç»Ÿè®¡è¯¯å·®ç±» ==========
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        mask = np.abs(y_true) > 1e-8
        if np.sum(mask) > 0:
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = None
        
        # ========== 2. æ–¹å‘é¢„æµ‹ç±» ==========
        true_direction = np.sign(y_true)
        pred_direction = np.sign(y_pred)
        directional_accuracy = np.mean(true_direction == pred_direction)
        
        # ========== 3. é‡åŒ–æŠ•èµ„ç±» ==========
        # IC (Information Coefficient)
        try:
            ic, ic_pvalue = pearsonr(y_pred, y_true)
            ic = float(ic)
        except:
            ic = None
            ic_pvalue = None
        
        # RankIC (Rank Information Coefficient)
        try:
            rank_ic, rank_ic_pvalue = spearmanr(y_pred, y_true)
            rank_ic = float(rank_ic)
        except:
            rank_ic = None
            rank_ic_pvalue = None
        
        # ä¼ ç»Ÿç›¸å…³ç³»æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
        try:
            correlation = np.corrcoef(y_true, y_pred)[0, 1]
            correlation = float(correlation)
        except:
            correlation = None
        
        best_metrics_epoch = {
            # ç»Ÿè®¡è¯¯å·®ç±»
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(rmse),
            'r2': float(r2),
            'mape': float(mape) if mape is not None else None,
            
            # æ–¹å‘é¢„æµ‹ç±»
            'directional_accuracy': float(directional_accuracy),
            
            # é‡åŒ–æŠ•èµ„ç±»
            'ic': ic,
            'ic_pvalue': float(ic_pvalue) if ic_pvalue is not None else None,
            'rank_ic': rank_ic,
            'rank_ic_pvalue': float(rank_ic_pvalue) if rank_ic_pvalue is not None else None,
            
            # å…¼å®¹æ€§æŒ‡æ ‡
            'correlation': correlation,
        }
    
    # ================= 9. ä¿å­˜ Loss æ•°å€¼åˆ—è¡¨å’Œè¯„ä¼°æŒ‡æ ‡ =================
    loss_data_path = os.path.join(current_dir, 'training_losses_gnn.json')
    loss_data = {
        'experiment_name': 'full_model_gnn',
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss,
        'best_epoch': val_losses.index(best_val_loss) + 1 if val_losses else 0,
        'total_epochs': len(train_losses),
        'metrics': best_metrics_epoch,  # æ·»åŠ å®Œæ•´è¯„ä¼°æŒ‡æ ‡
        'config': {
            'batch_size': CONFIG['batch_size'],
            'lr': CONFIG['lr'],
            'epochs': CONFIG['epochs'],
            'n_embd': CONFIG['n_embd'],
            'n_layers': CONFIG['n_layers'],
        }
    }
    with open(loss_data_path, 'w') as f:
        json.dump(loss_data, f, indent=2)
    print(f">>> Loss æ•°å€¼åˆ—è¡¨å’Œè¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {loss_data_path}")

    print("\n" + "=" * 60)
    print(">>> QL-MATCC-GNN è®­ç»ƒç»“æŸ")
    print(f"    Best Val Loss: {best_val_loss:.6f}")
    if best_metrics_epoch:
        print("\n    ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
        print(f"    ã€ç»Ÿè®¡è¯¯å·®ç±»ã€‘")
        print(f"      RÂ² Score: {best_metrics_epoch['r2']:.4f}")
        print(f"      MAE: {best_metrics_epoch['mae']:.6f}")
        print(f"      RMSE: {best_metrics_epoch['rmse']:.6f}")
        print(f"    ã€æ–¹å‘é¢„æµ‹ç±»ã€‘")
        print(f"      Directional Accuracy: {best_metrics_epoch['directional_accuracy']:.2%}")
        print(f"    ã€é‡åŒ–æŠ•èµ„ç±»ã€‘")
        if best_metrics_epoch.get('ic') is not None:
            print(f"      IC (Information Coefficient): {best_metrics_epoch['ic']:.4f}")
        if best_metrics_epoch.get('rank_ic') is not None:
            print(f"      RankIC (Rank Information Coefficient): {best_metrics_epoch['rank_ic']:.4f}")
    print(f"\n    Model: best_model_gnn.pth")
    print(f"    Loss Data: training_losses_gnn.json")
    print("=" * 60)


if __name__ == "__main__":
    main()
