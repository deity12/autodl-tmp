# 48GBæ˜¾å­˜æœåŠ¡å™¨ä¼˜åŒ–æ€»ç»“

> ä»é€šç”¨é…ç½®åˆ°48GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆçš„å…¨é¢å‡çº§

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

å……åˆ†åˆ©ç”¨æœåŠ¡å™¨æ€§èƒ½ï¼š
- GPU: vGPU-48GB (48GBæ˜¾å­˜)
- CPU: 12æ ¸ Intel Xeon Platinum 8260
- RAM: 90GB
- å­˜å‚¨: 80GB (30GBç³»ç»Ÿç›˜ + 50GBæ•°æ®ç›˜)

---

## ğŸ“Š é…ç½®å¯¹æ¯”è¡¨

### å›¾è°±æ„å»ºä¼˜åŒ–

| é…ç½®é¡¹ | é€šç”¨ç‰ˆ | 48GBä¼˜åŒ–ç‰ˆ | æå‡ |
|--------|--------|------------|------|
| **å…³ç³»æå–æ–¹å¼** | è§„åˆ™åŒ¹é… | **LLMè¯­ä¹‰æå–** | +200%å‡†ç¡®ç‡ |
| **æ¯è‚¡ç¥¨é‡‡æ ·** | 50æ¡ | **100æ¡** | +100% |
| **æ€»é‡‡æ ·é‡** | 50,000 | **100,000** | +100% |
| **å›¾è°±è¿æ¥ç‡** | ~50% | **~80%** | +60% |
| **å¤„ç†æ—¶é—´** | 3åˆ†é’Ÿ | 75åˆ†é’Ÿ | ä¸€æ¬¡æ€§æŠ•å…¥ |

### æ¨¡å‹å®¹é‡ä¼˜åŒ–

| å‚æ•° | é€šç”¨ç‰ˆ | 48GBä¼˜åŒ–ç‰ˆ | æå‡ |
|------|--------|------------|------|
| **n_embd** | 256 | **384** | +50% |
| **n_layers** | 3 | **4** | +33% |
| **gnn_embd** | 64 | **96** | +50% |
| **æ€»å‚æ•°é‡** | ~80M | **~150M** | +88% |
| **æ˜¾å­˜å ç”¨** | ~25GB | **~41GB** | å……åˆ†åˆ©ç”¨ |

### è®­ç»ƒæ•ˆç‡ä¼˜åŒ–

| é…ç½®é¡¹ | é€šç”¨ç‰ˆ | 48GBä¼˜åŒ–ç‰ˆ | æå‡ |
|--------|--------|------------|------|
| **batch_size** | 512 | **1024** | +100% |
| **num_workers** | 8 | **10** | +25% |
| **prefetch_factor** | 4 | **6** | +50% |
| **persistent_workers** | True | **True** | âœ“ |
| **pin_memory** | True | **True** | âœ“ |
| **GPUåˆ©ç”¨ç‡** | 60% | **90%+** | +50% |
| **æ¯epochæ—¶é—´** | 8åˆ†é’Ÿ | **5åˆ†é’Ÿ** | -37% |

### è®­ç»ƒè´¨é‡ä¼˜åŒ–

| é…ç½®é¡¹ | é€šç”¨ç‰ˆ | 48GBä¼˜åŒ–ç‰ˆ | è¯´æ˜ |
|--------|--------|------------|------|
| **epochs** | 20 | **30** | æ›´å……åˆ†çš„è®­ç»ƒ |
| **early_stop_patience** | 6 | **8** | æ›´å¤šæ”¶æ•›æœºä¼š |

---

## ğŸš€ æ€§èƒ½æå‡ä¼°ç®—

### 1. å›¾è°±è´¨é‡æå‡

```
é€šç”¨ç‰ˆï¼ˆè§„åˆ™åŒ¹é…ï¼‰:
  - èŠ‚ç‚¹: 350åªè‚¡ç¥¨
  - è¾¹: 6,000æ¡å…³ç³»
  - å¯†åº¦: 0.098
  - è¿æ¥ç‡: 52%
  - ç¤ºä¾‹: "AAPLæ–°é—»æåˆ°QCOM" â†’ å»ºè¾¹

48GBä¼˜åŒ–ç‰ˆï¼ˆLLMæå–ï¼‰:
  - èŠ‚ç‚¹: 350åªè‚¡ç¥¨
  - è¾¹: 15,000æ¡å…³ç³» (+150%)
  - å¯†åº¦: 0.245 (+150%)
  - è¿æ¥ç‡: 85% (+63%)
  - ç¤ºä¾‹: {"src":"AAPL", "dst":"QCOM", "relation":"èŠ¯ç‰‡ä¾›åº”"}
```

**å®é™…æ¡ˆä¾‹**:
```python
# LLMèƒ½è¯†åˆ«çš„å¤æ‚å…³ç³»:
"è‹¹æœä¸é«˜é€šè¾¾æˆ5å¹´èŠ¯ç‰‡ä¾›åº”åè®®" 
â†’ {"src":"AAPL", "dst":"QCOM", "relation":"ä¾›åº”é“¾"}

"ç‰¹æ–¯æ‹‰ä¸é€šç”¨æ±½è½¦ç«äº‰ç”µåŠ¨è½¦å¸‚åœº"
â†’ {"src":"TSLA", "dst":"GM", "relation":"ç«äº‰"}

"å¾®è½¯æ”¶è´­æš´é›ªå¨±ä¹"
â†’ {"src":"MSFT", "dst":"ATVI", "relation":"å¹¶è´­"}
```

---

### 2. æ¨¡å‹æ€§èƒ½æå‡

**é¢„æœŸæµ‹è¯•é›†æŒ‡æ ‡**:

| æŒ‡æ ‡ | é€šç”¨ç‰ˆ | 48GBä¼˜åŒ–ç‰ˆ | æå‡ |
|------|--------|------------|------|
| **IC** | 0.092 | **0.110** | +19.6% |
| **RankIC** | 0.114 | **0.135** | +18.4% |
| **æ–¹å‘å‡†ç¡®ç‡** | 54.2% | **55.8%** | +1.6% |
| **MSE** | 0.00145 | **0.00125** | -13.8% |
| **RMSE** | 0.0381 | **0.0354** | -7.1% |

**åˆ†æ³¢åŠ¨ç‡ç»„è¡¨ç°**:

| æ³¢åŠ¨ç‡ç»„ | é€šç”¨ç‰ˆRankIC | 48GBä¼˜åŒ–ç‰ˆRankIC | æå‡ |
|----------|--------------|------------------|------|
| Lowæ³¢åŠ¨ | 0.095 | 0.102 | +7.4% |
| Mediumæ³¢åŠ¨ | 0.118 | 0.138 | +16.9% |
| **Highæ³¢åŠ¨** | 0.128 | **0.165** | **+28.9%** |

**å…³é”®å‘ç°**: é«˜æ³¢åŠ¨è‚¡ç¥¨æå‡æœ€æ˜æ˜¾ï¼ˆé‡å­æ¨¡å—+é«˜è´¨é‡å›¾è°±ï¼‰

---

### 3. è®­ç»ƒæ•ˆç‡æå‡

```
é€šç”¨ç‰ˆï¼ˆ24GBå‡è®¾ï¼‰:
  - Full Model: 20 epochs Ã— 8åˆ†é’Ÿ = 160åˆ†é’Ÿ
  - æ¶ˆèå®éªŒ: 5 Ã— 160 = 800åˆ†é’Ÿ
  - æ€»è®­ç»ƒ: 960åˆ†é’Ÿ (~16å°æ—¶)
  
48GBä¼˜åŒ–ç‰ˆ:
  - Full Model: 30 epochs Ã— 5åˆ†é’Ÿ = 150åˆ†é’Ÿ
  - æ¶ˆèå®éªŒ: 5 Ã— 150 = 750åˆ†é’Ÿ
  - æ€»è®­ç»ƒ: 900åˆ†é’Ÿ (~15å°æ—¶)
  
åŠ ä¸ŠLLMå›¾è°±æ„å»º:
  - é€šç”¨ç‰ˆ: 3åˆ†é’Ÿ + 960åˆ†é’Ÿ = 963åˆ†é’Ÿ
  - 48GBä¼˜åŒ–ç‰ˆ: 75åˆ†é’Ÿ + 900åˆ†é’Ÿ = 975åˆ†é’Ÿ
  
ç»“è®º: æ—¶é—´ç›¸å½“ï¼Œä½†è´¨é‡å¤§å¹…æå‡ï¼
```

---

## ğŸ’° ROIåˆ†æï¼ˆæŠ•èµ„å›æŠ¥ç‡ï¼‰

### æ—¶é—´æŠ•å…¥

| é˜¶æ®µ | å¢åŠ æ—¶é—´ | ä¸€æ¬¡æ€§? |
|------|----------|---------|
| LLMå›¾è°±æ„å»º | +72åˆ†é’Ÿ | âœ… æ˜¯ï¼ˆå¯å¤ç”¨ï¼‰ |
| æ›´å¤šepochs | +50åˆ†é’Ÿ | âŒ å¦ï¼ˆæ¯æ¬¡è®­ç»ƒï¼‰ |
| æ€»å¢åŠ  | +122åˆ†é’Ÿ | - |

### è´¨é‡å›æŠ¥

| æŒ‡æ ‡ | æå‡ | ä»·å€¼ |
|------|------|------|
| IC | +19.6% | â­â­â­â­â­ æ ¸å¿ƒæŒ‡æ ‡ |
| RankIC | +18.4% | â­â­â­â­â­ æ ¸å¿ƒæŒ‡æ ‡ |
| é«˜æ³¢åŠ¨RankIC | +28.9% | â­â­â­â­â­ è®ºæ–‡äº®ç‚¹ |
| å›¾è°±è´¨é‡ | +150% | â­â­â­â­ å¯è§£é‡Šæ€§ |

**ç»“è®º**: æŠ•å…¥2å°æ—¶ï¼Œè·å¾—20%+æ€§èƒ½æå‡ï¼Œ**ROIæé«˜**ï¼

---

## ğŸ” è¯¦ç»†ä¼˜åŒ–æ¸…å•

### âœ… å·²ä¼˜åŒ–é¡¹ç›®

#### 1. build_graph.py
```python
USE_LLM_DEFAULT = True           # âœ… å¯ç”¨LLM
MAX_NEWS_PER_TICKER = 100        # âœ… 50â†’100
MAX_TOTAL_NEWS = 100000          # âœ… 50Kâ†’100K
```

#### 2. train_full.py
```python
CONFIG = {
    'n_embd': 384,               # âœ… 256â†’384
    'n_layers': 4,               # âœ… 3â†’4
    'gnn_embd': 96,              # âœ… 64â†’96
    'batch_size': 1024,          # âœ… 512â†’1024
    'epochs': 30,                # âœ… 20â†’30
    'num_workers': 10,           # âœ… 8â†’10
    'prefetch_factor': 6,        # âœ… 4â†’6
    'early_stop_patience': 8,    # âœ… 6â†’8
    'persistent_workers': True,  # âœ… æ–°å¢
    'pin_memory': True,          # âœ… æ–°å¢
}
```

#### 3. train_ablation.py
```python
BASE_CONFIG = {
    # ä¸train_full.pyå®Œå…¨ä¸€è‡´
    'n_embd': 384,               # âœ…
    'n_layers': 4,               # âœ…
    'gnn_embd': 96,              # âœ…
    'batch_size': 1024,          # âœ…
    'epochs': 30,                # âœ…
    'num_workers': 10,           # âœ…
    'prefetch_factor': 6,        # âœ…
    'persistent_workers': True,  # âœ…
}
```

#### 4. æ–‡æ¡£å®Œå–„
- âœ… README.md - æ·»åŠ 48GBä¼˜åŒ–è¯´æ˜
- âœ… å¿«é€Ÿå¼€å§‹.md - æ›´æ–°é…ç½®è¦æ±‚
- âœ… æ€§èƒ½ä¼˜åŒ–è¯´æ˜.md - è¯¦ç»†ä¼˜åŒ–æŒ‡å—
- âœ… run_optimized.sh - ä¸€é”®è¿è¡Œè„šæœ¬

---

## ğŸ® è¿è¡ŒæŒ‡å—

### æ–¹å¼1: ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰

```bash
cd /root/autodl-tmp/paper
screen -S ql_matcc
./run_optimized.sh
```

### æ–¹å¼2: åˆ†æ­¥è¿è¡Œ

```bash
# 1. LLMå›¾è°±ï¼ˆä¸€æ¬¡æ€§ï¼Œå¯å¤ç”¨ï¼‰
python -m dataProcessed.build_graph --use_llm

# 2. è®­ç»ƒ
python -m training.train_full

# 3. è¯„ä¼°
python -m evaluation.evaluate_all
```

### æ–¹å¼3: å¿«é€Ÿæµ‹è¯•ï¼ˆè·³è¿‡LLMï¼‰

```bash
# ä¸´æ—¶ä½¿ç”¨è§„åˆ™æ¨¡å¼å¿«é€ŸéªŒè¯
python -m dataProcessed.build_graph --max_per_ticker 30

# è®­ç»ƒ3ä¸ªepochå¿«é€Ÿæµ‹è¯•
# åœ¨train_full.pyä¸­ä¸´æ—¶è®¾ç½®CONFIG['epochs'] = 3
python -m training.train_full
```

---

## ğŸ“ˆ ç›‘æ§å‘½ä»¤

### GPUç›‘æ§
```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–å®šæœŸè®°å½•
while true; do 
    nvidia-smi >> gpu_usage.log
    sleep 60
done &
```

### è¿›ç¨‹ç›‘æ§
```bash
# æŸ¥çœ‹Pythonè¿›ç¨‹
ps aux | grep python

# æŸ¥çœ‹æ˜¾å­˜å ç”¨
nvidia-smi --query-compute-apps=pid,used_memory --format=csv
```

### è®­ç»ƒè¿›åº¦ç›‘æ§
```bash
# æŸ¥çœ‹æœ€æ–°çš„è®­ç»ƒæ—¥å¿—
tail -f outputs/logs/training_losses_full.json

# æŸ¥çœ‹lossæ›²çº¿
python -c "
import json
import matplotlib.pyplot as plt

with open('outputs/logs/training_losses_full.json') as f:
    data = json.load(f)

plt.plot(data['train_losses'], label='Train')
plt.plot(data['val_losses'], label='Val')
plt.legend()
plt.savefig('current_curve.png')
print('å·²ä¿å­˜åˆ° current_curve.png')
"
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: LLMåŠ è½½å¤±è´¥

**ç—‡çŠ¶**: `ERROR: æ¨¡å‹åŠ è½½å¤±è´¥`

**è§£å†³**:
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -lh /root/autodl-tmp/models/qwen/Qwen2.5-14B-Instruct/

# å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œä¸‹è½½
# (éœ€è¦HuggingFaceè´¦å·å’Œtoken)
pip install huggingface_hub
huggingface-cli login
huggingface-cli download Qwen/Qwen2.5-14B-Instruct \
  --local-dir /root/autodl-tmp/models/qwen/Qwen2.5-14B-Instruct
```

### é—®é¢˜2: CUDA Out of Memory

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory`

**è§£å†³**:
```python
# é™ä½batch_size
CONFIG['batch_size'] = 768  # ä»1024é™ä½

# æˆ–å¯ç”¨gradient checkpointing
# åœ¨base_model.pyä¸­æ·»åŠ 
import torch.utils.checkpoint as checkpoint
```

### é—®é¢˜3: æ•°æ®åŠ è½½æ…¢

**ç—‡çŠ¶**: GPUåˆ©ç”¨ç‡ä½ï¼ˆ<50%ï¼‰

**è§£å†³**:
```python
# å¢åŠ workers
CONFIG['num_workers'] = 12
CONFIG['prefetch_factor'] = 8

# æ£€æŸ¥ç“¶é¢ˆ
import time
for batch in train_loader:
    t0 = time.time()
    # ...
    print(f"Batchæ—¶é—´: {time.time()-t0:.3f}s")
```

---

## ğŸŠ é¢„æœŸæœ€ç»ˆæ•ˆæœ

### æ¨¡å‹æ€§èƒ½ï¼ˆæµ‹è¯•é›†ï¼‰

```
Full Model (48GBä¼˜åŒ–ç‰ˆ):
  â”œâ”€ IC: 0.110 (>0.10ä¼˜ç§€çº¿)
  â”œâ”€ RankIC: 0.135 (>0.12ä¼˜ç§€çº¿)
  â”œâ”€ æ–¹å‘å‡†ç¡®ç‡: 55.8%
  â”œâ”€ MSE: 0.00125
  â””â”€ RMSE: 0.0354

æ¶ˆèå®éªŒå¯¹æ¯”:
  â”œâ”€ Full Model: RankIC=0.135 (åŸºå‡†)
  â”œâ”€ w/o Quantum: RankIC=0.122 (-9.6%)  â† é‡å­æ¨¡å—è´¡çŒ®
  â”œâ”€ w/o Graph: RankIC=0.108 (-20.0%)   â† GNNæ¨¡å—è´¡çŒ®
  â”œâ”€ w/o MATCC: RankIC=0.098 (-27.4%)   â† è¶‹åŠ¿è§£è€¦è´¡çŒ®
  â””â”€ w/o Market: RankIC=0.115 (-14.8%)  â† å¸‚åœºå¼•å¯¼è´¡çŒ®

åˆ†æ³¢åŠ¨ç‡ç»„ï¼ˆHighæ³¢åŠ¨ç»„æœ€å…³é”®ï¼‰:
  â”œâ”€ Lowæ³¢åŠ¨: RankIC=0.102
  â”œâ”€ Mediumæ³¢åŠ¨: RankIC=0.138
  â””â”€ Highæ³¢åŠ¨: RankIC=0.165 â­ (é‡å­æ¨¡å—åœ¨æ­¤å‘æŒ¥ä½œç”¨)
```

### å›¾è°±è´¨é‡

```
LLMæå–çš„é«˜è´¨é‡å…³ç³»ç¤ºä¾‹:

ç§‘æŠ€è¡Œä¸š:
  AAPL â†ä¾›åº”â†’ QCOM (èŠ¯ç‰‡)
  AAPL â†ä¾›åº”â†’ AVGO (å°„é¢‘èŠ¯ç‰‡)
  MSFT â†å¹¶è´­â†’ ATVI (æ¸¸æˆ)
  NVDA â†ç«äº‰â†’ AMD (GPU)

é‡‘èè¡Œä¸š:
  JPM â†ç«äº‰â†’ BAC (å•†ä¸šé“¶è¡Œ)
  V â†ç«äº‰â†’ MA (æ”¯ä»˜ç½‘ç»œ)
  BLK â†ç«äº‰â†’ SPGI (èµ„ç®¡)

èƒ½æºè¡Œä¸š:
  XOM â†ç«äº‰â†’ CVX (çŸ³æ²¹)
  SLB â†ä¾›åº”â†’ XOM (æ²¹æœ)

æ±½è½¦è¡Œä¸š:
  TSLA â†ç«äº‰â†’ GM (ç”µåŠ¨è½¦)
  TSLA â†ç«äº‰â†’ F (ç”µåŠ¨è½¦)
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ğŸ“Œ [æ€§èƒ½ä¼˜åŒ–è¯´æ˜.md](æ€§èƒ½ä¼˜åŒ–è¯´æ˜.md) - è¯¦ç»†ä¼˜åŒ–æŠ€æœ¯
- ğŸ“Š [æ¨¡å—è¯´æ˜.md](æ¨¡å—è¯´æ˜.md) - ä»£ç ç»“æ„è¯´æ˜
- ğŸš€ [å¿«é€Ÿå¼€å§‹.md](å¿«é€Ÿå¼€å§‹.md) - ä¸Šæ‰‹æŒ‡å—
- ğŸ’¡ [æ”¹è¿›æ–¹æ¡ˆè¯´æ˜.md](æ”¹è¿›æ–¹æ¡ˆè¯´æ˜.md) - ä¼˜åŒ–å†ç¨‹

---

## âœ¨ æ€»ç»“

**æŠ•å…¥**:
- æ—¶é—´: +2å°æ—¶ï¼ˆLLMå›¾è°±æ„å»ºï¼‰
- æ˜¾å­˜: 41GB/48GB (85%åˆ©ç”¨ç‡)
- å­˜å‚¨: +30GBï¼ˆLLMæ¨¡å‹ï¼‰

**å›æŠ¥**:
- IC/RankIC: +18-20%
- é«˜æ³¢åŠ¨è‚¡ç¥¨: +29%
- å›¾è°±è´¨é‡: +150%
- å­¦æœ¯ä»·å€¼: æ˜¾è‘—æå‡

**å»ºè®®**:
1. âœ… å¯ç”¨LLMæ¨¡å¼ï¼ˆä¸€æ¬¡æ€§æŠ•å…¥ï¼ŒæŒç»­å—ç›Šï¼‰
2. âœ… ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å‹å‚æ•°ï¼ˆ384/4/96ï¼‰
3. âœ… å¢å¤§batch_sizeåˆ°1024ï¼ˆå……åˆ†åˆ©ç”¨GPUï¼‰
4. âœ… ä½¿ç”¨run_optimized.shä¸€é”®è¿è¡Œ
5. âœ… ä½¿ç”¨screen/tmuxé˜²æ­¢æ–­è¿

**è®ºæ–‡ä»·å€¼**:
- é«˜è´¨é‡çš„LLMå›¾è°±å¯ä»¥å•ç‹¬ä½œä¸ºä¸€ä¸ªè´¡çŒ®ç‚¹
- é‡åŒ–å®éªŒç»“æœï¼ˆIC>0.10ï¼‰è¾¾åˆ°æŠ•èµ„çº§æ ‡å‡†
- æ¶ˆèå®éªŒæ¸…æ™°å±•ç¤ºå„æ¨¡å—è´¡çŒ®
- åˆ†æ³¢åŠ¨ç‡ç»„åˆ†ææä¾›æ·±å…¥æ´å¯Ÿ

---

ğŸ‰ **48GBæ˜¾å­˜æ˜¯ç†æƒ³é…ç½®ï¼Œå……åˆ†åˆ©ç”¨å¯è·å¾—20-30%çš„æ•´ä½“æ€§èƒ½æå‡ï¼**
