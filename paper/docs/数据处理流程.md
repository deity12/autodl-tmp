# 数据处理流程详解

> ETL → 对齐 → 特征工程 → 图谱构建 → Dataset

## 流程概览

```
原始数据 (23GB+ 新闻 + 数千个股票CSV)
   ↓ [Step 1: ETL]
中间数据 (Stock_Prices.csv + Stock_News.csv)
   ↓ [Step 2: Align]
特征数据 (Final_Model_Data.csv + 技术指标)
   ↓ [Step 3: Build Graph]
图谱数据 (Graph_Adjacency.npy)
   ↓ [Step 4: Dataset]
模型输入 (PyTorch DataLoader)
```

---

## Step 1: ETL数据提取与转换

### 文件: `dataProcessed/etl.py`

### 1.1 股价数据合并

**输入**: `data/raw/FNSPID/full_history/*.csv` (数千个文件)

**问题**:
- 列名大小写不一致 (`Date` vs `date`)
- 需要统一时间范围
- 每个文件代表一只股票

**解决方案**:
```python
def merge_stock_prices():
    all_dfs = []
    for csv_file in tqdm(os.listdir(full_history_dir)):
        # 1. 提取Ticker (文件名去掉.csv)
        ticker = csv_file.replace('.csv', '').upper()
        
        # 2. 读取并标准化列名
        df = pd.read_csv(filepath)
        df.columns = [col.lower() for col in df.columns]
        df.rename(columns={...}, inplace=True)
        
        # 3. 过滤时间范围
        df = df[(df['Date'] >= '2019-01-01') & (df['Date'] <= '2023-12-31')]
        
        # 4. 添加Ticker列
        df['Ticker'] = ticker
        
        all_dfs.append(df)
    
    # 5. 合并
    merged = pd.concat(all_dfs, ignore_index=True)
    return merged
```

**输出**: `data/processed/Stock_Prices.csv`

**列**: `Date, Ticker, Open, High, Low, Close, Volume`

---

### 1.2 新闻数据分块处理

**输入**: `data/raw/FNSPID/nasdaq_exteral_data.csv` (23GB)

**问题**:
- 文件太大，无法一次性加载到内存
- 需要过滤时间和股票

**解决方案**:
```python
def process_news_chunked():
    # 1. 获取有效Ticker列表（来自股价数据）
    valid_tickers = set(price_df['Ticker'].unique())
    
    # 2. 分块读取 (每次10万行)
    chunks = []
    for chunk in pd.read_csv(news_path, chunksize=100000, low_memory=False):
        # 3. 过滤时间
        chunk['Date'] = pd.to_datetime(chunk['Date'])
        chunk = chunk[(chunk['Date'] >= '2019-01-01') & (chunk['Date'] <= '2023-12-31')]
        
        # 4. 过滤Ticker
        chunk = chunk[chunk['Ticker'].isin(valid_tickers)]
        
        # 5. 只保留关键列
        chunk = chunk[['Date', 'Ticker', 'Article_title', 'Publisher']]
        chunk.rename(columns={'Article_title': 'Headline'}, inplace=True)
        
        chunks.append(chunk)
    
    # 6. 合并所有块
    news_df = pd.concat(chunks, ignore_index=True)
    return news_df
```

**输出**: `data/processed/Stock_News.csv`

**列**: `Date, Ticker, Headline, Publisher`

**关键优势**:
- ✅ 内存占用: O(chunksize) 而非 O(file_size)
- ✅ 可处理任意大小文件
- ✅ 只保留有股价数据的新闻（避免后续对齐问题）

---

## Step 2: 数据对齐与特征工程

### 文件: `dataProcessed/align.py`

### 2.1 多源数据对齐

**输入**:
- `Stock_Prices.csv` - 股价数据
- `SP500_Index.csv` - 市场指数
- `Stock_News.csv` - 新闻数据

**对齐策略**:
```python
# 1. 主表: 股价数据 (确保每个交易日都被保留)
df = pd.read_csv('Stock_Prices.csv')

# 2. 左连接: 市场指数 (按Date)
market = pd.read_csv('SP500_Index.csv')
df = df.merge(market, on='Date', how='left')

# 3. 左连接: 新闻数据 (按Date+Ticker)
news = pd.read_csv('Stock_News.csv')

# 3.1 聚合同一天的多条新闻
news_agg = news.groupby(['Date', 'Ticker'])['Headline'].apply(
    lambda x: '|'.join(x)
).reset_index()

df = df.merge(news_agg, on=['Date', 'Ticker'], how='left')
```

**为什么用Left Join?**
- 保证每个`(Date, Ticker)`组合都有记录
- 市场指数缺失 → 前向填充
- 新闻缺失 → 填空字符串

---

### 2.2 技术指标计算

#### 对数收益率 (Log Return)

```python
# 公式
Log_Ret_t = log(Close_t / Close_{t-1})

# 实现
df['Log_Close'] = np.log(df['Close'])
df['Log_Ret'] = df.groupby('Ticker')['Log_Close'].diff()
```

**为什么用对数收益率?**
- ✅ 对称性: log(2)≈0.693, log(0.5)≈-0.693
- ✅ 可加性: log(P_t/P_0) = Σ log(P_i/P_{i-1})
- ✅ 近似正态分布

#### 滚动波动率 (Rolling Volatility)

```python
# 公式: 20日滚动标准差
Volatility_20d_t = std(Log_Ret_{t-19}, ..., Log_Ret_t)

# 实现
df['Volatility_20d'] = df.groupby('Ticker')['Log_Ret'].transform(
    lambda x: x.rolling(window=20, min_periods=20).std()
)
```

**为什么用20日?**
- ✅ 金融学标准: 约1个月的交易日
- ✅ 平衡: 短期波动 vs 长期趋势

---

### 2.3 缺失值处理

```python
# 1. 市场指数缺失: 前向填充
df[['Market_Close', 'Market_Vol']] = df[['Market_Close', 'Market_Vol']].fillna(method='ffill')

# 2. 首行填0 (前向填充无法处理)
df[['Market_Close', 'Market_Vol']] = df[['Market_Close', 'Market_Vol']].fillna(0)

# 3. 新闻缺失: 填空字符串
df['Headline'] = df['Headline'].fillna('')

# 4. 波动率前20行: 填0
df['Volatility_20d'] = df['Volatility_20d'].fillna(0)
```

**输出**: `data/processed/Final_Model_Data.csv`

**完整列**:
```
Date, Ticker, 
Open, High, Low, Close, Volume,     # 股价数据
Market_Close, Market_Vol,           # 市场数据
Headline,                            # 新闻数据
Log_Ret, Volatility_20d              # 计算特征
```

---

## Step 3: 动态语义图谱构建

### 文件: `dataProcessed/build_graph.py`

### 3.1 S&P 500成分股过滤

**动机**: 
- 全量7691只股票 → 图谱过于稀疏
- S&P 500成分股 (~300-500只) → 新闻质量高，关系真实

```python
# 1. 读取所有股票
all_tickers = sorted(df_price['Ticker'].unique())

# 2. 过滤S&P 500成分股
sp500_in_data = [t for t in all_tickers if t in SP500_TICKERS]

# 3. 只使用这些股票
tickers = sp500_in_data
df_price = df_price[df_price['Ticker'].isin(tickers)]
df_news = df_news[df_news['Ticker'].isin(tickers)]
```

**S&P 500成分股集合**: 约500只，定义在`SP500_TICKERS` set中

---

### 3.2 分层采样策略 ⭐

**问题**: 
- 原始`Stock_News.csv`按Ticker字母序排序
- 直接取前N条 → 只有A, AA, AAAU等少数股票
- 导致图谱严重不平衡

**解决方案**: 分层采样
```python
def stratified_sample_news(df_news, max_per_ticker=50, max_total=50000):
    sampled_dfs = []
    
    # 1. 对每个Ticker单独采样
    for ticker in df_news['Ticker'].unique():
        ticker_news = df_news[df_news['Ticker'] == ticker]
        
        # 至多采样max_per_ticker条
        n_sample = min(len(ticker_news), max_per_ticker)
        sampled = ticker_news.sample(n=n_sample, random_state=42)
        
        sampled_dfs.append(sampled)
    
    # 2. 合并
    df_sampled = pd.concat(sampled_dfs, ignore_index=True)
    
    # 3. 如果总数超过上限，再次随机采样
    if len(df_sampled) > max_total:
        df_sampled = df_sampled.sample(n=max_total, random_state=42)
    
    # 4. 全局打乱（关键！）
    df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)
    
    return df_sampled
```

**效果对比**:
| 方法 | A股票新闻 | MSFT股票新闻 | TSLA股票新闻 | 覆盖股票数 |
|------|-----------|--------------|--------------|------------|
| 顺序取前2000条 | 1800条 | 0条 | 0条 | 5只 |
| 分层采样 | 50条 | 50条 | 50条 | 300+只 |

---

### 3.3 关系提取 (两种模式)

#### 模式1: 规则匹配 (默认)

```python
def build_graph_rule_based(news_df, tickers):
    adj_matrix = np.eye(len(tickers))  # 初始化为单位阵（自环）
    
    for _, row in news_df.iterrows():
        src_ticker = row['Ticker']
        content = row['Headline']
        
        # 检查新闻中是否提到其他股票
        for dst_ticker in tickers:
            if dst_ticker != src_ticker and len(dst_ticker) >= 3:
                # 要求作为独立单词出现（避免误匹配）
                if dst_ticker.upper() in content.upper():
                    i, j = ticker2idx[src_ticker], ticker2idx[dst_ticker]
                    adj_matrix[i, j] = 1.0
                    adj_matrix[j, i] = 1.0  # 无向图
    
    return adj_matrix
```

**优点**:
- ✅ 快速（1-3分钟）
- ✅ 无需GPU
- ✅ 适合大规模实验

**缺点**:
- ⚠️ 可能误匹配（如 "APPLE" 匹配到 "AAPL"）
- ⚠️ 只能识别显式提及

---

#### 模式2: LLM提取 (--use_llm)

```python
def extract_relations_with_llm(news_text, model, tokenizer):
    prompt = f"""
请从以下财经新闻中提取公司之间的显式关系（如：供应、竞争、合作、母子公司、诉讼）。
新闻内容：{news_text[:500]}

请严格按以下 JSON 格式返回列表：
[{{"src": "公司A股票代码", "dst": "公司B股票代码", "relation": "关系类型"}}]
如果无明确关系，返回 []。
"""
    
    # 使用 Qwen2.5-14B-Instruct
    response = model.generate(prompt, ...)
    relations = json.loads(response)
    
    return relations
```

**优点**:
- ✅ 准确（语义理解）
- ✅ 可识别隐式关系
- ✅ 带关系类型

**缺点**:
- ⚠️ 慢（30-60分钟）
- ⚠️ 需要24GB+显存

---

### 3.4 累积式演化图谱

```python
# 1. 只使用训练集时段内的新闻
split_date = unique_dates[int(len(unique_dates) * 0.8)]
df_news = df_news[df_news['Date'] < split_date]

# 2. 关系随时间累积
adj_matrix = np.eye(num_nodes)
for date in sorted(unique_dates):
    news_today = df_news[df_news['Date'] == date]
    relations = extract_relations(news_today)
    
    # 累积: 一旦有关系就保留
    for (i, j) in relations:
        adj_matrix[i, j] = 1.0
        adj_matrix[j, i] = 1.0
```

**关键**: 避免Look-ahead Bias（未来信息泄露）

---

### 3.5 图谱质量评估

```python
# 计算统计指标
total_edges = (adj_matrix.sum() - num_nodes) / 2
density = total_edges / (num_nodes * (num_nodes - 1) / 2)
degrees = adj_matrix.sum(axis=1) - 1
non_isolated = np.sum(degrees > 0)

print(f"节点数: {num_nodes}")
print(f"边数: {int(total_edges)}")
print(f"密度: {density:.6f}")
print(f"有连接的股票: {non_isolated} / {num_nodes} ({non_isolated/num_nodes*100:.1f}%)")
print(f"平均度: {degrees.mean():.2f}")
```

**期望结果** (S&P 500 + 分层采样):
```
节点数: 350
边数: 8500
密度: 0.139
有连接的股票: 320 / 350 (91.4%)
平均度: 48.6
```

**输出**: `data/processed/Graph_Adjacency.npy` (N×N邻接矩阵)

---

## Step 4: PyTorch Dataset构建

### 文件: `dataProcessed/dataset.py`

### 4.1 数据清洗与异常值处理

```python
def clean_data(df):
    # 1. 裁剪异常值
    df['Log_Ret'] = df['Log_Ret'].clip(-0.5, 0.5)      # ±50%
    df['Volatility_20d'] = df['Volatility_20d'].clip(0, 2.0)
    
    # 2. 处理无穷大
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # 3. 填充缺失值
    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 
                    'Market_Close', 'Market_Vol', 'Log_Ret', 'Volatility_20d']
    df[numeric_cols] = df[numeric_cols].fillna(method='ffill')
    
    # 4. 删除仍有缺失的行
    df = df.dropna(subset=numeric_cols)
    
    return df
```

---

### 4.2 训练/测试划分

```python
# 按时间80/20划分
unique_dates = sorted(df['Date'].unique())
split_idx = int(len(unique_dates) * 0.8)
split_date = unique_dates[split_idx]

train_df = df[df['Date'] < split_date]
test_df = df[df['Date'] >= split_date]
```

**为什么按时间划分?**
- ✅ 避免数据泄露
- ✅ 符合真实预测场景
- ❌ 不能随机划分！

---

### 4.3 特征标准化

```python
# 训练集: fit + transform
scaler = StandardScaler()
train_features = scaler.fit_transform(train_df[feature_cols])

# 测试集: 只transform（使用训练集的均值和方差）
test_features = scaler.transform(test_df[feature_cols])
```

**可选**: 使用`RobustScaler`（对异常值更鲁棒）
```python
scaler = RobustScaler(quantile_range=(10, 90))
```

---

### 4.4 波动率统计（关键）

```python
# 训练集模式：计算统计量
if mode == 'train':
    vol_feature = train_features_scaled[:, 7]  # Volatility_20d是第8列
    self.vol_stats = {
        'mean': float(np.mean(vol_feature)),
        'std': float(np.std(vol_feature)),
        'p50': float(np.percentile(vol_feature, 50)),
        'p60': float(np.percentile(vol_feature, 60)),
        'p70': float(np.percentile(vol_feature, 70)),  # ← 推荐
        'p80': float(np.percentile(vol_feature, 80)),
        'p90': float(np.percentile(vol_feature, 90))
    }
    print(f"[波动率统计] p70={self.vol_stats['p70']:.3f} (推荐作为q_threshold)")

# 测试集模式：使用训练集统计量
else:
    self.vol_stats = vol_stats if vol_stats else {'p70': 0.5}
```

**为什么需要这个?**
- 量子模块需要动态阈值`q_threshold`
- 使用第70百分位数（p70）确保量子通道在高波动时激活

---

### 4.5 滑动窗口序列构建

```python
def create_sequences(df, seq_len=30):
    sequences = []
    
    # 对每只股票单独处理（不跨Ticker）
    for ticker in df['Ticker'].unique():
        ticker_data = df[df['Ticker'] == ticker].sort_values('Date')
        
        # 滑动窗口
        for i in range(len(ticker_data) - seq_len):
            x = ticker_data.iloc[i : i+seq_len][feature_cols].values  # (30, 8)
            y = ticker_data.iloc[i+seq_len]['Log_Ret']                 # 标量
            vol = ticker_data.iloc[i+seq_len-1]['Volatility_20d']     # 标量
            node_idx = ticker2idx[ticker]                               # 整数
            
            sequences.append({
                'x': torch.FloatTensor(x),
                'y': torch.FloatTensor([y]),
                'vol': torch.FloatTensor([vol]),
                'node_idx': node_idx
            })
    
    return sequences
```

**输入维度**: (batch_size, seq_len, input_dim) = (512, 30, 8)

**输出维度**: (batch_size, 1)

---

## 数据质量检查

### 检查脚本

```python
# check_data.py
import pandas as pd
import numpy as np

df = pd.read_csv('data/processed/Final_Model_Data.csv')

print("=== 基本信息 ===")
print(f"总行数: {len(df)}")
print(f"股票数: {df['Ticker'].nunique()}")
print(f"时间范围: {df['Date'].min()} ~ {df['Date'].max()}")

print("\n=== 缺失值 ===")
print(df.isnull().sum())

print("\n=== 异常值检测 ===")
print(df.describe())
print(f"\nLog_Ret > 1.0 的行数: {(df['Log_Ret'].abs() > 1.0).sum()}")
print(f"Volatility_20d > 2.0 的行数: {(df['Volatility_20d'] > 2.0).sum()}")

print("\n=== 数据类型 ===")
print(df.dtypes)
```

**期望输出**:
```
总行数: 2,500,000
股票数: 350
时间范围: 2019-01-02 ~ 2023-12-29
缺失值: 全部0
Log_Ret > 1.0 的行数: 0
Volatility_20d > 2.0 的行数: 0
```

---

## 常见问题

### Q1: 新闻匹配率低

**症状**: `Headline`列90%为空

**原因**: 
- Ticker不匹配（大小写问题）
- 时间范围不一致
- 新闻CSV损坏

**解决**:
```python
# 检查Ticker匹配
price_tickers = set(df_price['Ticker'].unique())
news_tickers = set(df_news['Ticker'].unique())
print(f"共同Ticker数: {len(price_tickers & news_tickers)}")
print(f"仅在价格中: {price_tickers - news_tickers}")
print(f"仅在新闻中: {news_tickers - price_tickers}")
```

### Q2: 图谱过于稀疏

**症状**: `有连接的股票数: 20 / 350 (5.7%)`

**原因**:
- 未使用分层采样
- 新闻数量不足
- 规则匹配不准确

**解决**:
```bash
# 增加采样数
python -m dataProcessed.build_graph --max_per_ticker 100

# 使用LLM模式
python -m dataProcessed.build_graph --use_llm
```

### Q3: Dataset加载慢

**症状**: `DataLoader`迭代很慢

**原因**:
- 滑动窗口序列预先构建（内存占用大）

**解决**:
```python
# 方案1: 增加num_workers
train_loader = DataLoader(
    train_dataset,
    batch_size=512,
    num_workers=4,  # ← 增加
    prefetch_factor=2
)

# 方案2: 使用pin_memory
train_loader = DataLoader(
    train_dataset,
    batch_size=512,
    pin_memory=True  # ← GPU加速
)
```

---

## 总结

数据处理流程的关键点：

1. **ETL**: 分块处理大文件，避免内存溢出
2. **对齐**: Left Join策略，确保数据完整性
3. **特征工程**: 对数收益率和滚动波动率是金融标准特征
4. **图谱构建**: 分层采样解决偏差问题，S&P 500提高质量
5. **Dataset**: 波动率统计为量子模块提供动态阈值

**数据流**: 原始数据 → ETL → 对齐 → 图谱 → Dataset → 模型

**总耗时**: 约15-30分钟（不含LLM模式）

**存储占用**: 约5-10GB

**质量指标**: 
- 新闻匹配率 >80%
- 图谱连接率 >50%
- 数据清洁度 100%
