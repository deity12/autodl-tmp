# æ¨¡å—è¯´æ˜æ–‡æ¡£

> æ¯ä¸ªPythonæ–‡ä»¶çš„è¯¦ç»†åŠŸèƒ½è¯´æ˜

## ğŸ“¦ æ•°æ®å¤„ç†æ¨¡å— (`dataProcessed/`)

### 1. `etl.py` - ETLæ•°æ®æå–ä¸è½¬æ¢

**ä½œç”¨**: åˆå¹¶ä¸ªè‚¡ä»·æ ¼æ–‡ä»¶å’Œæå–æ–°é—»æ•°æ®

**è¾“å…¥**:
- `data/raw/FNSPID/full_history/*.csv` - æ•°åƒä¸ªè‚¡ç¥¨CSVæ–‡ä»¶
- `data/raw/FNSPID/nasdaq_exteral_data.csv` - 23GBæ–°é—»æ•°æ®

**è¾“å‡º**:
- `data/processed/Stock_Prices.csv` - åˆå¹¶åçš„è‚¡ä»·æ•°æ®
- `data/processed/Stock_News.csv` - è¿‡æ»¤åçš„æ–°é—»æ•°æ®

**æ ¸å¿ƒåŠŸèƒ½**:
```python
1. è‚¡ä»·æ•°æ®åˆå¹¶
   - æ‰«æ full_history/ ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
   - ç»Ÿä¸€åˆ—åæ ¼å¼ï¼ˆå¤§å°å†™æ ‡å‡†åŒ–ï¼‰
   - è¿‡æ»¤æ—¶é—´èŒƒå›´ï¼ˆ2019-2023ï¼‰
   - æå–Tickerä»£ç 
   - åˆå¹¶ä¸ºå•ä¸€DataFrame

2. æ–°é—»æ•°æ®åˆ†å—å¤„ç†
   - ä½¿ç”¨ chunksize=100000 é¿å…å†…å­˜æº¢å‡º
   - è¿‡æ»¤æ—¶é—´èŒƒå›´å’ŒTicker
   - ä»…ä¿ç•™å…³é”®åˆ—ï¼ˆDate, Ticker, Headline, Publisherï¼‰
   - å¢é‡åˆå¹¶å¤„ç†ç»“æœ
```

**å…³é”®å‚æ•°**:
- `CHUNKSIZE = 100000` - æ¯æ¬¡å¤„ç†10ä¸‡è¡Œæ–°é—»
- æ—¶é—´èŒƒå›´: `2019-01-01` è‡³ `2023-12-31`

**è¿è¡Œæ—¶é—´**: çº¦5-10åˆ†é’Ÿ

---

### 2. `align.py` - æ•°æ®å¯¹é½ä¸ç‰¹å¾å·¥ç¨‹

**ä½œç”¨**: å¯¹é½è‚¡ä»·ã€å¸‚åœºæŒ‡æ•°ã€æ–°é—»ï¼Œå¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡

**è¾“å…¥**:
- `data/processed/Stock_Prices.csv`
- `data/raw/FNSPID/SP500_Index.csv`
- `data/processed/Stock_News.csv`

**è¾“å‡º**:
- `data/processed/Final_Model_Data.csv` - å®Œæ•´çš„æ¨¡å‹è¾“å…¥æ•°æ®

**æ ¸å¿ƒåŠŸèƒ½**:
```python
1. å¤šæºæ•°æ®å¯¹é½
   - Left Join: Stock_Prices â† SP500_Index (æŒ‰Date)
   - Left Join: Result â† Stock_News (æŒ‰Date+Ticker)
   
2. æ–°é—»èšåˆ
   - åŒä¸€æ—¥æœŸ+è‚¡ç¥¨çš„å¤šæ¡æ–°é—»åˆå¹¶ä¸ºä¸€æ¡
   - ä½¿ç”¨ '|' åˆ†éš”ç¬¦è¿æ¥æ ‡é¢˜
   
3. ç‰¹å¾è®¡ç®—
   - Log_Ret = log(Close_t / Close_{t-1})  # å¯¹æ•°æ”¶ç›Šç‡
   - Volatility_20d = std(Log_Ret[-20:])   # 20æ—¥æ»šåŠ¨æ³¢åŠ¨ç‡
   
4. ç¼ºå¤±å€¼å¤„ç†
   - å¸‚åœºæŒ‡æ•°: å‰å‘å¡«å…… (ffill)
   - æ–°é—»: å¡«å……ç©ºå­—ç¬¦ä¸²
```

**è¾“å‡ºåˆ—**:
```
Date, Ticker, Open, High, Low, Close, Volume,
Market_Close, Market_Vol,
Headline,
Log_Ret, Volatility_20d
```

**è¿è¡Œæ—¶é—´**: çº¦2-5åˆ†é’Ÿ

---

### 3. `build_graph.py` - åŠ¨æ€è¯­ä¹‰å›¾è°±æ„å»º â­

**ä½œç”¨**: ä»æ–°é—»ä¸­æå–è‚¡ç¥¨é—´å…³ç³»ï¼Œæ„å»ºé‚»æ¥çŸ©é˜µ

**è¾“å…¥**:
- `data/processed/Final_Model_Data.csv`
- `data/processed/Stock_News.csv`

**è¾“å‡º**:
- `data/processed/Graph_Adjacency.npy` - é‚»æ¥çŸ©é˜µ (NÃ—N)

**æ ¸å¿ƒåŠŸèƒ½**:
```python
1. S&P 500 æˆåˆ†è‚¡è¿‡æ»¤ï¼ˆé»˜è®¤ï¼‰
   - ä»7691åªè‚¡ç¥¨ä¸­ç­›é€‰å‡º~300-500åªS&P 500æˆåˆ†è‚¡
   - æé«˜å›¾è°±è´¨é‡å’Œè®¡ç®—æ•ˆç‡
   
2. åˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
   - å¯¹æ¯åªè‚¡ç¥¨éšæœºé‡‡æ ·è‡³å¤š50æ¡æ–°é—»
   - å…¨å±€æ‰“ä¹±é¿å…é¡ºåºåå·®
   - ç¡®ä¿æ¯åªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§
   
3. å…³ç³»æå–ï¼ˆä¸¤ç§æ¨¡å¼ï¼‰
   a) è§„åˆ™æ¨¡å¼ï¼ˆé»˜è®¤ï¼ŒUSE_LLM_DEFAULT=Falseï¼‰
      - æ£€æŸ¥æ–°é—»ä¸­æ˜¯å¦æåˆ°å…¶ä»–è‚¡ç¥¨ä»£ç 
      - è¦æ±‚Tickeré•¿åº¦â‰¥3ä¸”ä½œä¸ºç‹¬ç«‹å•è¯å‡ºç°
      - å¿«é€Ÿï¼Œæ— éœ€GPU
      
   b) LLMæ¨¡å¼ï¼ˆ--use_llmï¼‰
      - ä½¿ç”¨ Qwen2.5-14B-Instruct æå–è¯­ä¹‰å…³ç³»
      - éœ€è¦çº¦24GBæ˜¾å­˜
      - æ›´å‡†ç¡®ï¼Œä½†æ…¢
      
4. ç´¯ç§¯å¼æ¼”åŒ–å›¾è°±
   - ä»…ä½¿ç”¨è®­ç»ƒé›†æ—¶æ®µå†…çš„æ–°é—»ï¼ˆé˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰
   - å…³ç³»éšæ—¶é—´ç´¯ç§¯
```

**å…³é”®å‚æ•°**:
```python
MAX_NEWS_PER_TICKER = 200    # æ¯è‚¡ç¥¨é‡‡æ ·æ•°
MAX_TOTAL_NEWS = 100000      # æ€»é‡‡æ ·ä¸Šé™
USE_SP500_ONLY = True       # ä»…ç”¨S&P 500
USE_LLM_DEFAULT = False     # é»˜è®¤è§„åˆ™æ¨¡å¼
```

**å‘½ä»¤è¡Œå‚æ•°**:
```bash
--use_llm           # ä½¿ç”¨LLMæ¨¡å¼
--all_stocks        # ä½¿ç”¨å…¨é‡è‚¡ç¥¨
--max_per_ticker N  # è‡ªå®šä¹‰æ¯è‚¡ç¥¨é‡‡æ ·æ•°
--max_total N       # è‡ªå®šä¹‰æ€»é‡‡æ ·æ•°
```

**è¿è¡Œæ—¶é—´**: 
- è§„åˆ™æ¨¡å¼: 1-3åˆ†é’Ÿ
- LLMæ¨¡å¼: 30-60åˆ†é’Ÿ

---

### 4. `dataset.py` - PyTorchæ•°æ®é›†ç±»

**ä½œç”¨**: å°è£…æ•°æ®åŠ è½½ã€æ¸…æ´—ã€åˆ‡åˆ†ã€æ ‡å‡†åŒ–ä¸ºPyTorch Dataset

**è¾“å…¥**:
- `data/processed/Final_Model_Data.csv`
- `data/processed/Graph_Adjacency.npy`

**è¾“å‡º**: PyTorch `DataLoader` å¯ç”¨çš„Datasetå¯¹è±¡

**æ ¸å¿ƒåŠŸèƒ½**:
```python
class FinancialDataset(Dataset):
    def __init__(self, mode='train', seq_len=30, ...):
        1. æ•°æ®æ¸…æ´—
           - è£å‰ªå¼‚å¸¸å€¼ï¼ˆLog_Ret: -0.5~0.5, Vol: 0~2.0ï¼‰
           - å¤„ç†æ— ç©·å¤§å’ŒNaN
           - å‰å‘å¡«å……ç¼ºå¤±å€¼
           
        2. è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†
           - æŒ‰æ—¶é—´80/20åˆ’åˆ†
           - ç¡®ä¿æ—¶é—´è¿ç»­æ€§ï¼ˆä¸è·¨Tickerï¼‰
           
        3. ç‰¹å¾æ ‡å‡†åŒ–
           - StandardScalerï¼ˆå¯é€‰RobustScalerï¼‰
           - è®­ç»ƒé›†fitï¼Œæµ‹è¯•é›†transform
           
        4. æ³¢åŠ¨ç‡ç»Ÿè®¡ï¼ˆè®­ç»ƒé›†ï¼‰
           - è®¡ç®—p50, p60, p70, p80, p90åˆ†ä½æ•°
           - æ¨èä½¿ç”¨p70ä½œä¸ºq_threshold
           - å­˜å‚¨åœ¨ self.vol_stats
           
        5. æ»‘åŠ¨çª—å£åºåˆ—æ„å»º
           - è¾“å…¥: è¿‡å»30å¤© Ã— 8ç‰¹å¾
           - è¾“å‡º: ç¬¬31å¤©çš„Log_Ret
           - é™„åŠ : ç¬¬30å¤©çš„Volatility_20d
           
    def __getitem__(self, idx):
        return {
            'x': torch.Tensor (seq_len, input_dim),  # è¾“å…¥åºåˆ—
            'y': torch.Tensor (1,),                   # ç›®æ ‡å€¼
            'vol': torch.Tensor (1,),                 # æ³¢åŠ¨ç‡
            'node_idx': int                           # å›¾è°±èŠ‚ç‚¹ç´¢å¼•
        }
```

**é‡è¦å±æ€§**:
- `vol_stats`: æ³¢åŠ¨ç‡ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…è®­ç»ƒé›†ï¼‰
  ```python
  {
    'mean': 0.02,
    'std': 0.015,
    'p50': 0.45,
    'p70': 0.52,  # â† æ¨èä½œä¸ºq_threshold
    'p90': 0.68
  }
  ```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
# è®­ç»ƒé›†
train_dataset = FinancialDataset(
    mode='train',
    seq_len=30,
    use_robust_scaler=False
)

# è·å–æ³¢åŠ¨ç‡ç»Ÿè®¡
q_threshold = train_dataset.vol_stats.get('p70', 0.5)

# æµ‹è¯•é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼‰
test_dataset = FinancialDataset(
    mode='test',
    seq_len=30,
    vol_stats=train_dataset.vol_stats  # â† ä¼ å…¥è®­ç»ƒé›†ç»Ÿè®¡é‡
)
```

---

### 5. `download_market_index.py` - å¸‚åœºæŒ‡æ•°ä¸‹è½½

**ä½œç”¨**: ä¸‹è½½S&P 500æŒ‡æ•°æ•°æ®

**è¾“å…¥**: æ— ï¼ˆä»AkShare APIè·å–ï¼‰

**è¾“å‡º**: `data/raw/FNSPID/SP500_Index.csv`

**æ ¸å¿ƒåŠŸèƒ½**:
```python
ä½¿ç”¨ akshare åº“ä¸‹è½½S&P 500å†å²æ•°æ®
æ ¼å¼åŒ–ä¸º (Date, Market_Close, Market_Vol)
ä¿å­˜ä¸ºCSV
```

**è¿è¡Œæ—¶é—´**: çº¦1åˆ†é’Ÿ

---

## ğŸ§  æ¨¡å‹æ¨¡å— (`models/`)

### 6. `base_model.py` - åŸºç¡€æ¨¡å‹ç»„ä»¶

**ä½œç”¨**: å®šä¹‰Quantum-RWKVã€MATCCç­‰æ ¸å¿ƒç»„ä»¶

**åŒ…å«ç±»**:

#### 6.1 `VQC_Block` - å˜åˆ†é‡å­çº¿è·¯
```python
class VQC_Block(nn.Module):
    åŠŸèƒ½: PennyLaneé‡å­çº¿è·¯å°è£…
    ç»“æ„:
      - AngleEmbedding: ç»å…¸â†’é‡å­ç¼–ç 
      - StronglyEntanglingLayers: é‡å­çº ç¼ 
      - qml.expval(qml.PauliZ): æµ‹é‡è¾“å‡º
    å‚æ•°: n_qubits=4, n_layers=2
```

#### 6.2 `RWKV_TimeMixing` - æ—¶é—´æ··åˆå±‚
```python
class RWKV_TimeMixing(nn.Module):
    åŠŸèƒ½: RWKVçš„æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
    å…¬å¼: 
      r = W_r @ (Î¼ * x_prev + (1-Î¼) * x_t)
      k = W_k @ (Î¼ * x_prev + (1-Î¼) * x_t)
      v = W_v @ (Î¼ * x_prev + (1-Î¼) * x_t)
      wkv = WKV_operator(r, k, v, w)
      output = W_o @ (Ïƒ(r) âŠ™ wkv)
```

#### 6.3 `Quantum_ChannelMixing` - é‡å­é€šé“æ··åˆ â­
```python
class Quantum_ChannelMixing(nn.Module):
    åŠŸèƒ½: æ ¹æ®æ³¢åŠ¨ç‡è‡ªé€‚åº”é€‰æ‹©é‡å­/ç»å…¸é€šé“
    
    forward(x, vol):
        if vol > q_threshold:
            # é«˜æ³¢åŠ¨ â†’ é‡å­é€šé“
            x_q = VQC(x)
            return x + quantum_scale * dropout(x_q)
        else:
            # ä½æ³¢åŠ¨ â†’ ç»å…¸é€šé“
            x_c = MLP(x)
            return x + dropout(x_c)
    
    å…³é”®å‚æ•°:
      - q_threshold: åŠ¨æ€é˜ˆå€¼ï¼ˆä»datasetè·å–ï¼‰
      - quantum_scale: å¯å­¦ä¹ ç¼©æ”¾å› å­ï¼ˆåˆå§‹0.1ï¼‰
```

#### 6.4 `MATCCDecompose` - è¶‹åŠ¿è§£è€¦
```python
class MATCCDecompose(nn.Module):
    åŠŸèƒ½: åˆ†è§£è¶‹åŠ¿é¡¹å’Œæ³¢åŠ¨é¡¹
    
    forward(x, market_guidance):
        # å› æœæ»‘åŠ¨å¹³å‡
        trend = causal_moving_average(x)
        fluctuation = x - trend
        
        # å¸‚åœºå¼•å¯¼è°ƒæ•´
        trend_adjusted = trend + market_guidance
        
        return trend_adjusted, fluctuation
```

#### 6.5 `QL_MATCC_Model` - åŸºç¡€æ¨¡å‹
```python
class QL_MATCC_Model(nn.Module):
    åŠŸèƒ½: Quantum-RWKV + MATCC åŸºç¡€æ¨¡å‹
    
    ç»“æ„:
      1. MarketGuidance(market_features) â†’ market_emb
      2. MATCCDecompose(x, market_emb) â†’ trend, fluctuation
      3. N Ã— [
           RWKV_TimeMixing(fluctuation)
           LayerNorm
           Quantum_ChannelMixing(fluctuation, vol)  # â† é‡å­å¢å¼º
           LayerNorm
         ]
      4. Linear(trend) + RWKV_output â†’ final_output
      5. Dropout â†’ head(final_output) â†’ prediction
```

---

### 7. `gnn_model.py` - å®Œæ•´GNNæ¨¡å‹

**ä½œç”¨**: å°†åŸºç¡€æ¨¡å‹ä¸å›¾æ³¨æ„åŠ›ç½‘ç»œèåˆ

**åŒ…å«ç±»**:

#### 7.1 `GraphAttentionLayer` - GATå±‚
```python
class GraphAttentionLayer(nn.Module):
    åŠŸèƒ½: å›¾æ³¨æ„åŠ›æœºåˆ¶
    
    forward(h, adj):
        # çº¿æ€§å˜æ¢
        Wh = W @ h
        
        # æ³¨æ„åŠ›ç³»æ•°
        e_ij = LeakyReLU(a^T [Wh_i || Wh_j])
        Î±_ij = softmax_j(e_ij)
        
        # èšåˆé‚»å±…
        h' = Ïƒ(Î£_j Î±_ij * Wh_j)
        
        return h'
```

#### 7.2 `QL_MATCC_GNN_Model` - å®Œæ•´æ¨¡å‹ â­
```python
class QL_MATCC_GNN_Model(nn.Module):
    åŠŸèƒ½: æ—¶åº+å›¾è°±èåˆçš„å®Œæ•´æ¡†æ¶
    
    ç»“æ„:
      1. temporal_encoder = QL_MATCC_Model(...)
      2. graph_encoder = GraphAttentionLayer(...)
      3. fusion_head = Sequential([
           LayerNorm,
           Dropout,
           Linear(concat_dim â†’ hidden),
           GELU,
           Dropout,
           Linear(hidden â†’ 1)
         ])
    
    forward(x, vol, market, node_idx, adj):
        # æ—¶åºç‰¹å¾
        h_temporal = temporal_encoder(x, vol, market)
        
        # å›¾è°±ç‰¹å¾
        h_all = temporal_encoder.encode_all_nodes(...)
        h_graph = graph_encoder(h_all, adj)[node_idx]
        h_graph = LayerNorm(h_graph)
        
        # èåˆ
        h_concat = cat([h_temporal, h_graph], dim=-1)
        output = fusion_head(h_concat)
        
        return output
```

---

## ğŸ“ è®­ç»ƒæ¨¡å— (`training/`)

### 8. `train_full.py` - å®Œæ•´æ¨¡å‹è®­ç»ƒ

**ä½œç”¨**: è®­ç»ƒQL-MATCC-GNNå®Œæ•´æ¨¡å‹

**æ ¸å¿ƒé…ç½®**:
```python
CONFIG = {
    # æ¨¡å‹ç»´åº¦
    'n_embd': 256,
    'n_layers': 3,
    'gnn_embd': 64,
    
    # è®­ç»ƒå‚æ•°
    'batch_size': 512,
    'epochs': 20,
    
    # å­¦ä¹ ç‡ï¼ˆå·®å¼‚åŒ–ï¼‰
    'lr': 3e-4,              # ç»å…¸å±‚
    'quantum_lr_ratio': 0.1, # é‡å­å±‚=3e-5
    
    # æ­£åˆ™åŒ–
    'dropout': 0.15,
    'weight_decay': 1e-5,
    
    # åŠ¨æ€é˜ˆå€¼
    'q_threshold': None  # ä»datasetè‡ªåŠ¨è·å–p70
}
```

**è®­ç»ƒæµç¨‹**:
```python
1. åŠ è½½æ•°æ®
   - train_dataset = FinancialDataset(mode='train')
   - q_threshold = train_dataset.vol_stats['p70']
   - test_dataset = FinancialDataset(mode='test', vol_stats=...)
   
2. æ„å»ºæ¨¡å‹
   - model = QL_MATCC_GNN_Model(q_threshold=q_threshold, ...)
   
3. å·®å¼‚åŒ–ä¼˜åŒ–å™¨
   - quantum_params = [é‡å­å±‚å‚æ•°]
   - classic_params = [å…¶ä»–å‚æ•°]
   - optimizer = AdamW([
       {'params': quantum_params, 'lr': 3e-5},
       {'params': classic_params, 'lr': 3e-4}
     ])
   
4. è°ƒåº¦å™¨
   - scheduler = CosineAnnealingWarmRestarts(T_0=5, T_mult=2)
   
5. è®­ç»ƒå¾ªç¯
   - æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
   - æ¢¯åº¦è£å‰ª (max_norm=1.0)
   - Early Stopping (patience=6)
   
6. ä¿å­˜ç»“æœ
   - best_model_full.pth
   - training_losses_full.json
   - training_curve_full.png
```

**è¿è¡Œæ—¶é—´**: çº¦2-4å°æ—¶ï¼ˆ20 epochsï¼‰

---

### 9. `train_ablation.py` - æ¶ˆèå®éªŒ

**ä½œç”¨**: è¿è¡Œ5ç»„æ¶ˆèå®éªŒï¼ŒéªŒè¯å„æ¨¡å—æœ‰æ•ˆæ€§

**å®éªŒç»„**:
```python
experiments = [
    "full_model",          # åŸºå‡†: æ‰€æœ‰æ¨¡å—å¼€å¯
    "no_quantum",          # æ¶ˆè1: ç§»é™¤é‡å­æ¨¡å—
    "no_graph",            # æ¶ˆè2: ç§»é™¤å›¾æ³¨æ„åŠ›
    "no_matcc",            # æ¶ˆè3: ç§»é™¤è¶‹åŠ¿è§£è€¦
    "no_market_guidance"   # æ¶ˆè4: ç§»é™¤å¸‚åœºå¼•å¯¼
]
```

**æ ¸å¿ƒåŠŸèƒ½**:
```python
def run_experiment(exp_name, config):
    1. æ ¹æ®exp_nameè°ƒæ•´config
       - no_quantum: use_quantum = False
       - no_graph: ä½¿ç”¨ QL_MATCC_Model è€Œé GNN
       - no_matcc: use_matcc = False
       - no_market_guidance: market_guidanceç½®0
       
    2. è®­ç»ƒæ¨¡å‹ï¼ˆä¸train_full.pyç›¸åŒæµç¨‹ï¼‰
    
    3. ä¿å­˜ç»“æœ
       - best_model_{exp_name}.pth
       - losses_{exp_name}.json
       - curve_{exp_name}.png
       
    4. è¿”å›æœ€ä½³éªŒè¯loss

# è¿è¡Œæ‰€æœ‰å®éªŒ
for exp in experiments:
    best_val_loss = run_experiment(exp, BASE_CONFIG)
    results.append({
        'experiment': exp,
        'best_val_loss': best_val_loss,
        ...
    })

# ç”Ÿæˆæ±‡æ€»
save_summary_csv(results)
plot_comparison(all_losses)
```

**è¾“å‡º**:
- 5ä¸ªæ¨¡å‹æƒé‡æ–‡ä»¶
- 5ä¸ªlossæ›²çº¿JSON
- `ablation_results_summary.csv` - æ±‡æ€»è¡¨
- `ablation_comparison.png` - å¯¹æ¯”å›¾

**è¿è¡Œæ—¶é—´**: çº¦10-20å°æ—¶

---

## ğŸ“Š è¯„ä¼°æ¨¡å— (`evaluation/`)

### 10. `evaluate_all.py` - ç»Ÿä¸€è¯„ä¼°

**ä½œç”¨**: åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œè®¡ç®—IC/RankICç­‰æŒ‡æ ‡ï¼Œç”Ÿæˆå¯¹æ¯”è¡¨

**æ ¸å¿ƒåŠŸèƒ½**:
```python
1. åŠ è½½æ¨¡å‹åˆ—è¡¨
   models = [
       'best_model_full.pth',
       'best_model_no_quantum.pth',
       'best_model_no_graph.pth',
       'best_model_no_matcc.pth',
       'best_model_no_market_guidance.pth'
   ]

2. å¯¹æ¯ä¸ªæ¨¡å‹è®¡ç®—æŒ‡æ ‡
   for model in models:
       preds, trues = predict(model, test_loader)
       metrics = {
           'MSE': mean_squared_error(trues, preds),
           'MAE': mean_absolute_error(trues, preds),
           'RMSE': sqrt(MSE),
           'R2': r2_score(trues, preds),
           'IC': pearsonr(preds, trues)[0],     # â† æ ¸å¿ƒ
           'RankIC': spearmanr(preds, trues)[0], # â† æ ¸å¿ƒ
           'DirectionalAccuracy': accuracy_score(sign(trues), sign(preds))
       }

3. ç”Ÿæˆå¯¹æ¯”è¡¨
   df_results = pd.DataFrame(all_metrics)
   df_results.to_csv('evaluation_overall.csv')

4. å¯è§†åŒ–
   plot_comparison_chart(df_results)
```

**è¾“å‡º**:
- `evaluation_overall.csv` - æ•´ä½“æŒ‡æ ‡å¯¹æ¯”
- `evaluation_comparison.png` - å¯è§†åŒ–å¯¹æ¯”

---

### 11. `evaluate_by_group.py` - åˆ†ç»„è¯„ä¼°

**ä½œç”¨**: æŒ‰æ³¢åŠ¨ç‡åˆ†ç»„è¯„ä¼°ï¼Œåˆ†ææ¨¡å‹åœ¨ä¸åŒå¸‚åœºçŠ¶æ€ä¸‹çš„è¡¨ç°

**æ ¸å¿ƒåŠŸèƒ½**:
```python
1. æ³¢åŠ¨ç‡åˆ†ç»„
   volatility_groups = pd.qcut(test_data['vol'], q=3, labels=['Low', 'Medium', 'High'])

2. å¯¹æ¯ç»„åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
   for group in ['Low', 'Medium', 'High']:
       group_data = test_data[volatility_groups == group]
       metrics = calculate_metrics(group_data)
       results.append({
           'group': group,
           'IC': metrics['IC'],
           'RankIC': metrics['RankIC'],
           ...
       })

3. ç”Ÿæˆåˆ†ç»„å¯¹æ¯”è¡¨
   df_by_volatility = pd.DataFrame(results)
   df_by_volatility.to_csv('evaluation_by_volatility.csv')
```

**è¾“å‡º**:
- `evaluation_by_volatility.csv` - åˆ†æ³¢åŠ¨ç‡ç»„æŒ‡æ ‡

---

### 12. `plot_ablation_comparison.py` - æ¶ˆèå›¾è¡¨ç»˜åˆ¶

**ä½œç”¨**: å¯è§†åŒ–æ¶ˆèå®éªŒç»“æœ

**æ ¸å¿ƒåŠŸèƒ½**:
```python
1. åŠ è½½æ‰€æœ‰losså†å²
   losses = {
       'full': load_json('losses_full_model.json'),
       'no_quantum': load_json('losses_no_quantum.json'),
       ...
   }

2. ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¯¹æ¯”
   plt.figure(figsize=(12, 6))
   for name, loss_data in losses.items():
       plt.plot(loss_data['train_losses'], label=f'{name} (train)')
       plt.plot(loss_data['val_losses'], label=f'{name} (val)')
   plt.legend()
   plt.savefig('ablation_comparison.png')
```

**è¾“å‡º**:
- `ablation_comparison.png` - 5æ¡è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾

---

## ğŸ“– æ€»ç»“

### æ ¸å¿ƒæ–‡ä»¶ä¼˜å…ˆçº§

**å¿…é¡»è¿è¡Œ** (æŒ‰é¡ºåº):
1. `etl.py` â†’ åˆå¹¶æ•°æ®
2. `align.py` â†’ å¯¹é½æ•°æ®
3. `build_graph.py` â†’ æ„å»ºå›¾è°±
4. `train_full.py` â†’ è®­ç»ƒæ¨¡å‹

**å¯é€‰è¿è¡Œ**:
- `download_market_index.py` - å¦‚æœå·²æœ‰SP500æ•°æ®å¯è·³è¿‡
- `train_ablation.py` - å¦‚æœåªéœ€è¦å®Œæ•´æ¨¡å‹å¯è·³è¿‡
- `evaluate_*.py` - è¯„ä¼°é˜¶æ®µè¿è¡Œ

### å…³é”®åˆ›æ–°ç‚¹

1. **`dataset.py`**: åŠ¨æ€è®¡ç®—æ³¢åŠ¨ç‡åˆ†ä½æ•°ï¼Œä¸ºé‡å­æ¨¡å—æä¾›æ•°æ®é©±åŠ¨çš„é˜ˆå€¼
2. **`build_graph.py`**: åˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼Œè§£å†³Tickeræ’åºåå·®é—®é¢˜
3. **`base_model.py`**: é‡å­é€šé“è‡ªé€‚åº”é—¨æ§ï¼Œä»…åœ¨é«˜æ³¢åŠ¨æ—¶æ¿€æ´»
4. **`gnn_model.py`**: Late Fusionæ¶æ„ï¼Œå……åˆ†èåˆæ—¶åºå’Œå›¾è°±ç‰¹å¾
5. **`train_full.py`**: å·®å¼‚åŒ–å­¦ä¹ ç‡ï¼Œé’ˆå¯¹é‡å­å’Œç»å…¸å±‚åˆ†åˆ«ä¼˜åŒ–

### æ•°æ®æµå‘å›¾

```
åŸå§‹æ•°æ®
  â”œâ”€ full_history/*.csv
  â”œâ”€ nasdaq_exteral_data.csv
  â””â”€ SP500_Index.csv
       â†“
    etl.py
       â†“
  â”œâ”€ Stock_Prices.csv
  â””â”€ Stock_News.csv
       â†“
   align.py
       â†“
   Final_Model_Data.csv
       â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
  â†“             â†“
dataset.py  build_graph.py
  â†“             â†“
DataLoader  Graph_Adjacency.npy
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â†“
    train_full.py / train_ablation.py
         â†“
    best_model_*.pth
         â†“
    evaluate_all.py
         â†“
    evaluation_*.csv
```

---

**æç¤º**: å»ºè®®æŒ‰ç…§"å¿«é€Ÿå¼€å§‹"æŒ‡å—çš„é¡ºåºè¿è¡Œå„æ¨¡å—ï¼Œç¡®ä¿æ•°æ®å’Œæ¨¡å‹çš„å®Œæ•´æ€§ã€‚
