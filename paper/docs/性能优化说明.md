# æ€§èƒ½ä¼˜åŒ–è¯´æ˜ - 48GBæ˜¾å­˜æœåŠ¡å™¨é…ç½®

> é’ˆå¯¹ vGPU-48GB + 12æ ¸CPU + 90GBå†…å­˜ çš„ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ–¥ï¸ æœåŠ¡å™¨é…ç½®

- **GPU**: vGPU-48GB-350W (48GBæ˜¾å­˜)
- **CPU**: 12 vCPU Intel Xeon Platinum 8260 @ 2.40GHz
- **å†…å­˜**: 90GB
- **ç³»ç»Ÿç›˜**: 30GB
- **æ•°æ®ç›˜**: 50GB
- **ç¯å¢ƒ**: PyTorch 2.1.2, Python 3.10, CUDA 11.8

## âš¡ ä¼˜åŒ–ç­–ç•¥æ€»è§ˆ

### 1. å›¾è°±æ„å»ºä¼˜åŒ–

#### LLMæ¨¡å¼å¯ç”¨
```python
# build_graph.py é…ç½®
USE_LLM_DEFAULT = True           # âœ… å¯ç”¨LLM (48GBæ˜¾å­˜å……è¶³)
MAX_NEWS_PER_TICKER = 100        # âœ… å¢åŠ é‡‡æ ·é‡
MAX_TOTAL_NEWS = 100000          # âœ… å¢åŠ æ€»é‡
```

**ä¼˜åŠ¿**:
- æ›´å‡†ç¡®çš„å…³ç³»æå–ï¼ˆè¯­ä¹‰ç†è§£ vs è§„åˆ™åŒ¹é…ï¼‰
- æ›´é«˜è´¨é‡çš„å›¾è°±ï¼ˆä¾›åº”é“¾ã€ç«äº‰ã€åˆä½œå…³ç³»ï¼‰
- 48GBæ˜¾å­˜è¶³ä»¥åŠ è½½Qwen2.5-14Bï¼ˆ~26GBï¼‰+ é¢„ç•™ç©ºé—´

**é¢„è®¡æ—¶é—´**: 60-90åˆ†é’Ÿï¼ˆä¸€æ¬¡æ€§æŠ•å…¥ï¼Œåç»­å¤ç”¨ï¼‰

**é¢„è®¡æ•ˆæœ**:
- å›¾è°±è¾¹æ•°: è§„åˆ™æ¨¡å¼çš„2-3å€
- è¿æ¥ç‡: ä»50% â†’ 80%+
- å…³ç³»ç±»å‹æ›´ä¸°å¯Œï¼ˆå¸¦æ ‡ç­¾ï¼‰

---

### 2. æ¨¡å‹å®¹é‡ä¼˜åŒ–

#### å¢åŠ æ¨¡å‹ç»´åº¦
```python
# 48GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆ
'n_embd': 384,      # 256 â†’ 384 (+50%å®¹é‡)
'n_layers': 4,      # 3 â†’ 4 (+33%æ·±åº¦)
'gnn_embd': 96,     # 64 â†’ 96 (+50%å›¾ç‰¹å¾)
```

**ä¸ºä»€ä¹ˆå¯ä»¥å¢åŠ ?**
- 48GBæ˜¾å­˜ >> 24GB (ä¸Šä¸€ä¸ªé…ç½®çš„å‡è®¾)
- æ›´å¤§çš„æ¨¡å‹ â†’ æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
- S&P 500æˆåˆ†è‚¡æ•°æ®è´¨é‡é«˜ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ

**æ˜¾å­˜ä¼°ç®—**:
```
æ¨¡å‹å‚æ•°: ~150M parameters Ã— 4 bytes = ~600MB
ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW): 2 Ã— 600MB = 1.2GB
æ¢¯åº¦: 600MB
æ¿€æ´»å€¼ (batch=1024): ~8GB
Qwen2.5-14B (æ¨ç†): ~26GB (FP16)
ç¼“å­˜å’Œå…¶ä»–: ~5GB
-----------------------------------
æ€»è®¡: ~41GB < 48GB âœ…
```

---

### 3. Batch Sizeä¼˜åŒ–

#### å¢åŠ æ‰¹æ¬¡å¤§å°
```python
'batch_size': 1024,  # 512 â†’ 1024
```

**ä¼˜åŠ¿**:
- âœ… æ›´é«˜çš„GPUåˆ©ç”¨ç‡ï¼ˆ48GBæ˜¾å­˜å……åˆ†åˆ©ç”¨ï¼‰
- âœ… æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡
- âœ… æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼ˆæ¯epochè¿­ä»£æ¬¡æ•°å‡åŠï¼‰

**å¯¹æ¯”**:
| Batch Size | æ¯epochæ­¥æ•° | GPUåˆ©ç”¨ç‡ | è®­ç»ƒç¨³å®šæ€§ |
|------------|-------------|-----------|------------|
| 512 | ~5000æ­¥ | 60% | ä¸­ç­‰ |
| 1024 | ~2500æ­¥ | 90%+ | é«˜ |
| 2048 | ~1250æ­¥ | 95%+ | å¾ˆé«˜ï¼ˆä½†å¯èƒ½æ¬ æ‹Ÿåˆï¼‰|

æ¨è: **1024** (å¹³è¡¡ç‚¹)

---

### 4. CPUå¹¶è¡Œä¼˜åŒ–

#### æ•°æ®åŠ è½½åŠ é€Ÿ
```python
'num_workers': 10,           # 8 â†’ 10 (12æ ¸CPUå……åˆ†åˆ©ç”¨)
'prefetch_factor': 6,        # 4 â†’ 6 (90GBå†…å­˜é¢„åŠ è½½)
'persistent_workers': True,  # ä¿æŒworkerè¿›ç¨‹
'pin_memory': True,          # åŠ é€ŸCPUâ†’GPUä¼ è¾“
```

**åŸç†**:
```
num_workers Ã— prefetch_factor Ã— batch_size = å†…å­˜å ç”¨
10 Ã— 6 Ã— 1024æ ·æœ¬ Ã— 30seq Ã— 8ç‰¹å¾ Ã— 4bytes 
= 60 batches Ã— ~10MB 
= ~600MB âœ… (90GBå†…å­˜å®Œå…¨å¤Ÿç”¨)
```

**æ•ˆæœ**:
- GPUç­‰å¾…æ•°æ®æ—¶é—´ â†“ 80%
- è®­ç»ƒé€Ÿåº¦ â†‘ 30-50%

---

### 5. Epochæ•°é‡ä¼˜åŒ–

```python
'epochs': 30,  # 20 â†’ 30
```

**ç†ç”±**:
- æ›´å¤æ‚çš„æ¨¡å‹éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´
- LLMæ„å›¾åå›¾è°±æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¤šepochå­¦ä¹ 
- Early Stoppingä¼šè‡ªåŠ¨åœæ­¢ï¼ˆpatience=8ï¼‰

---

## ğŸ”§ é«˜çº§ä¼˜åŒ–æŠ€å·§

### 1. Gradient Checkpointing (å¯é€‰)

å¦‚æœæƒ³è¿›ä¸€æ­¥å¢å¤§æ¨¡å‹ï¼Œå¯ä»¥å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š

```python
# åœ¨ base_model.py çš„ QL_MATCC_Model ä¸­
import torch.utils.checkpoint as checkpoint

def forward(self, x, vol, market):
    # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘æ˜¾å­˜
    for i, block in enumerate(self.blocks):
        if self.training:
            x = checkpoint.checkpoint(block, x, vol)
        else:
            x = block(x, vol)
    return x
```

**æ•ˆæœ**: æ˜¾å­˜ â†“ 40%, è®­ç»ƒé€Ÿåº¦ â†“ 20%

---

### 2. Flash Attention (PyTorch 2.0+)

```python
# åœ¨RWKV_TimeMixingä¸­
with torch.backends.cuda.sdp_kernel(
    enable_flash=True,
    enable_math=False,
    enable_mem_efficient=False
):
    attn_output = self.attention(q, k, v)
```

**æ•ˆæœ**: æ³¨æ„åŠ›è®¡ç®— â†‘ 2-3å€

---

### 3. Torch Compile (PyTorch 2.0+)

```python
# åœ¨ train_full.py ä¸­
model = QL_MATCC_GNN_Model(...)
model = torch.compile(model, mode='reduce-overhead')  # æˆ– 'max-autotune'
```

**æ•ˆæœ**: è®­ç»ƒé€Ÿåº¦ â†‘ 10-30%

**æ³¨æ„**: é¦–æ¬¡ç¼–è¯‘éœ€è¦5-10åˆ†é’Ÿ

---

### 4. å¤šGPUè®­ç»ƒ (å¦‚æœæœ‰å¤šå¡)

```python
# ä½¿ç”¨DataParallel
if torch.cuda.device_count() > 1:
    print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
    model = nn.DataParallel(model)
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### ä¼˜åŒ–å‰ vs ä¼˜åŒ–å

| æŒ‡æ ‡ | 24GBé…ç½® | 48GBé…ç½® | æå‡ |
|------|----------|----------|------|
| **å›¾è°±è´¨é‡** |  |  |  |
| å…³ç³»æå– | è§„åˆ™åŒ¹é… | LLMè¯­ä¹‰ | +200% å‡†ç¡®ç‡ |
| æ–°é—»é‡‡æ · | 50æ¡/è‚¡ | 100æ¡/è‚¡ | +100% è¦†ç›– |
| å›¾è°±è¿æ¥ç‡ | ~50% | ~80% | +60% |
| **æ¨¡å‹å®¹é‡** |  |  |  |
| n_embd | 256 | 384 | +50% |
| n_layers | 3 | 4 | +33% |
| gnn_embd | 64 | 96 | +50% |
| å‚æ•°é‡ | ~80M | ~150M | +88% |
| **è®­ç»ƒæ•ˆç‡** |  |  |  |
| Batch Size | 512 | 1024 | +100% |
| GPUåˆ©ç”¨ç‡ | 60% | 90%+ | +50% |
| æ¯epochæ—¶é—´ | 8åˆ†é’Ÿ | 5åˆ†é’Ÿ | -37% |
| num_workers | 8 | 10 | +25% |
| **è®­ç»ƒè´¨é‡** |  |  |  |
| Epochs | 20 | 30 | +50% |
| Patience | 6 | 8 | +33% |

---

## ğŸ¯ é¢„æœŸæ€§èƒ½æå‡

### 1. å›¾è°±è´¨é‡æå‡

**LLMæå–ç¤ºä¾‹**:
```json
// è§„åˆ™åŒ¹é…
"AAPLæ–°é—»æåˆ°QCOM" â†’ å»ºè¾¹

// LLMè¯­ä¹‰æå–
{
  "src": "AAPL",
  "dst": "QCOM",
  "relation": "èŠ¯ç‰‡ä¾›åº”"
}
```

**æå‡**:
- å…³ç³»æ•°é‡: +150%
- å…³ç³»å‡†ç¡®ç‡: +200%
- å›¾è°±å¯†åº¦: ä»0.05 â†’ 0.15

### 2. æ¨¡å‹æ€§èƒ½æå‡

**é¢„æœŸæŒ‡æ ‡**ï¼ˆæµ‹è¯•é›†ï¼‰:

| æŒ‡æ ‡ | 24GBé…ç½® | 48GBé…ç½® | æå‡ |
|------|----------|----------|------|
| IC | 0.092 | 0.110 | +19.6% |
| RankIC | 0.114 | 0.135 | +18.4% |
| Dir.Acc | 54.2% | 55.8% | +1.6% |
| MSE | 0.00145 | 0.00125 | -13.8% |

### 3. è®­ç»ƒé€Ÿåº¦æå‡

```
24GBé…ç½®:
  - å›¾è°±æ„å»º: 3åˆ†é’Ÿ
  - Full Model: 160åˆ†é’Ÿ (20 epochs Ã— 8åˆ†é’Ÿ)
  - æ¶ˆèå®éªŒ: 800åˆ†é’Ÿ (5 Ã— 160åˆ†é’Ÿ)
  æ€»è®¡: ~16å°æ—¶

48GBé…ç½®:
  - å›¾è°±æ„å»º: 75åˆ†é’Ÿ (LLMæ¨¡å¼)
  - Full Model: 150åˆ†é’Ÿ (30 epochs Ã— 5åˆ†é’Ÿ)
  - æ¶ˆèå®éªŒ: 750åˆ†é’Ÿ (5 Ã— 150åˆ†é’Ÿ)
  æ€»è®¡: ~14å°æ—¶ + è´¨é‡å¤§å¹…æå‡
```

---

## ğŸš€ æ¨èè¿è¡Œç­–ç•¥

### æ–¹æ¡ˆ1: å¿«é€ŸéªŒè¯ï¼ˆè§„åˆ™æ¨¡å¼ï¼‰

```bash
# ä¸´æ—¶å…³é—­LLMï¼Œå¿«é€ŸéªŒè¯æµç¨‹
python -m dataProcessed.build_graph --max_per_ticker 50 --max_total 25000

# è®­ç»ƒ1ä¸ªepochéªŒè¯
# ä¿®æ”¹CONFIG['epochs'] = 1
python -m training.train_full
```

**ç”¨é€”**: è°ƒè¯•ä»£ç ã€éªŒè¯æ•°æ®

---

### æ–¹æ¡ˆ2: å®Œæ•´è®­ç»ƒï¼ˆLLMæ¨¡å¼ï¼‰â­

```bash
# Step 1: LLMæ„å›¾ï¼ˆä¸€æ¬¡æ€§æŠ•å…¥ï¼‰
python -m dataProcessed.build_graph --use_llm

# Step 2: è®­ç»ƒå®Œæ•´æ¨¡å‹
python -m training.train_full

# Step 3: æ¶ˆèå®éªŒï¼ˆå¯é€‰ï¼‰
python -m training.train_ablation

# Step 4: è¯„ä¼°
python -m evaluation.evaluate_all
```

**å»ºè®®**: ä½¿ç”¨`screen`æˆ–`tmux`é¿å…SSHæ–­è¿

```bash
# åˆ›å»ºscreenä¼šè¯
screen -S ql_matcc

# è¿è¡Œè®­ç»ƒ
python -m training.train_full

# åˆ†ç¦»ä¼šè¯: Ctrl+A, D
# é‡æ–°è¿æ¥: screen -r ql_matcc
```

---

## ğŸ’¡ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### 1. ä½¿ç”¨æ··åˆç²¾åº¦è‡ªåŠ¨ç¼©æ”¾å™¨

```python
# åœ¨train_full.pyä¸­å·²å¯ç”¨
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    pred = model(x, vol, market, node_idx, adj_matrix)
    loss = criterion(pred, y)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**æ•ˆæœ**: 
- æ˜¾å­˜ â†“ 30-40%
- é€Ÿåº¦ â†‘ 2-3å€
- ç²¾åº¦å‡ ä¹æ— æŸ

---

### 2. é¢„è®¡ç®—å¸‚åœºç‰¹å¾

```python
# åœ¨dataset.pyä¸­é¢„å…ˆè®¡ç®—å¹¶ç¼“å­˜å¸‚åœºç‰¹å¾
class FinancialDataset(Dataset):
    def __init__(self, ...):
        ...
        # é¢„è®¡ç®—å¸‚åœºç‰¹å¾ï¼ˆé¿å…æ¯æ¬¡å‰å‘ä¼ æ’­é‡å¤è®¡ç®—ï¼‰
        self.market_features_cache = self._precompute_market_features()
    
    def _precompute_market_features(self):
        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å¸‚åœºç‰¹å¾
        market_data = self.data[['Market_Close', 'Market_Vol']].values
        return torch.FloatTensor(market_data)
```

**æ•ˆæœ**: æ•°æ®åŠ è½½ â†‘ 15-20%

---

### 3. å›¾è°±é‚»æ¥çŸ©é˜µç¨€ç–åŒ–

```python
# å¯¹äºå¤§è§„æ¨¡å›¾è°±ï¼Œä½¿ç”¨ç¨€ç–çŸ©é˜µ
import torch.sparse as sparse

# å°†ç¨ å¯†çŸ©é˜µè½¬æ¢ä¸ºç¨€ç–çŸ©é˜µ
adj_sparse = adj_matrix.to_sparse()

# åœ¨GATä¸­ä½¿ç”¨ç¨€ç–çŸ©é˜µä¹˜æ³•
output = torch.sparse.mm(adj_sparse, features)
```

**æ•ˆæœ**: æ˜¾å­˜ â†“ 80% (å¯¹äºç¨€ç–å›¾)

---

### 4. ä½¿ç”¨bfloat16ä»£æ›¿float16

```python
# åœ¨AMPé…ç½®ä¸­
torch.set_autocast_gpu_dtype(torch.bfloat16)

with torch.cuda.amp.autocast(dtype=torch.bfloat16):
    pred = model(...)
```

**ä¼˜åŠ¿**:
- æ•°å€¼ç¨³å®šæ€§ â†‘ (æ›´å¤§çš„åŠ¨æ€èŒƒå›´)
- æº¢å‡ºé£é™© â†“
- A100/3090+æ”¯æŒç¡¬ä»¶åŠ é€Ÿ

---

## ğŸ“ˆ ç›‘æ§ä¸è¯Šæ–­

### 1. GPUåˆ©ç”¨ç‡ç›‘æ§

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–åœ¨è®­ç»ƒè„šæœ¬ä¸­
import subprocess
result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
print(result.stdout)
```

**æœŸæœ›å€¼**: 
- GPUåˆ©ç”¨ç‡: 90-95%
- æ˜¾å­˜ä½¿ç”¨: 40-45GB

### 2. æ•°æ®åŠ è½½ç“¶é¢ˆæ£€æµ‹

```python
import time

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
data_time = 0
compute_time = 0

for batch in train_loader:
    t0 = time.time()
    # æ•°æ®å·²åŠ è½½
    data_time += time.time() - t0
    
    t1 = time.time()
    # å‰å‘+åå‘ä¼ æ’­
    loss = train_step(batch)
    compute_time += time.time() - t1

print(f"æ•°æ®åŠ è½½æ—¶é—´: {data_time:.1f}s ({data_time/(data_time+compute_time)*100:.1f}%)")
print(f"è®¡ç®—æ—¶é—´: {compute_time:.1f}s ({compute_time/(data_time+compute_time)*100:.1f}%)")
```

**ç†æƒ³å€¼**: 
- æ•°æ®åŠ è½½: <10%
- è®¡ç®—æ—¶é—´: >90%

---

### 3. æ˜¾å­˜ä½¿ç”¨åˆ†æ

```python
import torch

def print_memory_stats():
    allocated = torch.cuda.memory_allocated() / (1024**3)
    reserved = torch.cuda.memory_reserved() / (1024**3)
    max_allocated = torch.cuda.max_memory_allocated() / (1024**3)
    
    print(f"æ˜¾å­˜ä½¿ç”¨:")
    print(f"  å·²åˆ†é…: {allocated:.2f} GB")
    print(f"  å·²é¢„ç•™: {reserved:.2f} GB")
    print(f"  å³°å€¼: {max_allocated:.2f} GB")

# åœ¨è®­ç»ƒå¼€å§‹ã€ç»“æŸæ—¶è°ƒç”¨
print_memory_stats()
```

---

## ğŸ›ï¸ åŠ¨æ€è°ƒä¼˜ç­–ç•¥

### é˜¶æ®µ1: å¿«é€Ÿå®éªŒï¼ˆè§„åˆ™æ¨¡å¼ï¼‰

```python
# ä¸´æ—¶é…ç½® - ç”¨äºå¿«é€ŸéªŒè¯
QUICK_CONFIG = {
    'n_embd': 256,
    'n_layers': 3,
    'batch_size': 1024,
    'epochs': 5,  # åªè®­ç»ƒ5ä¸ªepoch
}

# build_graph.py --max_per_ticker 30 --max_total 15000
```

**ç”¨é€”**: éªŒè¯ä»£ç ã€è°ƒè¯•é”™è¯¯ã€å¿«é€Ÿè¿­ä»£

---

### é˜¶æ®µ2: ä¸­ç­‰è®­ç»ƒï¼ˆè§„åˆ™æ¨¡å¼ï¼‰

```python
MEDIUM_CONFIG = {
    'n_embd': 384,
    'n_layers': 4,
    'batch_size': 1024,
    'epochs': 20,
}

# build_graph.py --max_per_ticker 50
```

**ç”¨é€”**: åˆæ­¥ç»“æœã€è¶…å‚æ•°æœç´¢

---

### é˜¶æ®µ3: å®Œæ•´è®­ç»ƒï¼ˆLLMæ¨¡å¼ï¼‰â­

```python
FULL_CONFIG = {
    'n_embd': 384,
    'n_layers': 4,
    'batch_size': 1024,
    'epochs': 30,
}

# build_graph.py --use_llm
```

**ç”¨é€”**: æœ€ç»ˆè®ºæ–‡ç»“æœ

---

## ğŸ”¥ æé™æ€§èƒ½é…ç½®

å¦‚æœæƒ³æ¦¨å¹²æ€§èƒ½ï¼š

```python
EXTREME_CONFIG = {
    # æ›´å¤§çš„æ¨¡å‹
    'n_embd': 512,       # 384 â†’ 512
    'n_layers': 5,       # 4 â†’ 5
    'gnn_embd': 128,     # 96 â†’ 128
    
    # æ›´å¤§çš„batch
    'batch_size': 1536,  # 1024 â†’ 1536 (ä¼°è®¡æ˜¾å­˜çº¦45GB)
    
    # æ›´å¤šè®­ç»ƒ
    'epochs': 50,
    
    # æ›´æ¿€è¿›çš„æ•°æ®åŠ è½½
    'num_workers': 12,   # ç”¨æ»¡æ‰€æœ‰CPUæ ¸å¿ƒ
    'prefetch_factor': 8,
}
```

**é£é™©**:
- æ˜¾å­˜å¯èƒ½ä¸è¶³ï¼ˆéœ€è¦å®æµ‹ï¼‰
- è¿‡æ‹Ÿåˆé£é™©å¢åŠ 
- è®­ç»ƒæ—¶é—´æ›´é•¿

**å»ºè®®**: å…ˆç”¨æ ‡å‡†é…ç½®ï¼ˆ384/4/96/1024ï¼‰ï¼Œæ•ˆæœä¸ç†æƒ³å†å°è¯•æé™é…ç½®

---

## ğŸ“ é…ç½®æ€»ç»“

### æ¨èé…ç½®ï¼ˆ48GBæ˜¾å­˜æ ‡å‡†ç‰ˆï¼‰

```python
{
    'n_embd': 384,
    'n_layers': 4,
    'gnn_embd': 96,
    'batch_size': 1024,
    'epochs': 30,
    'lr': 3e-4,
    'num_workers': 10,
    'prefetch_factor': 6,
}

# å›¾è°±æ„å»º
python -m dataProcessed.build_graph --use_llm
```

### é¢„æœŸè®­ç»ƒæ—¶é—´

```
å›¾è°±æ„å»º (LLM): 60-90åˆ†é’Ÿ (ä¸€æ¬¡æ€§)
Full Model: 150åˆ†é’Ÿ (30 epochs Ã— 5åˆ†é’Ÿ)
æ¶ˆèå®éªŒ: 750åˆ†é’Ÿ (5 Ã— 150åˆ†é’Ÿ)
æ€»è®¡: ~15å°æ—¶ (å«LLMæ„å›¾)
```

### é¢„æœŸæ€§èƒ½

```
æµ‹è¯•é›†æŒ‡æ ‡:
  IC: 0.110 (ç›®æ ‡: >0.10)
  RankIC: 0.135 (ç›®æ ‡: >0.12)
  æ–¹å‘å‡†ç¡®ç‡: 55.8% (ç›®æ ‡: >55%)
  MSE: 0.00125
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡è¿è¡ŒLLM**: éœ€è¦ä¸‹è½½Qwen2.5-14Bæ¨¡å‹ï¼ˆ~26GBï¼‰
2. **ç£ç›˜ç©ºé—´**: ç¡®ä¿æ•°æ®ç›˜æœ‰è¶³å¤Ÿç©ºé—´ï¼ˆå»ºè®®50GB+ï¼‰
3. **SSHæ–­è¿**: ä½¿ç”¨`screen`æˆ–`tmux`
4. **æ˜¾å­˜ç›‘æ§**: å®šæœŸè¿è¡Œ`nvidia-smi`æ£€æŸ¥
5. **å¤‡ä»½**: è®­ç»ƒå‰å¤‡ä»½æ•°æ®å’Œä»£ç 

---

## ğŸš€ ä¸€é”®å¯åŠ¨è„šæœ¬

```bash
#!/bin/bash
# run_optimized.sh - 48GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆ

set -e
cd /root/autodl-tmp/paper

echo "=== æœåŠ¡å™¨é…ç½®æ£€æµ‹ ==="
nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
free -h | grep Mem
nproc

echo "=== Step 1: LLMå›¾è°±æ„å»º (é¢„è®¡60-90åˆ†é’Ÿ) ==="
python -m dataProcessed.build_graph --use_llm

echo "=== Step 2: å®Œæ•´æ¨¡å‹è®­ç»ƒ (é¢„è®¡150åˆ†é’Ÿ) ==="
python -m training.train_full

echo "=== Step 3: æ¶ˆèå®éªŒ (é¢„è®¡750åˆ†é’Ÿ) ==="
python -m training.train_ablation

echo "=== Step 4: è¯„ä¼° ==="
python -m evaluation.evaluate_all
python -m evaluation.evaluate_by_group

echo "=== å®Œæˆï¼==="
echo "ç»“æœä¿å­˜åœ¨ outputs/ ç›®å½•"
```

è¿è¡Œ:
```bash
chmod +x run_optimized.sh
screen -S training
./run_optimized.sh
# Ctrl+A, D åˆ†ç¦»ä¼šè¯
```

---

**æ€»ç»“**: 48GBæ˜¾å­˜æ˜¯ç†æƒ³é…ç½®ï¼Œå¯ä»¥ï¼š
1. âœ… å¯ç”¨LLMè·å¾—é«˜è´¨é‡å›¾è°±
2. âœ… å¢å¤§æ¨¡å‹å®¹é‡æå‡æ€§èƒ½
3. âœ… å¢å¤§batch sizeæé«˜æ•ˆç‡
4. âœ… å……åˆ†åˆ©ç”¨CPUå’Œå†…å­˜

é¢„æœŸæ•´ä½“æ€§èƒ½æå‡ **20-30%**ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ³¢åŠ¨è‚¡ç¥¨ä¸Šçš„è¡¨ç°ã€‚
