# å¿«é€Ÿå¼€å§‹æŒ‡å—

> 5åˆ†é’Ÿä¸Šæ‰‹ QL-MATCC-GNN é¡¹ç›®

## å‰ç½®æ¡ä»¶

### ç¡¬ä»¶è¦æ±‚
- **GPU**: 48GBæ˜¾å­˜ï¼ˆvGPU/RTX 3090/A6000ç­‰ï¼‰
- **CPU**: 8æ ¸+ (æ¨è12æ ¸)
- **RAM**: 32GB+ (æ¨è90GB)
- **å­˜å‚¨**: 100GB+ å¯ç”¨ç©ºé—´

### è½¯ä»¶ç¯å¢ƒ
```bash
OS: Ubuntu 22.04
Python: 3.10
CUDA: 11.8
PyTorch: 2.1.2+
```

### é»˜è®¤é…ç½®è¯´æ˜

**è®ºæ–‡å¤ç°é»˜è®¤é…ç½®**ï¼š
- âœ… **LLMé»˜è®¤å¯ç”¨**: ä½¿ç”¨Qwen2.5-14Bæå–è¯­ä¹‰å…³ç³»
- âœ… **æ¨¡å‹é…ç½®**: n_embd=256, n_layers=3, gnn_embd=64
- âœ… **è®­ç»ƒé…ç½®**: batch_size=512, epochs=20
- âœ… **æ•°æ®é‡‡æ ·**: 200æ¡/è‚¡ç¥¨ (é«˜è´¨é‡å›¾è°±)

**å…³äºâ€œæ›´å¤§é…ç½®/48GBä¼˜åŒ–â€**ï¼š
å½“å‰ `training/train_full.py` / `training/train_ablation.py` **æœªæä¾› CLI è¶…å‚**ï¼ˆæ²¡æœ‰ `--n_embd/--batch_size/...` å‚æ•°ï¼‰ã€‚
å¦‚éœ€æ›´å¤§æ¨¡å‹/æ›´å¤§ batch/æ›´å¤š epochï¼Œè¯·ç›´æ¥ä¿®æ”¹è„šæœ¬å†…çš„ `CONFIG` / `BASE_CONFIG` å­—å…¸ï¼ˆREADME å·²è¯´æ˜ä½ç½®ï¼‰ã€‚

## ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒé…ç½®

### 1.1 å®‰è£… PyTorch

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 1.2 å®‰è£…å…¶ä»–ä¾èµ–

```bash
pip install pennylane pandas numpy scikit-learn matplotlib tqdm akshare
```

### 1.3 éªŒè¯å®‰è£…

```python
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
python -c "import pennylane as qml; print(f'PennyLaneç‰ˆæœ¬: {qml.__version__}')"
```

## ç¬¬äºŒæ­¥ï¼šæ•°æ®å‡†å¤‡

### 2.1 æ•°æ®ç›®å½•ç»“æ„

```
paper/data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ FNSPID/
â”‚       â”œâ”€â”€ full_history/      # ä¸ªè‚¡CSVæ–‡ä»¶
â”‚       â”‚   â”œâ”€â”€ AAPL.csv
â”‚       â”‚   â”œâ”€â”€ MSFT.csv
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ nasdaq_exteral_data.csv  # æ–°é—»æ•°æ®ï¼ˆ23GBï¼‰
â”‚       â””â”€â”€ SP500_Index.csv    # S&P 500æŒ‡æ•°ï¼ˆå¯è‡ªåŠ¨ä¸‹è½½ï¼‰
â””â”€â”€ processed/                 # è‡ªåŠ¨ç”Ÿæˆ
```

### 2.2 ä¸‹è½½S&P 500æŒ‡æ•°ï¼ˆå¯é€‰ï¼‰

å¦‚æœ `SP500_Index.csv` ä¸å­˜åœ¨ï¼š

```bash
cd paper
python -m dataProcessed.download_market_index
```

**æ‰‹åŠ¨æ–¹å¼**ï¼šä¹Ÿå¯ä» Yahoo Finance ä¸‹è½½ï¼Œæ ¼å¼ä¸ºï¼š
```csv
Date,Market_Close,Market_Vol
2019-01-02,2510.03,3682930000
...
```

## ç¬¬ä¸‰æ­¥ï¼šæ•°æ®å¤„ç†

### 3.1 ETLå¤„ç†ï¼ˆåˆå¹¶è‚¡ä»·+æ–°é—»ï¼‰

```bash
python -m dataProcessed.etl
```

**è¾“å‡º**ï¼š
- `data/processed/Stock_Prices.csv` (~500MB)
- `data/processed/Stock_News.csv` (~2GB)

**è€—æ—¶**: çº¦5-10åˆ†é’Ÿ

### 3.2 æ•°æ®å¯¹é½ï¼ˆè®¡ç®—ç‰¹å¾ï¼‰

```bash
python -m dataProcessed.align
```

**è¾“å‡º**ï¼š
- `data/processed/Final_Model_Data.csv` (~800MB)

**è€—æ—¶**: çº¦2-5åˆ†é’Ÿ

### 3.3 æ„å»ºå›¾è°±ï¼ˆS&P 500æˆåˆ†è‚¡ï¼‰

```bash
python -m dataProcessed.build_graph
```

**è¾“å‡º**ï¼š
- `data/processed/Graph_Adjacency.npy`
- `data/processed/Graph_Adjacency_tickers.json`ï¼ˆå¼ºçƒˆå»ºè®®ä¿ç•™ï¼šç”¨äºæ ¡éªŒå›¾è°±èŠ‚ç‚¹é¡ºåºï¼Œé¿å…â€œé™é»˜é”™ä½â€ï¼‰

**è€—æ—¶**:
- LLM æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šé€šå¸¸ä¸ºæ•°å°æ—¶é‡çº§ï¼ˆä¸ batch/æ¨ç†å‚æ•°/GPU ååç›¸å…³ï¼‰
- è§„åˆ™æ¨¡å¼ï¼šçº¦1-3åˆ†é’Ÿ

**é«˜çº§é€‰é¡¹**ï¼š
```bash
# å¼ºåˆ¶ä½¿ç”¨ LLM æå–å…³ç³»ï¼ˆéœ€è¦å¤§æ˜¾å­˜ï¼‰
python -m dataProcessed.build_graph --use_llm

# å¼ºåˆ¶ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼ˆæ›´å¿«ï¼Œæ˜¾å­˜å ç”¨å°ï¼‰
python -m dataProcessed.build_graph --no_llm

# ä½¿ç”¨å…¨é‡è‚¡ç¥¨ï¼ˆä¸æ¨èï¼‰
python -m dataProcessed.build_graph --all_stocks

# è‡ªå®šä¹‰é‡‡æ ·å‚æ•°
python -m dataProcessed.build_graph --max_per_ticker 100 --max_total 100000
```

## ç¬¬å››æ­¥ï¼šè®­ç»ƒæ¨¡å‹

### 4.1 è®­ç»ƒå®Œæ•´æ¨¡å‹

```bash
python -m training.train_full
```

**è¾“å‡º**ï¼š
- `outputs/checkpoints/best_model_full.pth`
- `outputs/logs/training_losses_full.json`
- `outputs/figures/training_curve_full.png`

**è€—æ—¶**: çº¦2-4å°æ—¶ï¼ˆ20 epochsï¼‰

### 4.2 è¿è¡Œæ¶ˆèå®éªŒ

```bash
python -m training.train_ablation
```

**è¾“å‡º**ï¼š
- `outputs/checkpoints/best_model_*.pth`ï¼ˆ3ä¸ªæ¶ˆèæ¨¡å‹ï¼‰
- `outputs/logs/losses_*.json`ï¼ˆæ¯ä¸ªå®éªŒä¸€ä¸ªï¼‰
- `outputs/results/ablation_results_summary.csv`
- `outputs/figures/ablation_comparison.png`

**è€—æ—¶**: çº¦6-12å°æ—¶ï¼ˆ3ä¸ªå®éªŒ Ã— 20 epochsï¼‰

## ç¬¬äº”æ­¥ï¼šè¯„ä¼°æ¨¡å‹

### 5.1 ç»Ÿä¸€è¯„ä¼°

```bash
python -m evaluation.evaluate_all
```

**è¾“å‡º**ï¼š
- `outputs/results/evaluation_overall.csv`
- `outputs/results/evaluation_by_volatility.csv`
- `outputs/figures/evaluation_comparison.png`

### 5.2 åˆ†ç»„è¯„ä¼°

```bash
python -m evaluation.evaluate_by_group
```

**è¾“å‡º**ï¼š
- `outputs/results/group_evaluation_*.csv`

## å¸¸è§é—®é¢˜

### Q1: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨ train_full.py ä¸­è°ƒæ•´å‚æ•°
batch_size = 256  # ä»512é™åˆ°256
n_embd = 128      # ä»256é™åˆ°128
```

### Q2: build_graph.py è¿è¡Œå¾ˆæ…¢

**åŸå› **: é»˜è®¤å¯ç”¨ LLM æ¨¡å¼ï¼Œéœ€è¦åŠ è½½ 14B å‚æ•°æ¨¡å‹å¹¶å¯¹å¤§é‡æ–°é—»åšæ¨ç†

**è§£å†³æ–¹æ¡ˆ**: å¦‚åªæƒ³å…ˆè·‘é€šæµç¨‹/æ˜¾å­˜ä¸è¶³ï¼Œå¯åˆ‡æ¢åˆ°è§„åˆ™æ¨¡å¼
```bash
python -m dataProcessed.build_graph --no_llm
```

### Q3: æ•°æ®æ–‡ä»¶æ‰¾ä¸åˆ°

**æ£€æŸ¥è·¯å¾„**ï¼š
```bash
ls data/raw/FNSPID/full_history/ | head -5
ls data/raw/FNSPID/nasdaq_exteral_data.csv
```

**ç¡®ä¿**ï¼š
- è‚¡ç¥¨CSVæ–‡ä»¶åœ¨ `full_history/` ç›®å½•ä¸‹
- æ–‡ä»¶åæ ¼å¼ä¸º `TICKER.csv` (å¦‚ `AAPL.csv`)

### Q4: å›¾è°±è¿æ¥ç‡å¤ªä½

**æ£€æŸ¥ç»Ÿè®¡è¾“å‡º**ï¼š
```
æœ‰è¿æ¥çš„è‚¡ç¥¨æ•°: 150 / 300 (50.0%)
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```bash
# å¢åŠ æ¯è‚¡ç¥¨é‡‡æ ·æ•°
python -m dataProcessed.build_graph --max_per_ticker 100

# æˆ–ä½¿ç”¨LLMæ¨¡å¼ï¼ˆæ›´å‡†ç¡®ï¼‰
python -m dataProcessed.build_graph --use_llm
```

### Q5: è®­ç»ƒlossä¸ä¸‹é™

**å¯èƒ½åŸå› **ï¼š
1. å­¦ä¹ ç‡è¿‡å¤§/è¿‡å°
2. æ•°æ®å¼‚å¸¸å€¼æœªå¤„ç†
3. æ¨¡å‹åˆå§‹åŒ–é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. è°ƒæ•´å­¦ä¹ ç‡
lr = 1e-4  # ä»3e-4é™ä½

# 2. æ£€æŸ¥æ•°æ®
python -c "
import pandas as pd
df = pd.read_csv('data/processed/Final_Model_Data.csv')
print(df.describe())
"

# 3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
n_embd = 128
n_layers = 2
```

## ä¸‹ä¸€æ­¥

- é˜…è¯» [æ¨¡å—è¯´æ˜](æ¨¡å—è¯´æ˜.md) äº†è§£å„ä¸ªæ–‡ä»¶çš„ä½œç”¨
- é˜…è¯» [è®­ç»ƒä¸è¯„ä¼°](è®­ç»ƒä¸è¯„ä¼°.md) äº†è§£è¯¦ç»†çš„è®­ç»ƒæŠ€å·§
- é˜…è¯» [æ”¹è¿›æ–¹æ¡ˆè¯´æ˜](æ”¹è¿›æ–¹æ¡ˆè¯´æ˜.md) äº†è§£ä¼˜åŒ–å†ç¨‹

## ä¸€é”®è¿è¡Œï¼ˆ48GBä¼˜åŒ–ç‰ˆï¼‰âš¡

### æ¨èæ–¹å¼: ä½¿ç”¨ä¼˜åŒ–è„šæœ¬

```bash
cd /root/autodl-tmp/paper

# åˆ›å»ºscreenä¼šè¯ï¼ˆé˜²æ­¢SSHæ–­è¿ï¼‰
screen -S ql_matcc

# è¿è¡Œä¼˜åŒ–è„šæœ¬ï¼ˆå·²é’ˆå¯¹48GBæ˜¾å­˜ä¼˜åŒ–ï¼‰
./run_optimized.sh

# åˆ†ç¦»ä¼šè¯: Ctrl+A, D
# é‡æ–°è¿æ¥: screen -r ql_matcc
# æŸ¥çœ‹æ‰€æœ‰ä¼šè¯: screen -ls
```

### æ‰‹åŠ¨é€æ­¥è¿è¡Œ

```bash
cd /root/autodl-tmp/paper

# Step 1: æ•°æ®å¤„ç†
python -m dataProcessed.etl
python -m dataProcessed.align
python -m dataProcessed.build_graph --use_llm  # LLMæ¨¡å¼ (60-90åˆ†é’Ÿ)

# Step 2: è®­ç»ƒå®Œæ•´æ¨¡å‹
python -m training.train_full  # 120-150åˆ†é’Ÿ (20 epochs)

# Step 3: æ¶ˆèå®éªŒï¼ˆå¯é€‰ï¼‰
python -m training.train_ablation  # 360-450åˆ†é’Ÿ (3Ã—20 epochs)

# Step 4: è¯„ä¼°
python -m evaluation.evaluate_all
python -m evaluation.evaluate_by_group
```

---

**é¢„è®¡æ—¶é—´** (é»˜è®¤é…ç½®):
- ğŸ”„ æ•°æ®å¤„ç†: 70-100åˆ†é’Ÿï¼ˆå«LLMå›¾è°±ï¼‰
- ğŸ“ å®Œæ•´æ¨¡å‹: 120-150åˆ†é’Ÿ (20 epochs)
- ğŸ§ª æ¶ˆèå®éªŒ: 360-450åˆ†é’Ÿ (3ä¸ªå®éªŒ Ã— 20 epochs)
- ğŸ“Š è¯„ä¼°: 10åˆ†é’Ÿ
- **æ€»è®¡**: ~9-12å°æ—¶

**æ³¨æ„äº‹é¡¹**:
1. âœ… ä½¿ç”¨ `screen` æˆ– `tmux` é˜²æ­¢SSHæ–­è¿
2. âœ… ç£ç›˜ç©ºé—´å……è¶³ï¼ˆè‡³å°‘50GBå¯ç”¨ï¼‰
3. âœ… é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½Qwen2.5-14Bæ¨¡å‹ï¼ˆ~26GBï¼‰
4. âœ… å®šæœŸè¿è¡Œ `nvidia-smi` ç›‘æ§æ˜¾å­˜ä½¿ç”¨
