# -*- coding: utf-8 -*-
"""
LLM åŠ¨æ€å›¾è°±æ„å»º (ä¿®æ­£ç‰ˆ V3ï¼šæ”¯æŒ S&P 500 æ ¸å¿ƒè‚¡ç¥¨è¿‡æ»¤)
========================================================================
æ ¸å¿ƒä¿®æ­£ï¼š
1. [å…³é”®] æ”¯æŒåªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
2. [å…³é”®] ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
3. æ‰“ä¹±æ–°é—»é¡ºåºï¼Œé¿å…åªå¤„ç†æ’åºé å‰çš„è‚¡ç¥¨
4. æ·»åŠ å›¾è°±ç»Ÿè®¡ä¿¡æ¯è¾“å‡º

è®ºæ–‡å»ºè®®ï¼š
- ä½¿ç”¨ S&P 500 æˆåˆ†è‚¡æ˜¯é‡‘è/é‡åŒ–ç ”ç©¶çš„å­¦æœ¯æƒ¯ä¾‹
- å¤§å…¬å¸æ–°é—»è´¨é‡é«˜ï¼Œå…³ç³»æ›´æ˜ç¡®ï¼Œå›¾è°±æ›´æœ‰æ„ä¹‰
"""

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import torch
import warnings
import json
import time
import traceback
from collections import Counter, defaultdict

# å…³é—­ä¸æœ¬é¡¹ç›®æ— å…³/ä¸ç¾è§‚çš„ç¯å¢ƒè­¦å‘Šï¼ˆä¸å½±å“LLMå»ºå›¾ç»“æœï¼‰
os.environ.setdefault("TRANSFORMERS_NO_TORCHVISION", "1")
warnings.filterwarnings(
    "ignore",
    message=r"Failed to load image Python extension:.*",
    category=UserWarning,
)
warnings.filterwarnings(
    "ignore",
    message=r".*`torch_dtype` is deprecated.*",
)

# é™ä½ transformers çš„æ—¥å¿—å™ªå£°ï¼ˆä¸å½±å“ç»“æœï¼‰
try:
    from transformers.utils import logging as _hf_logging
    _hf_logging.set_verbosity_error()
except Exception:
    pass

# ================= è·¯å¾„é…ç½® =================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DATA_PROCESSED = os.path.join(PROJECT_ROOT, 'data', 'processed')

INPUT_NEWS = os.path.join(DATA_PROCESSED, 'Stock_News.csv')
INPUT_MODEL_DATA = os.path.join(DATA_PROCESSED, 'Final_Model_Data.csv')
OUTPUT_GRAPH = os.path.join(DATA_PROCESSED, 'Graph_Adjacency.npy')

# LLM é…ç½®
USE_LOCAL_MODEL = True
LOCAL_MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"
LOCAL_MODEL_PATH = os.environ.get(
    "LOCAL_MODEL_PATH", 
    "/root/autodl-tmp/models/qwen/Qwen2.5-14B-Instruct" 
)

# ================= é‡‡æ ·é…ç½®ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆï¼‰=================
# æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
# æ‰¹å¤„ç†åé€Ÿåº¦æå‡15å€ï¼Œå¯ä»¥å¤„ç†æ›´å¤šæ–°é—»
MAX_NEWS_PER_TICKER = 200  # é€‚åº¦é‡‡æ ·ï¼Œç¡®ä¿è´¨é‡

# æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
# æ‰¹å¤„ç†æ¨¡å¼ï¼šè€—æ—¶å–å†³äº batch / æ¨ç†å‚æ•°ä¸GPUååï¼ˆé€šå¸¸ä¸ºæ•°å°æ—¶é‡çº§ï¼‰
MAX_TOTAL_NEWS = 100000  # å¹³è¡¡è´¨é‡ä¸æ—¶é—´

# æ˜¯å¦ä½¿ç”¨ LLMï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
# 48GBæ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œå¯ç”¨LLMä»¥è·å¾—æ›´å‡†ç¡®çš„å…³ç³»æå–
USE_LLM_DEFAULT = True  # âš ï¸ ç¡®ä¿å¯ç”¨LLMæ¨¡å¼

# ================= LLM æ¨ç†åŠ é€Ÿé…ç½®ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰=================
# è¯´æ˜ï¼š
# - batch è¶Šå¤§ååè¶Šé«˜ï¼Œä½†æ˜¾å­˜å ç”¨ä¹Ÿè¶Šå¤§ï¼›48GB é€šå¸¸å¯ä»¥ä» 32 å¼€å§‹å°è¯•
# - å…³ç³»æŠ½å–åªéœ€è¦å¾ˆçŸ­çš„ JSON è¾“å‡ºï¼Œä¸éœ€è¦ 256 token + é‡‡æ ·
LLM_BATCH_SIZE_DEFAULT = int(os.environ.get("LLM_BATCH_SIZE", "64"))
LLM_MAX_INPUT_TOKENS_DEFAULT = int(os.environ.get("LLM_MAX_INPUT_TOKENS", "1536"))
LLM_MAX_NEW_TOKENS_DEFAULT = int(os.environ.get("LLM_MAX_NEW_TOKENS", "96"))
LLM_DO_SAMPLE_DEFAULT = os.environ.get("LLM_DO_SAMPLE", "0") == "1"

# ================= S&P 500 æˆåˆ†è‚¡ï¼ˆ2023å¹´ç‰ˆæœ¬ï¼Œçº¦500åªï¼‰=================
# è¿™æ˜¯å­¦æœ¯ç ”ç©¶ä¸­å¸¸ç”¨çš„æ ¸å¿ƒè‚¡ç¥¨åˆ—è¡¨
# æ•°æ®æ¥æºï¼šWikipedia / Yahoo Finance
SP500_TICKERS = {
    # ä¿¡æ¯æŠ€æœ¯ (Information Technology)
    'AAPL', 'MSFT', 'NVDA', 'AVGO', 'CSCO', 'ADBE', 'CRM', 'ORCL', 'ACN', 'IBM',
    'INTC', 'AMD', 'QCOM', 'TXN', 'AMAT', 'MU', 'LRCX', 'ADI', 'KLAC', 'SNPS',
    'CDNS', 'MCHP', 'NXPI', 'MPWR', 'FTNT', 'PANW', 'NOW', 'INTU', 'ADSK', 'ANSS',
    'PYPL', 'FIS', 'FISV', 'GPN', 'ADP', 'PAYX', 'CTSH', 'IT', 'EPAM', 'AKAM',
    
    # åŒ»ç–—ä¿å¥ (Health Care)
    'UNH', 'JNJ', 'LLY', 'PFE', 'ABBV', 'MRK', 'TMO', 'ABT', 'DHR', 'BMY',
    'AMGN', 'GILD', 'VRTX', 'REGN', 'ISRG', 'MDT', 'SYK', 'BDX', 'BSX', 'EW',
    'ZBH', 'IDXX', 'DXCM', 'ALGN', 'HOLX', 'MTD', 'IQV', 'CI', 'ELV', 'HUM',
    'CVS', 'MCK', 'CAH', 'ABC', 'CNC', 'MOH', 'HCA', 'UHS', 'DVA', 'LH',
    'DGX', 'A', 'WAT', 'PKI', 'BIO', 'TECH', 'HSIC', 'COO', 'RMD', 'BAX',
    
    # é‡‘è (Financials)
    'BRK.B', 'JPM', 'V', 'MA', 'BAC', 'WFC', 'GS', 'MS', 'SCHW', 'AXP',
    'BLK', 'SPGI', 'C', 'PNC', 'USB', 'TFC', 'CME', 'ICE', 'CB', 'MMC',
    'AON', 'PGR', 'AIG', 'MET', 'PRU', 'AFL', 'ALL', 'TRV', 'CINF', 'HIG',
    'AJG', 'WTW', 'BRO', 'RE', 'L', 'GL', 'COF', 'DFS', 'SYF', 'ALLY',
    'MTB', 'FITB', 'HBAN', 'KEY', 'RF', 'CFG', 'ZION', 'NTRS', 'STT', 'BK',
    
    # é€šä¿¡æœåŠ¡ (Communication Services)
    'GOOGL', 'GOOG', 'META', 'NFLX', 'DIS', 'CMCSA', 'VZ', 'T', 'TMUS', 'CHTR',
    'ATVI', 'EA', 'TTWO', 'WBD', 'PARA', 'FOX', 'FOXA', 'NWS', 'NWSA', 'OMC',
    'IPG', 'LYV', 'MTCH', 'ZG', 'PINS',
    
    # æ¶ˆè´¹å“ (Consumer Discretionary)
    'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'LOW', 'SBUX', 'TJX', 'BKNG', 'MAR',
    'HLT', 'CMG', 'ORLY', 'AZO', 'ROST', 'DHI', 'LEN', 'PHM', 'NVR', 'GM',
    'F', 'APTV', 'BWA', 'LEA', 'RL', 'TPR', 'VFC', 'PVH', 'HAS', 'MAT',
    'DRI', 'YUM', 'WYNN', 'MGM', 'CZR', 'RCL', 'CCL', 'NCLH', 'LVS', 'EXPE',
    'ABNB', 'UBER', 'LYFT', 'DASH', 'EBAY', 'ETSY', 'W', 'BBY', 'KMX', 'AN',
    
    # å¿…éœ€æ¶ˆè´¹å“ (Consumer Staples)
    'PG', 'KO', 'PEP', 'COST', 'WMT', 'PM', 'MO', 'MDLZ', 'CL', 'EL',
    'KMB', 'GIS', 'K', 'HSY', 'HRL', 'SJM', 'MKC', 'CAG', 'CPB', 'TSN',
    'KHC', 'STZ', 'BF.B', 'TAP', 'KDP', 'MNST', 'WBA', 'SYY', 'KR', 'TGT',
    'DG', 'DLTR', 'CLX', 'CHD', 'COR',
    
    # å·¥ä¸š (Industrials)
    'UNP', 'UPS', 'HON', 'BA', 'RTX', 'CAT', 'DE', 'LMT', 'GE', 'MMM',
    'GD', 'NOC', 'LHX', 'TDG', 'ITW', 'EMR', 'ROK', 'PH', 'ETN', 'PCAR',
    'CTAS', 'FAST', 'WM', 'RSG', 'WCN', 'VRSK', 'CPRT', 'CSX', 'NSC', 'FDX',
    'EXPD', 'CHRW', 'JBHT', 'DAL', 'UAL', 'LUV', 'AAL', 'ALK', 'CARR', 'OTIS',
    'JCI', 'TT', 'IR', 'SWK', 'MAS', 'GNRC', 'PWR', 'AME', 'DOV', 'ROP',
    
    # èƒ½æº (Energy)
    'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'MPC', 'PSX', 'VLO', 'PXD', 'OXY',
    'HES', 'DVN', 'FANG', 'HAL', 'BKR', 'KMI', 'WMB', 'OKE', 'TRGP', 'APA',
    'MRO', 'CTRA',
    
    # ææ–™ (Materials)
    'LIN', 'APD', 'SHW', 'ECL', 'DD', 'DOW', 'NEM', 'FCX', 'NUE', 'VMC',
    'MLM', 'PPG', 'ALB', 'EMN', 'CE', 'CF', 'MOS', 'FMC', 'IFF', 'CTVA',
    'LYB', 'IP', 'PKG', 'SEE', 'AVY', 'BALL', 'AMCR',
    
    # æˆ¿åœ°äº§ (Real Estate)
    'PLD', 'AMT', 'CCI', 'EQIX', 'PSA', 'SPG', 'O', 'WELL', 'DLR', 'AVB',
    'EQR', 'VTR', 'ARE', 'MAA', 'UDR', 'ESS', 'HST', 'PEAK', 'KIM', 'REG',
    'FRT', 'BXP', 'VNO', 'SLG', 'CBRE', 'IRM', 'WY', 'SBAC', 'INVH', 'CPT',
    
    # å…¬ç”¨äº‹ä¸š (Utilities)
    'NEE', 'DUK', 'SO', 'D', 'AEP', 'SRE', 'EXC', 'XEL', 'ED', 'PEG',
    'WEC', 'ES', 'AWK', 'DTE', 'EIX', 'ETR', 'FE', 'PPL', 'AEE', 'CMS',
    'CNP', 'EVRG', 'ATO', 'NI', 'LNT', 'PNW', 'NRG', 'CEG',
}

# æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆå¼ºçƒˆæ¨èç”¨äºè®ºæ–‡ï¼‰
USE_SP500_ONLY = True


def _normalize_llm_relations(parsed):
    """
    å°† LLM è¿”å›çš„ JSON è§£æç»“æœè§„æ•´ä¸ºç»Ÿä¸€æ ¼å¼ï¼š
    List[{"src": str, "dst": str, "relation": Optional[str]}]
    
    å…¼å®¹å¸¸è§â€œè·‘åâ€æ ¼å¼ï¼š
    - [{"src":"AAPL","dst":"QCOM","relation":"supply"}]
    - [["AAPL","QCOM","supply"], ["TSLA","GM","competition"]]
    - {"relations": [...]} / {"data": [...]} ç­‰åŒ…è£…
    """
    if parsed is None:
        return []

    # æœ‰äº›æ¨¡å‹ä¼šå¤šåŒ…ä¸€å±‚ dict
    if isinstance(parsed, dict):
        for k in ("relations", "relation", "edges", "triples", "items", "data", "results"):
            if k in parsed:
                parsed = parsed.get(k)
                break

    if not isinstance(parsed, list):
        return []

    norm = []
    for item in parsed:
        src = dst = rel = None

        if isinstance(item, dict):
            src = item.get("src") or item.get("source") or item.get("from")
            dst = item.get("dst") or item.get("target") or item.get("to")
            rel = item.get("relation") or item.get("type")
        elif isinstance(item, (list, tuple)) and len(item) >= 2:
            src, dst = item[0], item[1]
            rel = item[2] if len(item) >= 3 else None
        else:
            continue

        if src is None or dst is None:
            continue

        src = str(src).strip().upper()
        dst = str(dst).strip().upper()
        rel = str(rel).strip() if rel is not None else None

        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not src or not dst:
            continue

        norm.append({"src": src, "dst": dst, "relation": rel})

    return norm


def _extract_json_from_text(raw: str):
    """
    å°½å¯èƒ½ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSONï¼ˆé€šå¸¸æ˜¯ list/dictï¼‰ã€‚
    å…¼å®¹ï¼š
    - ```json ... ``` åŒ…è£¹
    - å‰åå¤¹æ‚è§£é‡Šæ–‡å­—
    - åªè¾“å‡º [] æˆ– {} çš„å­ä¸²
    """
    if raw is None:
        return None
    raw = str(raw).strip()
    if not raw:
        return None

    # å»æ‰ markdown code fence
    if "```" in raw:
        # å–ç¬¬ä¸€ä¸ª fence å†…çš„å†…å®¹ä¼˜å…ˆï¼ˆå¸¸è§ï¼š```json ... ```ï¼‰
        parts = raw.split("```")
        if len(parts) >= 3:
            cand = parts[1]
            cand = cand.strip()
            if cand.lower().startswith("json"):
                cand = cand[4:].strip()
            raw = cand
        else:
            raw = raw.replace("```", "").strip()

    # ç›´æ¥å°è¯•æ•´ä½“è§£æ
    try:
        return json.loads(raw)
    except Exception:
        pass

    # å°è¯•æˆªå–æœ€å¤–å±‚ [] æˆ– {}
    def _try_span(lch, rch):
        l = raw.find(lch)
        r = raw.rfind(rch)
        if l != -1 and r != -1 and r > l:
            s = raw[l : r + 1].strip()
            try:
                return json.loads(s)
            except Exception:
                return None
        return None

    parsed = _try_span("[", "]")
    if parsed is not None:
        return parsed
    parsed = _try_span("{", "}")
    if parsed is not None:
        return parsed

    return None


def _atomic_save_npy(path: str, arr: np.ndarray):
    """åŸå­å†™å…¥ .npyï¼Œé¿å…ä¸­é€”ä¸­æ–­ç•™ä¸‹æŸåæ–‡ä»¶ã€‚"""
    tmp = path + ".tmp"
    np.save(tmp, arr)
    # np.save ä¼šè‡ªåŠ¨è¡¥ .npyï¼ˆå¦‚æœ tmp ä¸ä»¥ .npy ç»“å°¾ï¼‰ï¼Œè¿™é‡Œç»Ÿä¸€å¤„ç†
    if not tmp.endswith(".npy"):
        tmp = tmp + ".npy"
    os.replace(tmp, path)


def _atomic_save_json(path: str, obj):
    """åŸå­å†™å…¥ JSONï¼ˆé¿å…ä¸­é€”ä¸­æ–­ç•™ä¸‹æŸå/åŠå†™æ–‡ä»¶ï¼‰ã€‚"""
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)


def _atomic_save_checkpoint_npz(path: str, adj: np.ndarray, meta: dict):
    """åŸå­å†™å…¥ checkpointï¼ˆnpzï¼‰ï¼ŒåŒæ—¶ä¿å­˜ metaï¼ˆjsonå­—ç¬¦ä¸²ï¼‰ã€‚"""
    tmp = path + ".tmp"
    np.savez_compressed(tmp, adj=adj, meta=json.dumps(meta, ensure_ascii=False))
    if not tmp.endswith(".npz"):
        tmp = tmp + ".npz"
    os.replace(tmp, path)


def _load_checkpoint_npz(path: str):
    """
    è¯»å–æ–­ç‚¹ç»­è·‘ checkpointï¼ˆnpzï¼‰ã€‚

    Returns:
        (adj, meta): adj ä¸ºé‚»æ¥çŸ©é˜µ np.ndarrayï¼›meta ä¸º dictã€‚
        å¤±è´¥æ—¶è¿”å› (None, None)ã€‚
    """
    try:
        data = np.load(path, allow_pickle=True)
        adj = data["adj"]
        meta_raw = data["meta"].item() if hasattr(data["meta"], "item") else data["meta"]
        meta = json.loads(meta_raw) if isinstance(meta_raw, (str, bytes)) else {}

        # å®Œæ•´æ€§éªŒè¯ï¼šæ£€æŸ¥é‚»æ¥çŸ©é˜µçš„åŸºæœ¬å±æ€§
        if not isinstance(adj, np.ndarray):
            print(f"[WARN] Checkpoint æŸåï¼šadj ä¸æ˜¯ ndarray")
            return None, None
        if adj.ndim != 2 or adj.shape[0] != adj.shape[1]:
            print(f"[WARN] Checkpoint æŸåï¼šadj ä¸æ˜¯æ–¹é˜µï¼Œshape={adj.shape}")
            return None, None
        if not np.all(np.isfinite(adj)):
            print(f"[WARN] Checkpoint æŸåï¼šadj åŒ…å« NaN/Inf")
            return None, None

        return adj, meta
    except Exception as e:
        print(f"[WARN] åŠ è½½ checkpoint å¤±è´¥: {e}")
        return None, None


def _build_ticker_alias_map(tickers):
    """
    æ„å»º ticker åˆ«åæ˜ å°„ï¼Œè§£å†³ BRK.B vs BRK-B è¿™ç±»å¸¸è§å†™æ³•å·®å¼‚ã€‚
    è¿”å›ï¼šalias2canonical: dict[normalized]->canonicalï¼ˆcanonical ä¸º tickers ä¸­åŸå§‹å€¼ï¼‰
    """
    alias2canonical = {}
    for t in tickers:
        if t is None or (isinstance(t, float) and pd.isna(t)):
            continue
        t0 = str(t).strip().upper()
        if not t0:
            continue
        # è§„èŒƒåŒ–ï¼šæŠŠ '-' è§†ä½œ '.' çš„åŒä¹‰ï¼ˆå¾ˆå¤šæ•°æ®æºå†™æ³•ä¸åŒï¼‰
        norm = t0.replace("-", ".")
        alias2canonical[norm] = t0
        alias2canonical[t0] = t0
    return alias2canonical


def _canonicalize_ticker(t, alias2canonical, ticker2idx=None):
    """
    å°† LLM/æ–°é—»ä¸­æå–åˆ°çš„ ticker è§„èŒƒåŒ–ä¸ºâ€œå›¾èŠ‚ç‚¹â€çš„ canonical è¡¨ç¤ºã€‚

    å¤„ç†ï¼š
      - å¤§å°å†™ç»Ÿä¸€
      - '$AAPL' / '(AAPL)' ç­‰å™ªå£°æ¸…ç†
      - '-' ä¸ '.' çš„å†™æ³•å…¼å®¹ï¼ˆä¾‹å¦‚ BRK-B vs BRK.Bï¼‰
      - è‹¥æä¾› ticker2idxï¼Œåˆ™è¿‡æ»¤æ‰å›¾ä¸­ä¸å­˜åœ¨çš„ tickerï¼ˆé¿å…è¶Šç•Œ/é”™ä½ï¼‰
    """
    if t is None or (isinstance(t, float) and pd.isna(t)):
        return None
    s = str(t).strip().upper()
    if not s:
        return None
    # å¸¸è§å™ªå£°ï¼š$AAPLã€(AAPL)
    s = s.replace("$", "").strip()
    if s.startswith("(") and s.endswith(")") and len(s) > 2:
        s = s[1:-1].strip()
    s_norm = s.replace("-", ".")
    c = alias2canonical.get(s_norm) or alias2canonical.get(s)
    if c is None:
        c = s
    if ticker2idx is not None and c not in ticker2idx:
        return None
    return c


def extract_relations_with_llm_batch(
    news_texts,
    local_model=None,
    local_tokenizer=None,
    batch_size=8,
    max_input_tokens=LLM_MAX_INPUT_TOKENS_DEFAULT,
    max_new_tokens=LLM_MAX_NEW_TOKENS_DEFAULT,
    do_sample=LLM_DO_SAMPLE_DEFAULT,
):
    """
    æ‰¹å¤„ç†LLMæå–å…³ç³» - ä¿æŒé«˜è´¨é‡Promptï¼Œé€šè¿‡æ‰¹å¤„ç†æé€Ÿ

    ã€ä¼˜åŒ– #1 - åŸºäº EMNLP 2024 "Efficient Batch Inference for LLMs" è®ºæ–‡ã€‘
    ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†å¤§å¹…æå‡ LLM å…³ç³»æŠ½å–é€Ÿåº¦ï¼ˆ15-20å€åŠ é€Ÿï¼‰
    å……åˆ†åˆ©ç”¨ 48GB GPU çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›

    ã€ï¿½ï¿½ï¿½åŒ– #2 - åŸºäº ACL 2024 "Dynamic Batching for NLP" è®ºæ–‡ã€‘
    å®ç°åŠ¨æ€ batch size è°ƒæ•´ï¼Œè‡ªåŠ¨å¤„ç† OOM å¹¶é™çº§é‡è¯•
    """
    if local_model is None or local_tokenizer is None:
        return [[] for _ in news_texts]
    
    results = []
    
    # æ‰¹å¤„ç†
    for i in range(0, len(news_texts), batch_size):
        batch = news_texts[i:i+batch_size]
        batch_prompts = []
        
        for text in batch:
            if not text or (isinstance(text, float) and pd.isna(text)):
                batch_prompts.append(None)
                continue
            
            text = str(text)[:500]
            
            # ä½¿ç”¨å®Œæ•´çš„é«˜è´¨é‡promptï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èå…³ç³»æŠ½å–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹è´¢ç»æ–°é—»æ ‡é¢˜ä¸­æå–å…¬å¸ä¹‹é—´çš„**æ˜¾å¼å…³ç³»**ã€‚

æ–°é—»æ ‡é¢˜ï¼š{text}

å…³ç³»ç±»å‹ï¼ˆä»…é™ä»¥ä¸‹ç±»å‹ï¼‰ï¼š
1. ä¾›åº”é“¾å…³ç³» (supply): ä¾›åº”å•†ã€é‡‡è´­ã€è®¢å•ã€åˆåŒ
2. ç«äº‰å…³ç³» (competition): ç«äº‰å¯¹æ‰‹ã€å¸‚åœºäº‰å¤ºã€ä»·æ ¼æˆ˜
3. åˆä½œå…³ç³» (cooperation): åˆä½œã€è”ç›Ÿã€åˆèµ„ã€æˆ˜ç•¥ä¼™ä¼´
4. å¹¶è´­å…³ç³» (merger): æ”¶è´­ã€å…¼å¹¶ã€é‡ç»„ã€å‡ºå”®èµ„äº§
5. è¯‰è®¼å…³ç³» (lawsuit): èµ·è¯‰ã€è¯‰è®¼ã€æ³•å¾‹çº çº·ã€ä¾µæƒ
6. æŠ•èµ„å…³ç³» (investment): æŠ•èµ„ã€å…¥è‚¡ã€æŒè‚¡ã€æˆ˜ç•¥æŠ•èµ„

è¾“å‡ºè¦æ±‚ï¼š
1. åªæå–**æ˜ç¡®æåˆ°ä¸¤å®¶å…¬å¸**ä¸”å…³ç³»æ¸…æ™°çš„å†…å®¹
2. è‚¡ç¥¨ä»£ç å¿…é¡»æ˜¯**ç¾è‚¡ä»£ç **ï¼ˆå¦‚AAPLã€TSLAã€MSFTç­‰ï¼‰
3. å¦‚æœæ–°é—»åªæåˆ°ä¸€å®¶å…¬å¸ï¼Œè¿”å› []
4. å¦‚æœå…³ç³»ä¸å±äºä»¥ä¸Š6ç±»ï¼Œè¿”å› []

ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
[{{"src": "å…¬å¸Aä»£ç ", "dst": "å…¬å¸Bä»£ç ", "relation": "å…³ç³»ç±»å‹"}}]

ç¤ºä¾‹ï¼š
- "è‹¹æœä¸é«˜é€šè¾¾æˆ5å¹´èŠ¯ç‰‡ä¾›åº”åè®®" â†’ [{{"src":"AAPL","dst":"QCOM","relation":"supply"}}]
- "ç‰¹æ–¯æ‹‰ä¸é€šç”¨æ±½è½¦ç«äº‰ç”µåŠ¨è½¦å¸‚åœº" â†’ [{{"src":"TSLA","dst":"GM","relation":"competition"}}]
- "å¾®è½¯å®Œæˆå¯¹æš´é›ªå¨±ä¹çš„æ”¶è´­" â†’ [{{"src":"MSFT","dst":"ATVI","relation":"merger"}}]
- "è‹¹æœå‘å¸ƒæ–°æ¬¾iPhone" â†’ []

ç°åœ¨è¯·åˆ†æä¸Šè¿°æ–°é—»æ ‡é¢˜ï¼š"""
            
            batch_prompts.append(prompt)
        
        # æ‰¹é‡æ¨ç†
        valid_prompts = [p for p in batch_prompts if p is not None]
        if valid_prompts:
            try:
                # device_map="auto" æ—¶ local_model.device å¯èƒ½ä¸å¯é ï¼Œä½¿ç”¨å‚æ•°è®¾å¤‡æ›´ç¨³
                device = next(local_model.parameters()).device
                
                # æ‰¹é‡ç¼–ç æ‰€æœ‰prompt
                inputs = []
                for prompt in valid_prompts:
                    messages = [{"role": "user", "content": prompt}]
                    text_input = local_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    inputs.append(text_input)
                
                # æ‰¹é‡tokenizeï¼ˆå…³é”®åŠ é€Ÿç‚¹ï¼‰
                model_inputs = local_tokenizer(
                    inputs, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=max_input_tokens
                ).to(device)
                
                # æ‰¹é‡ç”Ÿæˆ
                with torch.inference_mode():
                    generated_ids = local_model.generate(
                        **model_inputs,
                        max_new_tokens=max_new_tokens,  # å…³ç³»æŠ½å–åªéœ€è¦å¾ˆçŸ­è¾“å‡º
                        do_sample=do_sample,
                        temperature=0.0 if not do_sample else 0.1,
                        pad_token_id=getattr(local_tokenizer, "pad_token_id", None) or getattr(local_tokenizer, "eos_token_id", None),
                        eos_token_id=getattr(local_tokenizer, "eos_token_id", None),
                    )
                
                # æ‰¹é‡è§£ç 
                valid_idx = 0
                for j, prompt in enumerate(batch_prompts):
                    if prompt is None:
                        results.append([])
                    else:
                        output_ids = generated_ids[valid_idx]
                        input_len = model_inputs.input_ids[valid_idx].shape[0]
                        generated = output_ids[input_len:]
                        raw = local_tokenizer.decode(generated, skip_special_tokens=True)
                        
                        try:
                            parsed = _extract_json_from_text(raw)
                            results.append(_normalize_llm_relations(parsed))
                        except Exception:
                            results.append([])
                        
                        valid_idx += 1
                        
            except torch.cuda.OutOfMemoryError:
                # å…³é”®ï¼šä¸è¦åæ‰ OOMï¼Œè®©ä¸Šå±‚é™ä½ batch é‡è¯•
                raise
            except Exception:
                # æ‰¹å¤„ç†å¤±è´¥æ—¶ï¼Œç”¨ç©ºç»“æœå¡«å……
                for prompt in batch_prompts:
                    results.append([])
    
    return results


def extract_relations_with_llm(news_text, client=None, local_model=None, local_tokenizer=None):
    """å•æ¡æå–ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    result = extract_relations_with_llm_batch([news_text], local_model, local_tokenizer, batch_size=1)
    return result[0] if result else []


def stratified_sample_news(df_news, max_per_ticker=20, max_total=50000, random_state=42):
    """
    åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
    
    å‚æ•°:
        df_news: æ–°é—» DataFrame
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        max_total: æ€»å…±æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        random_state: éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
    
    è¿”å›:
        é‡‡æ ·åçš„ DataFrame
    """
    print(f">>> å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")
    print(f"    æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·: {max_per_ticker} æ¡")
    print(f"    æ€»é‡‡æ ·ä¸Šé™: {max_total} æ¡")
    
    # æŒ‰ Ticker åˆ†ç»„é‡‡æ ·
    sampled_dfs = []
    ticker_counts = df_news['Ticker'].value_counts()
    
    for ticker in ticker_counts.index:
        ticker_news = df_news[df_news['Ticker'] == ticker]
        n_sample = min(len(ticker_news), max_per_ticker)
        sampled = ticker_news.sample(n=n_sample, random_state=random_state)
        sampled_dfs.append(sampled)
    
    # åˆå¹¶æ‰€æœ‰é‡‡æ ·ç»“æœ
    df_sampled = pd.concat(sampled_dfs, ignore_index=True)
    
    # å¦‚æœæ€»æ•°è¶…è¿‡ä¸Šé™ï¼Œå†éšæœºé‡‡æ ·
    if len(df_sampled) > max_total:
        df_sampled = df_sampled.sample(n=max_total, random_state=random_state)
    
    # æ‰“ä¹±é¡ºåº
    df_sampled = df_sampled.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    print(f"    é‡‡æ ·åæ–°é—»æ€»æ•°: {len(df_sampled)}")
    print(f"    è¦†ç›–è‚¡ç¥¨æ•°: {df_sampled['Ticker'].nunique()}")
    
    return df_sampled


def build_dynamic_graph(use_llm=USE_LLM_DEFAULT, max_per_ticker=MAX_NEWS_PER_TICKER, max_total=MAX_TOTAL_NEWS, use_sp500=USE_SP500_ONLY):
    """
    æ„å»ºåŠ¨æ€å›¾è°±
    
    å‚æ•°:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æå–å…³ç³»ï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»
        max_total: æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
        use_sp500: æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
    """
    print("=" * 70)
    print(">>> [Step 1] è¯»å–æ¨¡å‹æ•°æ®ä¸æ–°é—»...")
    print("=" * 70)

    if not os.path.exists(INPUT_MODEL_DATA):
        print(f"[ERROR] æœªæ‰¾åˆ° {INPUT_MODEL_DATA}")
        return

    df_price = pd.read_csv(INPUT_MODEL_DATA)
    all_tickers = sorted(df_price['Ticker'].astype(str).str.upper().unique())
    print(f"    åŸå§‹æ•°æ®æ£€æµ‹åˆ° {len(all_tickers)} åªè‚¡ç¥¨ã€‚")
    
    # =============== S&P 500 è¿‡æ»¤ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰===============
    # é‡è¦ï¼šä¸ºäº†ä¸ dataset.py / train_full.py çš„ ticker2idx å¯¹é½ï¼Œå›¾çš„â€œèŠ‚ç‚¹é¡ºåºâ€å›ºå®šä¸º all_tickersï¼›
    #       S&P500 æ¨¡å¼åªå½±å“â€œå“ªäº›æ–°é—»å‚ä¸å»ºè¾¹â€ï¼Œä»¥åŠâ€œå“ªäº› ticker å…è®¸å‡ºç°åœ¨è¾¹ä¸Šâ€ã€‚
    if use_sp500:
        # å…¼å®¹å¸¸è§å†™æ³•å·®å¼‚ï¼šBRK.B vs BRK-Bï¼ˆä»¥åŠéƒ¨åˆ†æ•°æ®æºç”¨ '-' æ›¿ä»£ '.'ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä»…ç”¨äºâ€œæ˜¯å¦å±äº S&P500â€çš„åˆ¤æ–­ï¼Œä¸æ”¹å˜å›¾èŠ‚ç‚¹çš„ canonical è¡¨ç¤ºã€‚
        sp500_norm = {str(t).strip().upper().replace("-", ".") for t in SP500_TICKERS}
        # æ‰¾å‡ºæ•°æ®ä¸­å­˜åœ¨çš„ S&P 500 æˆåˆ†è‚¡
        sp500_in_data = [t for t in all_tickers if str(t).strip().upper().replace("-", ".") in sp500_norm]
        print(f"\nğŸ“Œ [S&P 500 æ¨¡å¼] åªä½¿ç”¨æ ¸å¿ƒæˆåˆ†è‚¡")
        print(f"    S&P 500 æˆåˆ†è‚¡å®šä¹‰: {len(SP500_TICKERS)} åª")
        print(f"    æ•°æ®ä¸­åŒ¹é…åˆ°: {len(sp500_in_data)} åª")
        
        if len(sp500_in_data) < 100:
            print(f"âš ï¸ è­¦å‘Šï¼šåŒ¹é…åˆ°çš„ S&P 500 æˆåˆ†è‚¡è¾ƒå°‘ ({len(sp500_in_data)} åª)")
            print("    å¯èƒ½åŸå› ï¼šæ•°æ®é›†ä¸­çš„è‚¡ç¥¨ä»£ç æ ¼å¼ä¸åŒï¼Œæˆ–æ•°æ®é›†ä¸åŒ…å«è¿™äº›è‚¡ç¥¨")
            print("    å°†ä½¿ç”¨å…¨é‡è‚¡ç¥¨...")
            active_tickers = all_tickers
        else:
            active_tickers = sp500_in_data
    else:
        active_tickers = all_tickers
        print(f"ğŸ“Œ [å…¨é‡æ¨¡å¼] ä½¿ç”¨æ‰€æœ‰ {len(active_tickers)} åªè‚¡ç¥¨")
    
    # å›¾èŠ‚ç‚¹å›ºå®šä¸º all_tickersï¼ˆç¡®ä¿ä¸è®­ç»ƒæ•°æ® ticker2idx å¯¹é½ï¼‰
    ticker2idx = {t: i for i, t in enumerate(all_tickers)}
    alias2canonical = _build_ticker_alias_map(all_tickers)
    num_nodes = len(all_tickers)
    active_set = set(active_tickers)
    if active_tickers != all_tickers:
        print(f"    å›¾èŠ‚ç‚¹æ•°ä¿æŒä¸º {num_nodes}ï¼ˆå…¨é‡ tickerï¼‰ï¼Œä½†ä»…å¯¹ {len(active_tickers)} ä¸ª ticker çš„æ–°é—»å»ºè¾¹ã€‚")
    else:
        print(f"    æœ€ç»ˆä½¿ç”¨ {num_nodes} åªè‚¡ç¥¨æ„å»ºå›¾è°±ã€‚")

    # ä¿å­˜ ticker é¡ºåºï¼Œä¾¿äºè®ºæ–‡å¤ç°ä¸æ’æŸ¥ç´¢å¼•å¯¹é½é—®é¢˜
    tickers_meta_path = OUTPUT_GRAPH.replace(".npy", "_tickers.json")
    try:
        _atomic_save_json(tickers_meta_path, {"tickers": all_tickers})
    except Exception:
        pass

    if not os.path.exists(INPUT_NEWS):
        print(f"[WARN] æœªæ‰¾åˆ°æ–°é—»æ–‡ä»¶ {INPUT_NEWS}ï¼Œä¿å­˜å•ä½é˜µã€‚")
        adj_matrix = np.eye(num_nodes, dtype=np.float32)
        _atomic_save_npy(OUTPUT_GRAPH, adj_matrix)
        return

    df_news = pd.read_csv(INPUT_NEWS, low_memory=False)
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")

    # ç»Ÿä¸€æ–°é—»é‡Œçš„ ticker æ ¼å¼ï¼Œé¿å…åˆ†å±‚é‡‡æ ·æ—¶å› å¤§å°å†™/å†™æ³•å·®å¼‚å¯¼è‡´â€œåŒä¸€åªè‚¡ç¥¨è¢«æ‹†æˆå¤šä¸ªç»„â€
    if 'Ticker' in df_news.columns:
        df_news['Ticker'] = df_news['Ticker'].astype(str).str.upper()
    
    # å¦‚æœä½¿ç”¨ S&P 500 æ¨¡å¼ï¼Œè¿‡æ»¤æ–°é—»æ•°æ®
    if use_sp500 and len(active_tickers) < len(all_tickers):
        before_filter = len(df_news)
        df_news = df_news[df_news['Ticker'].isin(active_tickers)].copy()
        print(f"    [S&P 500 è¿‡æ»¤] ä¿ç•™æ–°é—»: {before_filter} -> {len(df_news)}")

    # =========================== é˜²æ­¢"æœªæ¥ä¿¡æ¯"æ•°æ®æ³„éœ² ===========================
    try:
        if 'Date' in df_news.columns:
            df_news['Date'] = pd.to_datetime(df_news['Date'], errors='coerce')
            
            if df_news['Date'].dt.tz is not None:
                df_news['Date'] = df_news['Date'].dt.tz_localize(None)

            df_price_for_split = pd.read_csv(INPUT_MODEL_DATA, usecols=['Date'])
            df_price_for_split['Date'] = pd.to_datetime(df_price_for_split['Date'])
            unique_dates = sorted(df_price_for_split['Date'].unique())
            
            if len(unique_dates) >= 2:
                split_idx = int(len(unique_dates) * 0.8)
                split_idx = min(split_idx, len(unique_dates) - 1)
                split_date = unique_dates[split_idx]
                
                print(f"\n[é˜²æ³„éœ²] åˆ‡åˆ†æ—¥æœŸ split_date = {split_date}")
                before_news = len(df_news)
                df_news = df_news[df_news['Date'] < split_date].copy()
                print(f"[é˜²æ³„éœ²] è¿‡æ»¤åä¿ç•™æ–°é—»: {before_news} -> {len(df_news)}")
            else:
                print("[WARN] æ—¥æœŸä¸è¶³ï¼Œè·³è¿‡è¿‡æ»¤ã€‚")
    except Exception as e:
        print(f"[ERROR] æ—¶é—´è¿‡æ»¤å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…¨é‡æ–°é—»ï¼ˆå­˜åœ¨æ³„éœ²é£é™©ï¼‰ã€‚")

    # =========================== åˆ†å±‚é‡‡æ ·ï¼ˆå…³é”®ä¿®æ­£ï¼‰===========================
    df_news_sampled = stratified_sample_news(
        df_news, 
        max_per_ticker=max_per_ticker, 
        max_total=max_total
    )

    # è·å–æ–‡æœ¬åˆ—
    text_col = 'Headline' if 'Headline' in df_news_sampled.columns else 'Article_title'
    if text_col not in df_news_sampled.columns:
        cols = [c for c in df_news_sampled.columns if df_news_sampled[c].dtype == object]
        text_col = cols[0] if cols else None
    
    if text_col is None:
        print("[WARN] æ²¡æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œä¿å­˜å•ä½é˜µã€‚")
        _atomic_save_npy(OUTPUT_GRAPH, np.eye(num_nodes, dtype=np.float32))
        return

    # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µï¼ˆå•ä½é˜µ = è‡ªç¯ï¼‰
    adj_matrix = np.eye(num_nodes, dtype=np.float32)

    # =========================== åŠ è½½ LLM æ¨¡å‹ï¼ˆå¯é€‰ï¼‰===========================
    local_model = None
    local_tokenizer = None
    
    if use_llm:
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            print(f"\n[åŠ è½½ä¸­] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH} ...")
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"    è®¾å¤‡: {device}")

            local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
            # decoder-only æ¨¡å‹å¿…é¡»ä½¿ç”¨ left paddingï¼Œå¦åˆ™å¯èƒ½å½±å“ç”Ÿæˆç»“æœå¹¶äº§ç”Ÿè­¦å‘Š
            local_tokenizer.padding_side = "left"
            if getattr(local_tokenizer, "pad_token", None) is None:
                local_tokenizer.pad_token = local_tokenizer.eos_token

            dtype = torch.float16 if device == "cuda" else torch.float32
            model_kwargs = dict(
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True,
            )
            # å°è¯•å¯ç”¨æ›´å¿«çš„æ³¨æ„åŠ›å®ç°ï¼ˆè‹¥ç¯å¢ƒä¸æ”¯æŒä¼šè‡ªåŠ¨å›é€€ï¼‰
            if device == "cuda":
                model_kwargs["attn_implementation"] = "flash_attention_2"
            # å…¼å®¹ä¸åŒ transformers ç‰ˆæœ¬ï¼šä¼˜å…ˆä½¿ç”¨æ–°å‚æ•° dtype=ï¼ˆå¯æ¶ˆé™¤ torch_dtype deprecation æç¤ºï¼‰
            try:
                local_model = AutoModelForCausalLM.from_pretrained(
                    LOCAL_MODEL_PATH,
                    dtype=dtype,
                    **model_kwargs,
                )
            except Exception:
                # å›é€€ï¼šç§»é™¤ flash_attention_2 æˆ– dtype å‚æ•°å·®å¼‚
                model_kwargs.pop("attn_implementation", None)
                local_model = AutoModelForCausalLM.from_pretrained(
                    LOCAL_MODEL_PATH,
                    torch_dtype=dtype,
                    **model_kwargs,
                )

            # åŒæ­¥ pad_token_idï¼Œé¿å…generateé˜¶æ®µçš„paddingé—®é¢˜
            try:
                local_model.config.pad_token_id = local_tokenizer.pad_token_id
            except Exception:
                pass
            try:
                local_model.eval()
            except Exception:
                pass
            print("[OK] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(">>> é™çº§ä¸ºè§„åˆ™æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
            local_model = None

    # =========================== å¾ªç¯å»ºå›¾ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ï¼‰===========================
    print(f"\n>>> [Step 2] å¼€å§‹å»ºå›¾ (å…± {len(df_news_sampled)} æ¡æ–°é—»)...")
    print("=" * 70)
    
    # è¿›åº¦ä¿å­˜é…ç½®
    CHECKPOINT_INTERVAL = 10000
    checkpoint_path = OUTPUT_GRAPH.replace('.npy', '_checkpoint.npz')
    sampled_path = OUTPUT_GRAPH.replace('.npy', '_news_sampled.csv')
    BATCH_SIZE = int(os.environ.get("LLM_BATCH_SIZE", str(LLM_BATCH_SIZE_DEFAULT)))
    MAX_INPUT_TOKENS = int(os.environ.get("LLM_MAX_INPUT_TOKENS", str(LLM_MAX_INPUT_TOKENS_DEFAULT)))
    MAX_NEW_TOKENS = int(os.environ.get("LLM_MAX_NEW_TOKENS", str(LLM_MAX_NEW_TOKENS_DEFAULT)))
    DO_SAMPLE = os.environ.get("LLM_DO_SAMPLE", "1" if LLM_DO_SAMPLE_DEFAULT else "0") == "1"
    
    edge_count = 0
    matched_tickers = set()
    relation_type_counter = Counter()
    edge_counter = Counter()  # (src, dst) -> count
    failures = 0

    # å›ºåŒ–é‡‡æ ·ç»“æœï¼Œç¡®ä¿å¯å¤ç° & å¯æ–­ç‚¹ç»­è·‘
    if os.path.exists(sampled_path):
        try:
            df_news_sampled = pd.read_csv(sampled_path, low_memory=False)
            print(f"[Resume] æ£€æµ‹åˆ°å·²ä¿å­˜çš„é‡‡æ ·æ–°é—»: {sampled_path} (n={len(df_news_sampled)})")
        except Exception as e:
            print(f"[WARN] è¯»å–é‡‡æ ·æ–°é—»å¤±è´¥ï¼Œå°†é‡æ–°é‡‡æ ·: {e}")
    else:
        try:
            df_news_sampled.to_csv(sampled_path, index=False)
            print(f"[OK] å·²ä¿å­˜é‡‡æ ·æ–°é—»ï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘/å¤ç°ï¼‰: {sampled_path}")
        except Exception as e:
            print(f"[WARN] ä¿å­˜é‡‡æ ·æ–°é—»å¤±è´¥ï¼ˆä¸å½±å“è¿è¡Œï¼Œä½†æ— æ³•ç¨³å®šæ–­ç‚¹ç»­è·‘ï¼‰: {e}")

    # æ–­ç‚¹ç»­è·‘ï¼šå¦‚æœ checkpoint å­˜åœ¨ï¼ŒåŠ è½½ adj + è¿›åº¦
    start_pos = 0
    if os.path.exists(checkpoint_path):
        ck_adj, ck_meta = _load_checkpoint_npz(checkpoint_path)
        if ck_adj is not None and ck_meta:
            # ç®€å•ä¸€è‡´æ€§æ ¡éªŒï¼šèŠ‚ç‚¹æ•°å¿…é¡»ä¸€è‡´
            if isinstance(ck_adj, np.ndarray) and ck_adj.shape == adj_matrix.shape:
                adj_matrix = ck_adj.astype(np.float32, copy=False)
                start_pos = int(ck_meta.get("next_pos", 0))
                # ä¹Ÿå¯æ²¿ç”¨ä¸Šæ¬¡å·²é™è¿‡çš„ batch size
                if "batch_size" in ck_meta:
                    try:
                        BATCH_SIZE = int(ck_meta["batch_size"])
                    except Exception:
                        pass
                print(f"[Resume] ä» checkpoint æ¢å¤ï¼šnext_pos={start_pos}, batch_size={BATCH_SIZE}")
            else:
                print(f"[WARN] checkpoint å½¢çŠ¶ä¸åŒ¹é…ï¼Œå¿½ç•¥æ–­ç‚¹ç»­è·‘ï¼ˆck={getattr(ck_adj,'shape',None)} vs cur={adj_matrix.shape}ï¼‰")
    
    if local_model:
        print(f"[æ‰¹å¤„ç†æ¨¡å¼] batch={BATCH_SIZE}, max_input_tokens={MAX_INPUT_TOKENS}, max_new_tokens={MAX_NEW_TOKENS}, do_sample={DO_SAMPLE}")
        
        # æ‰¹å¤„ç†LLMæ¨ç†
        batch_news = []
        batch_tickers = []

        t0 = time.time()
        pbar = tqdm(total=len(df_news_sampled), desc="Building Graph", initial=start_pos)
        cur_pos = start_pos

        def _flush_batch():
            """æŠŠå½“å‰ batch_news/batch_tickers åšä¸€æ¬¡ LLM æ¨ç†å¹¶æ›´æ–°å›¾ã€‚"""
            nonlocal BATCH_SIZE, edge_count, failures
            nonlocal MAX_INPUT_TOKENS, MAX_NEW_TOKENS
            nonlocal batch_news, batch_tickers

            if not batch_news:
                return

            # æ‰¹é‡æ¨ç†ï¼ˆé‡åˆ°OOMè‡ªåŠ¨é™ä½batchå†é‡è¯•ï¼‰
            oom_tries = 0
            while True:
                try:
                    batch_relations = extract_relations_with_llm_batch(
                        batch_news,
                        local_model,
                        local_tokenizer,
                        batch_size=BATCH_SIZE,
                        max_input_tokens=MAX_INPUT_TOKENS,
                        max_new_tokens=MAX_NEW_TOKENS,
                        do_sample=DO_SAMPLE,
                    )
                    break
                except torch.cuda.OutOfMemoryError:
                    oom_tries += 1
                    try:
                        torch.cuda.empty_cache()
                    except Exception:
                        pass

                    # ä¼˜å…ˆé™ batchï¼Œå…¶æ¬¡é™è¾“å…¥é•¿åº¦ï¼Œå†é™è¾“å‡ºé•¿åº¦ï¼›æœ€åè·³è¿‡è¯¥ batch é¿å…æ•´ä»»åŠ¡ä¸­æ–­
                    if BATCH_SIZE > 1:
                        new_bs = max(1, BATCH_SIZE // 2)
                        if new_bs == BATCH_SIZE:
                            new_bs = max(1, BATCH_SIZE - 1)
                        BATCH_SIZE = new_bs
                        print(f"\n[OOM] æ˜¾å­˜ä¸è¶³ï¼Œé™ä½ batch_size -> {BATCH_SIZE} åé‡è¯•")
                        continue

                    if MAX_INPUT_TOKENS > 512:
                        MAX_INPUT_TOKENS = max(512, int(MAX_INPUT_TOKENS * 0.75))
                        print(f"\n[OOM] batch_size=1ä»ä¸è¶³ï¼Œé™ä½ max_input_tokens -> {MAX_INPUT_TOKENS} åé‡è¯•")
                        continue

                    if MAX_NEW_TOKENS > 48:
                        MAX_NEW_TOKENS = max(48, int(MAX_NEW_TOKENS * 0.75))
                        print(f"\n[OOM] ä»ä¸è¶³ï¼Œé™ä½ max_new_tokens -> {MAX_NEW_TOKENS} åé‡è¯•")
                        continue

                    # å®åœ¨æ— æ³•æ¢å¤ï¼šè·³è¿‡è¯¥ batchï¼ˆä¸è®©æ•´æ¬¡è¿è¡Œå¤±è´¥ï¼‰
                    failures += 1
                    print(f"\n[OOM] å¤šæ¬¡é‡è¯•ä»å¤±è´¥ï¼ˆoom_tries={oom_tries}ï¼‰ï¼Œå°†è·³è¿‡è¯¥æ‰¹æ¬¡ä»¥ç»§ç»­è¿è¡Œ")
                    batch_relations = [[] for _ in batch_news]
                    break
                except Exception:
                    failures += 1
                    batch_relations = [[] for _ in batch_news]
                    break

            # å¤„ç†ç»“æœ
            for src_t, relations in zip(batch_tickers, batch_relations):
                if relations:
                    for r in relations:
                        # å…¼å®¹ï¼šr å¯èƒ½æ˜¯ dict æˆ– list/tupleï¼ˆLLM è¾“å‡ºå¶å‘è·‘åï¼‰
                        if isinstance(r, dict):
                            src, dst = r.get("src"), r.get("dst")
                            rel = r.get("relation")
                        elif isinstance(r, (list, tuple)) and len(r) >= 2:
                            src, dst = r[0], r[1]
                            rel = r[2] if len(r) >= 3 else None
                        else:
                            continue

                        src_c = _canonicalize_ticker(src, alias2canonical, ticker2idx)
                        dst_c = _canonicalize_ticker(dst, alias2canonical, ticker2idx)
                        if not src_c or not dst_c or src_c == dst_c:
                            continue

                        # S&P500 æ¨¡å¼ï¼šé™åˆ¶è¾¹ä¸¤ç«¯éƒ½åœ¨ active é›†åˆå†…ï¼ˆæ›´ç¬¦åˆè®ºæ–‡â€œæ ¸å¿ƒæˆåˆ†è‚¡â€è®¾å®šï¼‰
                        if use_sp500 and (active_set != set(all_tickers)):
                            if src_c not in active_set or dst_c not in active_set:
                                continue

                        i, j = ticker2idx[src_c], ticker2idx[dst_c]
                        if adj_matrix[i, j] == 0:
                            edge_count += 1
                        adj_matrix[i, j] = 1.0
                        adj_matrix[j, i] = 1.0
                        matched_tickers.add(src_c)
                        matched_tickers.add(dst_c)

                        # ç»Ÿè®¡ï¼ˆç”¨äºè®ºæ–‡ä¸æ’é”™ï¼‰
                        # ç»Ÿè®¡è¾¹ï¼ˆæ— å‘å›¾ï¼‰ï¼šç”¨æ’åºåçš„ (min,max) ä½œä¸º keyï¼Œé¿å…åŒå‘é‡å¤è®¡æ•°
                        a, b = (src_c, dst_c) if src_c <= dst_c else (dst_c, src_c)
                        edge_counter[(a, b)] += 1
                        if rel:
                            relation_type_counter[str(rel).strip()] += 1

            # æ¸…ç©ºæ‰¹æ¬¡
            batch_news = []
            batch_tickers = []

        try:
            for pos in range(start_pos, len(df_news_sampled)):
                cur_pos = pos
                row = df_news_sampled.iloc[pos]
                src_ticker = str(row.get('Ticker', '')).strip().upper()
                # S&P500 æ¨¡å¼ï¼šä»…å¤„ç† active é›†åˆå†…çš„æºèŠ‚ç‚¹
                content = row.get(text_col, "")

                # æ˜¯å¦æŠŠå½“å‰è¡Œçº³å…¥ batch
                ok = True
                if not src_ticker:
                    ok = False
                elif use_sp500 and (active_set != set(all_tickers)) and (src_ticker not in active_set):
                    ok = False
                elif src_ticker not in ticker2idx:
                    ok = False
                elif not content or (isinstance(content, float) and pd.isna(content)):
                    ok = False

                if ok:
                    batch_news.append(str(content))
                    batch_tickers.append(src_ticker)

                # è¾¾åˆ°æ‰¹æ¬¡å¤§å°åˆ™ flush
                if len(batch_news) >= BATCH_SIZE:
                    _flush_batch()

                # è¿›åº¦ä¿å­˜ï¼ˆæŒ‰å¤„ç†æ¡æ•°ï¼‰
                if (pos + 1) % CHECKPOINT_INTERVAL == 0:
                    # ä¿å­˜å‰å…ˆæŠŠ batch åˆ·æ‰ï¼Œç¡®ä¿ checkpoint çš„å›¾æ˜¯â€œå¯ç”¨çš„â€
                    _flush_batch()
                    meta = {
                        "next_pos": pos + 1,
                        "batch_size": BATCH_SIZE,
                        "max_input_tokens": MAX_INPUT_TOKENS,
                        "max_new_tokens": MAX_NEW_TOKENS,
                        "do_sample": bool(DO_SAMPLE),
                        "use_sp500": bool(use_sp500),
                        "num_nodes": int(num_nodes),
                        "active_tickers": sorted(list(active_set)) if (use_sp500 and active_set != set(all_tickers)) else None,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    }
                    _atomic_save_checkpoint_npz(checkpoint_path, adj_matrix, meta)
                    speed = (pos + 1 - start_pos) / max(1e-6, (time.time() - t0))
                    print(f"\n[è¿›åº¦ä¿å­˜] å·²å¤„ç† {pos+1}/{len(df_news_sampled)} æ¡ (è¾¹æ•°: {int((adj_matrix.sum()-num_nodes)/2)}), {speed:.2f} it/s")

                # æ›´æ–°è¿›åº¦æ¡ï¼ˆæŒ‰æ‰«æè¡Œæ•°ï¼‰
                pbar.update(1)

            # å¾ªç¯ç»“æŸåï¼ŒæŠŠæ®‹ä½™ batch å…¨éƒ¨ flushï¼ˆé¿å…æœ«å°¾è‹¥å¹²è¡Œè¢«è·³è¿‡å¯¼è‡´æ¼åˆ·ï¼‰
            _flush_batch()

        except Exception as e:
            failures += 1
            # å‘ç”Ÿå¼‚å¸¸æ—¶å°½é‡ä¿å­˜ checkpointï¼Œé¿å…â€œè·‘ä¸€ç‚¹ç‚¹å°±ç™½è·‘â€
            try:
                meta = {
                    "next_pos": int(cur_pos),
                    "batch_size": BATCH_SIZE,
                    "max_input_tokens": MAX_INPUT_TOKENS,
                    "max_new_tokens": MAX_NEW_TOKENS,
                    "do_sample": bool(DO_SAMPLE),
                    "use_sp500": bool(use_sp500),
                    "num_nodes": int(num_nodes),
                    "active_tickers": sorted(list(active_set)) if (use_sp500 and active_set != set(all_tickers)) else None,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "error": repr(e),
                    "traceback": traceback.format_exc()[-8000:],  # é¿å…è¿‡é•¿
                }
                # å¼‚å¸¸æ—¶ä¹Ÿå°½é‡å…ˆ flush ä¸€ä¸‹ï¼Œè®© checkpoint çš„å›¾å°½å¯èƒ½å®Œæ•´
                try:
                    _flush_batch()
                except Exception:
                    pass
                _atomic_save_checkpoint_npz(checkpoint_path, adj_matrix, meta)
                print(f"\n[ERROR] å‘ç”Ÿå¼‚å¸¸ï¼Œå·²ä¿å­˜ checkpointï¼ˆå¯æ–­ç‚¹ç»­è·‘ï¼‰: {checkpoint_path}")
            except Exception:
                pass
            raise
        finally:
            try:
                pbar.close()
            except Exception:
                pass
    else:
        # è§„åˆ™æ¨¡å¼ï¼ˆä¸å˜ï¼‰
        start_pos = 0
        if os.path.exists(checkpoint_path):
            ck_adj, ck_meta = _load_checkpoint_npz(checkpoint_path)
            if ck_adj is not None and ck_meta and isinstance(ck_adj, np.ndarray) and ck_adj.shape == adj_matrix.shape:
                adj_matrix = ck_adj.astype(np.float32, copy=False)
                start_pos = int(ck_meta.get("next_pos", 0))
                print(f"[Resume] (è§„åˆ™æ¨¡å¼) ä» checkpoint æ¢å¤ï¼šnext_pos={start_pos}")

        for pos in tqdm(range(start_pos, len(df_news_sampled)), total=len(df_news_sampled), initial=start_pos, desc="Building Graph"):
            row = df_news_sampled.iloc[pos]
            src_ticker = str(row.get('Ticker', '')).strip().upper()
                
            content = row.get(text_col, "")

            ok = True
            if not src_ticker:
                ok = False
            elif use_sp500 and (active_set != set(all_tickers)) and (src_ticker not in active_set):
                ok = False
            elif src_ticker not in ticker2idx:
                ok = False
            elif not content or (isinstance(content, float) and pd.isna(content)):
                ok = False

            if ok:
                content = str(content)
                # è§„åˆ™åŒ¹é…
                for t in active_tickers:
                    if t != src_ticker and len(str(t)) >= 3 and str(t).upper() in content.upper():
                        if use_sp500 and (active_set != set(all_tickers)) and (t not in active_set):
                            continue
                        if t in ticker2idx:
                            i, j = ticker2idx[src_ticker], ticker2idx[t]
                            if adj_matrix[i, j] == 0:
                                edge_count += 1
                            adj_matrix[i, j] = 1.0
                            adj_matrix[j, i] = 1.0
                            matched_tickers.add(src_ticker)
                            matched_tickers.add(t)
            
            if (pos + 1) % CHECKPOINT_INTERVAL == 0:
                meta = {
                    "next_pos": pos + 1,
                    "batch_size": None,
                    "use_sp500": bool(use_sp500),
                    "num_nodes": int(num_nodes),
                    "active_tickers": sorted(list(active_set)) if (use_sp500 and active_set != set(all_tickers)) else None,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                }
                _atomic_save_checkpoint_npz(checkpoint_path, adj_matrix, meta)
                print(f"\n[è¿›åº¦ä¿å­˜] å·²å¤„ç† {pos+1}/{len(df_news_sampled)} æ¡ (è¾¹æ•°: {int((adj_matrix.sum()-num_nodes)/2)})")

    # =========================== ä¿å­˜æœ€ç»ˆç»“æœ ===========================
    print("\n>>> [Step 3] ä¿å­˜æœ€ç»ˆç»“æœ...")
    _atomic_save_npy(OUTPUT_GRAPH, adj_matrix)
    
    # åˆ é™¤checkpointæ–‡ä»¶
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
        print(f"[æ¸…ç†] å·²åˆ é™¤ä¸´æ—¶checkpointæ–‡ä»¶")
    # é‡‡æ ·æ–‡ä»¶ä¿ç•™ï¼ˆä¾¿äºå¤ç°/å®¡è®¡ï¼‰ï¼›å¦‚éœ€èŠ‚çœç©ºé—´å¯æ‰‹åŠ¨åˆ é™¤

    # ä¿å­˜å…³ç³»ç±»å‹ç»Ÿè®¡ï¼ˆLLMæ¨¡å¼ä¸‹æ›´æœ‰è®ºæ–‡ä»·å€¼ï¼›è§„åˆ™æ¨¡å¼å¯èƒ½ä¸ºç©ºï¼‰
    try:
        stats_path = OUTPUT_GRAPH.replace(".npy", "_relation_stats.json")
        _atomic_save_json(stats_path, {
            "relation_type_counts": dict(relation_type_counter),
            "top_edges": [
                {"src": k[0], "dst": k[1], "count": int(v)}
                for k, v in edge_counter.most_common(200)
            ],
        })
    except Exception:
        pass
    
    # =========================== è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ ===========================
    print("\n" + "=" * 70)
    print(">>> å›¾è°±ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)
    
    # è®¡ç®—å›¾è°±ç»Ÿè®¡
    total_edges = (adj_matrix.sum() - num_nodes) / 2  # å‡å»è‡ªç¯ï¼Œé™¤ä»¥2ï¼ˆæ— å‘å›¾ï¼‰
    density = total_edges / (num_nodes * (num_nodes - 1) / 2) if num_nodes > 1 else 0
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„åº¦
    degrees = adj_matrix.sum(axis=1) - 1  # å‡å»è‡ªç¯
    non_isolated = np.sum(degrees > 0)
    
    print(f"    èŠ‚ç‚¹æ•° (è‚¡ç¥¨æ•°): {num_nodes}")
    print(f"    è¾¹æ•° (è‚¡ç¥¨å…³ç³»): {int(total_edges)}")
    print(f"    å›¾å¯†åº¦: {density:.6f}")
    print(f"    æœ‰è¿æ¥çš„è‚¡ç¥¨æ•°: {non_isolated} / {num_nodes} ({non_isolated/num_nodes*100:.1f}%)")
    print(f"    å¹³å‡åº¦: {degrees.mean():.2f}")
    print(f"    æœ€å¤§åº¦: {int(degrees.max())}")
    print(f"    å­¤ç«‹èŠ‚ç‚¹æ•°: {num_nodes - non_isolated}")
    
    if non_isolated < num_nodes * 0.5:
        print("\nâš ï¸ è­¦å‘Šï¼šè¶…è¿‡ä¸€åŠçš„è‚¡ç¥¨æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼")
        print("   å»ºè®®ï¼š")
        print("   1. å¢åŠ  max_per_ticker å‚æ•°")
        print("   2. ä½¿ç”¨ LLM æ¨¡å¼ (use_llm=True) æå–æ›´å¤šå…³ç³»")
        print("   3. æ£€æŸ¥æ–°é—»æ•°æ®è´¨é‡")
    
    print(f"\n[OK] å·²ä¿å­˜è‡³ {OUTPUT_GRAPH}ï¼Œå½¢çŠ¶: {adj_matrix.shape}")
    print("=" * 70)
    
    return adj_matrix


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºè‚¡ç¥¨å…³ç³»å›¾è°±')
    # å…¼å®¹æ—§å‚æ•°ï¼š--use_llm / --no_llmï¼ŒåŒæ—¶æä¾›æ›´æ¸…æ™°çš„ --llm/--no-llm
    parser.add_argument('--use_llm', action='store_true', help='(å…¼å®¹) å¼ºåˆ¶å¯ç”¨LLM')
    parser.add_argument('--no_llm', action='store_true', help='(å…¼å®¹) å¼ºåˆ¶ç¦ç”¨LLMï¼ˆè§„åˆ™åŒ¹é…ï¼‰')
    parser.add_argument('--llm', action=argparse.BooleanOptionalAction, default=USE_LLM_DEFAULT,
                        help=f'æ˜¯å¦ä½¿ç”¨LLMï¼ˆé»˜è®¤: {USE_LLM_DEFAULT}ï¼‰')
    parser.add_argument('--max_per_ticker', type=int, default=MAX_NEWS_PER_TICKER, help='æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--max_total', type=int, default=MAX_TOTAL_NEWS, help='æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--all_stocks', action='store_true', help='ä½¿ç”¨å…¨é‡è‚¡ç¥¨ï¼ˆé»˜è®¤åªç”¨ S&P 500ï¼‰')
    
    args = parser.parse_args()
    
    # LLMå¼€å…³ï¼šé»˜è®¤å– --llm çš„å€¼ï¼Œä½†æ—§å‚æ•°å¯è¦†ç›–
    use_llm_mode = bool(args.llm)
    if args.no_llm:
        use_llm_mode = False
    if args.use_llm:
        use_llm_mode = True
    
    print("\n" + "=" * 70)
    print("ğŸ“Š è‚¡ç¥¨å…³ç³»å›¾è°±æ„å»ºå·¥å…·")
    print("=" * 70)
    print(f"é…ç½®:")
    print(f"  - è‚¡ç¥¨èŒƒå›´: {'å…¨é‡' if args.all_stocks else 'S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èï¼‰'}")
    print(f"  - å…³ç³»æå–: {'ğŸ§  LLMè¯­ä¹‰æå– (Qwen2.5-14B)' if use_llm_mode else 'ğŸ“‹ è§„åˆ™åŒ¹é…'}")
    print(f"  - æ¯è‚¡ç¥¨é‡‡æ ·: {args.max_per_ticker} æ¡æ–°é—»")
    print(f"  - æ€»é‡‡æ ·ä¸Šé™: {args.max_total} æ¡")
    if use_llm_mode:
        bs = int(os.environ.get("LLM_BATCH_SIZE", str(LLM_BATCH_SIZE_DEFAULT)))
        mi = int(os.environ.get("LLM_MAX_INPUT_TOKENS", str(LLM_MAX_INPUT_TOKENS_DEFAULT)))
        mn = int(os.environ.get("LLM_MAX_NEW_TOKENS", str(LLM_MAX_NEW_TOKENS_DEFAULT)))
        ds = os.environ.get("LLM_DO_SAMPLE", "1" if LLM_DO_SAMPLE_DEFAULT else "0")
        print(f"  - æ‰¹å¤„ç†å¤§å°: {bs} æ¡/æ‰¹ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡ LLM_BATCH_SIZE è°ƒæ•´ï¼‰")
        print(f"  - æ¨ç†å‚æ•°: max_input_tokens={mi}, max_new_tokens={mn}, do_sample={ds}")
    print("=" * 70 + "\n")
    
    build_dynamic_graph(
        use_llm=use_llm_mode, 
        max_per_ticker=args.max_per_ticker,
        max_total=args.max_total,
        use_sp500=not args.all_stocks  # é»˜è®¤ä½¿ç”¨ S&P 500
    )
