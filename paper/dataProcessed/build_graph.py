# -*- coding: utf-8 -*-
"""
LLM åŠ¨æ€å›¾è°±æ„å»º (ä¿®æ­£ç‰ˆ V3ï¼šæ”¯æŒ S&P 500 æ ¸å¿ƒè‚¡ç¥¨è¿‡æ»¤)
========================================================================
æ ¸å¿ƒä¿®æ­£ï¼š
1. [å…³é”®] æ”¯æŒåªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
2. [å…³é”®] ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
3. æ‰“ä¹±æ–°é—»é¡ºåºï¼Œé¿å…åªå¤„ç†æ’åºé å‰çš„è‚¡ç¥¨
4. æ·»åŠ å›¾è°±ç»Ÿè®¡ä¿¡æ¯è¾“å‡º

è®ºæ–‡å»ºè®®ï¼š
- ä½¿ç”¨ S&P 500 æˆåˆ†è‚¡æ˜¯é‡‘è/é‡åŒ–ç ”ç©¶çš„å­¦æœ¯æƒ¯ä¾‹
- å¤§å…¬å¸æ–°é—»è´¨é‡é«˜ï¼Œå…³ç³»æ›´æ˜ç¡®ï¼Œå›¾è°±æ›´æœ‰æ„ä¹‰
"""

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import torch
import warnings

# å…³é—­ä¸æœ¬é¡¹ç›®æ— å…³/ä¸ç¾è§‚çš„ç¯å¢ƒè­¦å‘Šï¼ˆä¸å½±å“LLMå»ºå›¾ç»“æœï¼‰
os.environ.setdefault("TRANSFORMERS_NO_TORCHVISION", "1")
warnings.filterwarnings(
    "ignore",
    message=r"Failed to load image Python extension:.*",
    category=UserWarning,
)
warnings.filterwarnings(
    "ignore",
    message=r".*`torch_dtype` is deprecated.*",
)

# é™ä½ transformers çš„æ—¥å¿—å™ªå£°ï¼ˆä¸å½±å“ç»“æœï¼‰
try:
    from transformers.utils import logging as _hf_logging
    _hf_logging.set_verbosity_error()
except Exception:
    pass

# ================= è·¯å¾„é…ç½® =================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DATA_PROCESSED = os.path.join(PROJECT_ROOT, 'data', 'processed')

INPUT_NEWS = os.path.join(DATA_PROCESSED, 'Stock_News.csv')
INPUT_MODEL_DATA = os.path.join(DATA_PROCESSED, 'Final_Model_Data.csv')
OUTPUT_GRAPH = os.path.join(DATA_PROCESSED, 'Graph_Adjacency.npy')

# LLM é…ç½®
USE_LOCAL_MODEL = True
LOCAL_MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"
LOCAL_MODEL_PATH = os.environ.get(
    "LOCAL_MODEL_PATH", 
    "/root/autodl-tmp/models/qwen/Qwen2.5-14B-Instruct" 
)

# ================= é‡‡æ ·é…ç½®ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆï¼‰=================
# æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
# æ‰¹å¤„ç†åé€Ÿåº¦æå‡15å€ï¼Œå¯ä»¥å¤„ç†æ›´å¤šæ–°é—»
MAX_NEWS_PER_TICKER = 200  # é€‚åº¦é‡‡æ ·ï¼Œç¡®ä¿è´¨é‡

# æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
# æ‰¹å¤„ç†æ¨¡å¼ï¼šè€—æ—¶å–å†³äº batch / æ¨ç†å‚æ•°ä¸GPUååï¼ˆé€šå¸¸ä¸ºæ•°å°æ—¶é‡çº§ï¼‰
MAX_TOTAL_NEWS = 100000  # å¹³è¡¡è´¨é‡ä¸æ—¶é—´

# æ˜¯å¦ä½¿ç”¨ LLMï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
# 48GBæ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œå¯ç”¨LLMä»¥è·å¾—æ›´å‡†ç¡®çš„å…³ç³»æå–
USE_LLM_DEFAULT = True  # âš ï¸ ç¡®ä¿å¯ç”¨LLMæ¨¡å¼

# ================= LLM æ¨ç†åŠ é€Ÿé…ç½®ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰=================
# è¯´æ˜ï¼š
# - batch è¶Šå¤§ååè¶Šé«˜ï¼Œä½†æ˜¾å­˜å ç”¨ä¹Ÿè¶Šå¤§ï¼›48GB é€šå¸¸å¯ä»¥ä» 32 å¼€å§‹å°è¯•
# - å…³ç³»æŠ½å–åªéœ€è¦å¾ˆçŸ­çš„ JSON è¾“å‡ºï¼Œä¸éœ€è¦ 256 token + é‡‡æ ·
LLM_BATCH_SIZE_DEFAULT = int(os.environ.get("LLM_BATCH_SIZE", "64"))
LLM_MAX_INPUT_TOKENS_DEFAULT = int(os.environ.get("LLM_MAX_INPUT_TOKENS", "1536"))
LLM_MAX_NEW_TOKENS_DEFAULT = int(os.environ.get("LLM_MAX_NEW_TOKENS", "96"))
LLM_DO_SAMPLE_DEFAULT = os.environ.get("LLM_DO_SAMPLE", "0") == "1"

# ================= S&P 500 æˆåˆ†è‚¡ï¼ˆ2023å¹´ç‰ˆæœ¬ï¼Œçº¦500åªï¼‰=================
# è¿™æ˜¯å­¦æœ¯ç ”ç©¶ä¸­å¸¸ç”¨çš„æ ¸å¿ƒè‚¡ç¥¨åˆ—è¡¨
# æ•°æ®æ¥æºï¼šWikipedia / Yahoo Finance
SP500_TICKERS = {
    # ä¿¡æ¯æŠ€æœ¯ (Information Technology)
    'AAPL', 'MSFT', 'NVDA', 'AVGO', 'CSCO', 'ADBE', 'CRM', 'ORCL', 'ACN', 'IBM',
    'INTC', 'AMD', 'QCOM', 'TXN', 'AMAT', 'MU', 'LRCX', 'ADI', 'KLAC', 'SNPS',
    'CDNS', 'MCHP', 'NXPI', 'MPWR', 'FTNT', 'PANW', 'NOW', 'INTU', 'ADSK', 'ANSS',
    'PYPL', 'FIS', 'FISV', 'GPN', 'ADP', 'PAYX', 'CTSH', 'IT', 'EPAM', 'AKAM',
    
    # åŒ»ç–—ä¿å¥ (Health Care)
    'UNH', 'JNJ', 'LLY', 'PFE', 'ABBV', 'MRK', 'TMO', 'ABT', 'DHR', 'BMY',
    'AMGN', 'GILD', 'VRTX', 'REGN', 'ISRG', 'MDT', 'SYK', 'BDX', 'BSX', 'EW',
    'ZBH', 'IDXX', 'DXCM', 'ALGN', 'HOLX', 'MTD', 'IQV', 'CI', 'ELV', 'HUM',
    'CVS', 'MCK', 'CAH', 'ABC', 'CNC', 'MOH', 'HCA', 'UHS', 'DVA', 'LH',
    'DGX', 'A', 'WAT', 'PKI', 'BIO', 'TECH', 'HSIC', 'COO', 'RMD', 'BAX',
    
    # é‡‘è (Financials)
    'BRK.B', 'JPM', 'V', 'MA', 'BAC', 'WFC', 'GS', 'MS', 'SCHW', 'AXP',
    'BLK', 'SPGI', 'C', 'PNC', 'USB', 'TFC', 'CME', 'ICE', 'CB', 'MMC',
    'AON', 'PGR', 'AIG', 'MET', 'PRU', 'AFL', 'ALL', 'TRV', 'CINF', 'HIG',
    'AJG', 'WTW', 'BRO', 'RE', 'L', 'GL', 'COF', 'DFS', 'SYF', 'ALLY',
    'MTB', 'FITB', 'HBAN', 'KEY', 'RF', 'CFG', 'ZION', 'NTRS', 'STT', 'BK',
    
    # é€šä¿¡æœåŠ¡ (Communication Services)
    'GOOGL', 'GOOG', 'META', 'NFLX', 'DIS', 'CMCSA', 'VZ', 'T', 'TMUS', 'CHTR',
    'ATVI', 'EA', 'TTWO', 'WBD', 'PARA', 'FOX', 'FOXA', 'NWS', 'NWSA', 'OMC',
    'IPG', 'LYV', 'MTCH', 'ZG', 'PINS',
    
    # æ¶ˆè´¹å“ (Consumer Discretionary)
    'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'LOW', 'SBUX', 'TJX', 'BKNG', 'MAR',
    'HLT', 'CMG', 'ORLY', 'AZO', 'ROST', 'DHI', 'LEN', 'PHM', 'NVR', 'GM',
    'F', 'APTV', 'BWA', 'LEA', 'RL', 'TPR', 'VFC', 'PVH', 'HAS', 'MAT',
    'DRI', 'YUM', 'WYNN', 'MGM', 'CZR', 'RCL', 'CCL', 'NCLH', 'LVS', 'EXPE',
    'ABNB', 'UBER', 'LYFT', 'DASH', 'EBAY', 'ETSY', 'W', 'BBY', 'KMX', 'AN',
    
    # å¿…éœ€æ¶ˆè´¹å“ (Consumer Staples)
    'PG', 'KO', 'PEP', 'COST', 'WMT', 'PM', 'MO', 'MDLZ', 'CL', 'EL',
    'KMB', 'GIS', 'K', 'HSY', 'HRL', 'SJM', 'MKC', 'CAG', 'CPB', 'TSN',
    'KHC', 'STZ', 'BF.B', 'TAP', 'KDP', 'MNST', 'WBA', 'SYY', 'KR', 'TGT',
    'DG', 'DLTR', 'CLX', 'CHD', 'COR',
    
    # å·¥ä¸š (Industrials)
    'UNP', 'UPS', 'HON', 'BA', 'RTX', 'CAT', 'DE', 'LMT', 'GE', 'MMM',
    'GD', 'NOC', 'LHX', 'TDG', 'ITW', 'EMR', 'ROK', 'PH', 'ETN', 'PCAR',
    'CTAS', 'FAST', 'WM', 'RSG', 'WCN', 'VRSK', 'CPRT', 'CSX', 'NSC', 'FDX',
    'EXPD', 'CHRW', 'JBHT', 'DAL', 'UAL', 'LUV', 'AAL', 'ALK', 'CARR', 'OTIS',
    'JCI', 'TT', 'IR', 'SWK', 'MAS', 'GNRC', 'PWR', 'AME', 'DOV', 'ROP',
    
    # èƒ½æº (Energy)
    'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'MPC', 'PSX', 'VLO', 'PXD', 'OXY',
    'HES', 'DVN', 'FANG', 'HAL', 'BKR', 'KMI', 'WMB', 'OKE', 'TRGP', 'APA',
    'MRO', 'CTRA',
    
    # ææ–™ (Materials)
    'LIN', 'APD', 'SHW', 'ECL', 'DD', 'DOW', 'NEM', 'FCX', 'NUE', 'VMC',
    'MLM', 'PPG', 'ALB', 'EMN', 'CE', 'CF', 'MOS', 'FMC', 'IFF', 'CTVA',
    'LYB', 'IP', 'PKG', 'SEE', 'AVY', 'BALL', 'AMCR',
    
    # æˆ¿åœ°äº§ (Real Estate)
    'PLD', 'AMT', 'CCI', 'EQIX', 'PSA', 'SPG', 'O', 'WELL', 'DLR', 'AVB',
    'EQR', 'VTR', 'ARE', 'MAA', 'UDR', 'ESS', 'HST', 'PEAK', 'KIM', 'REG',
    'FRT', 'BXP', 'VNO', 'SLG', 'CBRE', 'IRM', 'WY', 'SBAC', 'INVH', 'CPT',
    
    # å…¬ç”¨äº‹ä¸š (Utilities)
    'NEE', 'DUK', 'SO', 'D', 'AEP', 'SRE', 'EXC', 'XEL', 'ED', 'PEG',
    'WEC', 'ES', 'AWK', 'DTE', 'EIX', 'ETR', 'FE', 'PPL', 'AEE', 'CMS',
    'CNP', 'EVRG', 'ATO', 'NI', 'LNT', 'PNW', 'NRG', 'CEG',
}

# æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆå¼ºçƒˆæ¨èç”¨äºè®ºæ–‡ï¼‰
USE_SP500_ONLY = True


def _normalize_llm_relations(parsed):
    """
    å°† LLM è¿”å›çš„ JSON è§£æç»“æœè§„æ•´ä¸ºç»Ÿä¸€æ ¼å¼ï¼š
    List[{"src": str, "dst": str, "relation": Optional[str]}]
    
    å…¼å®¹å¸¸è§â€œè·‘åâ€æ ¼å¼ï¼š
    - [{"src":"AAPL","dst":"QCOM","relation":"supply"}]
    - [["AAPL","QCOM","supply"], ["TSLA","GM","competition"]]
    - {"relations": [...]} / {"data": [...]} ç­‰åŒ…è£…
    """
    if parsed is None:
        return []

    # æœ‰äº›æ¨¡å‹ä¼šå¤šåŒ…ä¸€å±‚ dict
    if isinstance(parsed, dict):
        for k in ("relations", "relation", "edges", "triples", "items", "data", "results"):
            if k in parsed:
                parsed = parsed.get(k)
                break

    if not isinstance(parsed, list):
        return []

    norm = []
    for item in parsed:
        src = dst = rel = None

        if isinstance(item, dict):
            src = item.get("src") or item.get("source") or item.get("from")
            dst = item.get("dst") or item.get("target") or item.get("to")
            rel = item.get("relation") or item.get("type")
        elif isinstance(item, (list, tuple)) and len(item) >= 2:
            src, dst = item[0], item[1]
            rel = item[2] if len(item) >= 3 else None
        else:
            continue

        if src is None or dst is None:
            continue

        src = str(src).strip().upper()
        dst = str(dst).strip().upper()
        rel = str(rel).strip() if rel is not None else None

        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not src or not dst:
            continue

        norm.append({"src": src, "dst": dst, "relation": rel})

    return norm


def extract_relations_with_llm_batch(
    news_texts,
    local_model=None,
    local_tokenizer=None,
    batch_size=8,
    max_input_tokens=LLM_MAX_INPUT_TOKENS_DEFAULT,
    max_new_tokens=LLM_MAX_NEW_TOKENS_DEFAULT,
    do_sample=LLM_DO_SAMPLE_DEFAULT,
):
    """æ‰¹å¤„ç†LLMæå–å…³ç³» - ä¿æŒé«˜è´¨é‡Promptï¼Œé€šè¿‡æ‰¹å¤„ç†æé€Ÿ"""
    if local_model is None or local_tokenizer is None:
        return [[] for _ in news_texts]
    
    results = []
    
    # æ‰¹å¤„ç†
    for i in range(0, len(news_texts), batch_size):
        batch = news_texts[i:i+batch_size]
        batch_prompts = []
        
        for text in batch:
            if not text or (isinstance(text, float) and pd.isna(text)):
                batch_prompts.append(None)
                continue
            
            text = str(text)[:500]
            
            # ä½¿ç”¨å®Œæ•´çš„é«˜è´¨é‡promptï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èå…³ç³»æŠ½å–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹è´¢ç»æ–°é—»æ ‡é¢˜ä¸­æå–å…¬å¸ä¹‹é—´çš„**æ˜¾å¼å…³ç³»**ã€‚

æ–°é—»æ ‡é¢˜ï¼š{text}

å…³ç³»ç±»å‹ï¼ˆä»…é™ä»¥ä¸‹ç±»å‹ï¼‰ï¼š
1. ä¾›åº”é“¾å…³ç³» (supply): ä¾›åº”å•†ã€é‡‡è´­ã€è®¢å•ã€åˆåŒ
2. ç«äº‰å…³ç³» (competition): ç«äº‰å¯¹æ‰‹ã€å¸‚åœºäº‰å¤ºã€ä»·æ ¼æˆ˜
3. åˆä½œå…³ç³» (cooperation): åˆä½œã€è”ç›Ÿã€åˆèµ„ã€æˆ˜ç•¥ä¼™ä¼´
4. å¹¶è´­å…³ç³» (merger): æ”¶è´­ã€å…¼å¹¶ã€é‡ç»„ã€å‡ºå”®èµ„äº§
5. è¯‰è®¼å…³ç³» (lawsuit): èµ·è¯‰ã€è¯‰è®¼ã€æ³•å¾‹çº çº·ã€ä¾µæƒ
6. æŠ•èµ„å…³ç³» (investment): æŠ•èµ„ã€å…¥è‚¡ã€æŒè‚¡ã€æˆ˜ç•¥æŠ•èµ„

è¾“å‡ºè¦æ±‚ï¼š
1. åªæå–**æ˜ç¡®æåˆ°ä¸¤å®¶å…¬å¸**ä¸”å…³ç³»æ¸…æ™°çš„å†…å®¹
2. è‚¡ç¥¨ä»£ç å¿…é¡»æ˜¯**ç¾è‚¡ä»£ç **ï¼ˆå¦‚AAPLã€TSLAã€MSFTç­‰ï¼‰
3. å¦‚æœæ–°é—»åªæåˆ°ä¸€å®¶å…¬å¸ï¼Œè¿”å› []
4. å¦‚æœå…³ç³»ä¸å±äºä»¥ä¸Š6ç±»ï¼Œè¿”å› []

ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
[{{"src": "å…¬å¸Aä»£ç ", "dst": "å…¬å¸Bä»£ç ", "relation": "å…³ç³»ç±»å‹"}}]

ç¤ºä¾‹ï¼š
- "è‹¹æœä¸é«˜é€šè¾¾æˆ5å¹´èŠ¯ç‰‡ä¾›åº”åè®®" â†’ [{{"src":"AAPL","dst":"QCOM","relation":"supply"}}]
- "ç‰¹æ–¯æ‹‰ä¸é€šç”¨æ±½è½¦ç«äº‰ç”µåŠ¨è½¦å¸‚åœº" â†’ [{{"src":"TSLA","dst":"GM","relation":"competition"}}]
- "å¾®è½¯å®Œæˆå¯¹æš´é›ªå¨±ä¹çš„æ”¶è´­" â†’ [{{"src":"MSFT","dst":"ATVI","relation":"merger"}}]
- "è‹¹æœå‘å¸ƒæ–°æ¬¾iPhone" â†’ []

ç°åœ¨è¯·åˆ†æä¸Šè¿°æ–°é—»æ ‡é¢˜ï¼š"""
            
            batch_prompts.append(prompt)
        
        # æ‰¹é‡æ¨ç†
        valid_prompts = [p for p in batch_prompts if p is not None]
        if valid_prompts:
            try:
                device = local_model.device
                
                # æ‰¹é‡ç¼–ç æ‰€æœ‰prompt
                inputs = []
                for prompt in valid_prompts:
                    messages = [{"role": "user", "content": prompt}]
                    text_input = local_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    inputs.append(text_input)
                
                # æ‰¹é‡tokenizeï¼ˆå…³é”®åŠ é€Ÿç‚¹ï¼‰
                model_inputs = local_tokenizer(
                    inputs, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=max_input_tokens
                ).to(device)
                
                # æ‰¹é‡ç”Ÿæˆ
                with torch.inference_mode():
                    generated_ids = local_model.generate(
                        **model_inputs,
                        max_new_tokens=max_new_tokens,  # å…³ç³»æŠ½å–åªéœ€è¦å¾ˆçŸ­è¾“å‡º
                        do_sample=do_sample,
                        temperature=0.0 if not do_sample else 0.1,
                        pad_token_id=getattr(local_tokenizer, "pad_token_id", None),
                        eos_token_id=getattr(local_tokenizer, "eos_token_id", None),
                    )
                
                # æ‰¹é‡è§£ç 
                valid_idx = 0
                for j, prompt in enumerate(batch_prompts):
                    if prompt is None:
                        results.append([])
                    else:
                        output_ids = generated_ids[valid_idx]
                        input_len = model_inputs.input_ids[valid_idx].shape[0]
                        generated = output_ids[input_len:]
                        raw = local_tokenizer.decode(generated, skip_special_tokens=True)
                        
                        try:
                            import json
                            if "```" in raw:
                                raw = raw.split("```")[1]
                                if raw.startswith("json"):
                                    raw = raw[4:]
                            parsed = json.loads(raw)
                            results.append(_normalize_llm_relations(parsed))
                        except:
                            results.append([])
                        
                        valid_idx += 1
                        
            except Exception as e:
                # æ‰¹å¤„ç†å¤±è´¥æ—¶ï¼Œç”¨ç©ºç»“æœå¡«å……
                for prompt in batch_prompts:
                    results.append([])
    
    return results


def extract_relations_with_llm(news_text, client=None, local_model=None, local_tokenizer=None):
    """å•æ¡æå–ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    result = extract_relations_with_llm_batch([news_text], local_model, local_tokenizer, batch_size=1)
    return result[0] if result else []


def stratified_sample_news(df_news, max_per_ticker=20, max_total=50000, random_state=42):
    """
    åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
    
    å‚æ•°:
        df_news: æ–°é—» DataFrame
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        max_total: æ€»å…±æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        random_state: éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
    
    è¿”å›:
        é‡‡æ ·åçš„ DataFrame
    """
    print(f">>> å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")
    print(f"    æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·: {max_per_ticker} æ¡")
    print(f"    æ€»é‡‡æ ·ä¸Šé™: {max_total} æ¡")
    
    # æŒ‰ Ticker åˆ†ç»„é‡‡æ ·
    sampled_dfs = []
    ticker_counts = df_news['Ticker'].value_counts()
    
    for ticker in ticker_counts.index:
        ticker_news = df_news[df_news['Ticker'] == ticker]
        n_sample = min(len(ticker_news), max_per_ticker)
        sampled = ticker_news.sample(n=n_sample, random_state=random_state)
        sampled_dfs.append(sampled)
    
    # åˆå¹¶æ‰€æœ‰é‡‡æ ·ç»“æœ
    df_sampled = pd.concat(sampled_dfs, ignore_index=True)
    
    # å¦‚æœæ€»æ•°è¶…è¿‡ä¸Šé™ï¼Œå†éšæœºé‡‡æ ·
    if len(df_sampled) > max_total:
        df_sampled = df_sampled.sample(n=max_total, random_state=random_state)
    
    # æ‰“ä¹±é¡ºåº
    df_sampled = df_sampled.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    print(f"    é‡‡æ ·åæ–°é—»æ€»æ•°: {len(df_sampled)}")
    print(f"    è¦†ç›–è‚¡ç¥¨æ•°: {df_sampled['Ticker'].nunique()}")
    
    return df_sampled


def build_dynamic_graph(use_llm=USE_LLM_DEFAULT, max_per_ticker=MAX_NEWS_PER_TICKER, max_total=MAX_TOTAL_NEWS, use_sp500=USE_SP500_ONLY):
    """
    æ„å»ºåŠ¨æ€å›¾è°±
    
    å‚æ•°:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æå–å…³ç³»ï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»
        max_total: æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
        use_sp500: æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
    """
    print("=" * 70)
    print(">>> [Step 1] è¯»å–æ¨¡å‹æ•°æ®ä¸æ–°é—»...")
    print("=" * 70)

    if not os.path.exists(INPUT_MODEL_DATA):
        print(f"[ERROR] æœªæ‰¾åˆ° {INPUT_MODEL_DATA}")
        return

    df_price = pd.read_csv(INPUT_MODEL_DATA)
    all_tickers = sorted(df_price['Ticker'].unique())
    print(f"    åŸå§‹æ•°æ®æ£€æµ‹åˆ° {len(all_tickers)} åªè‚¡ç¥¨ã€‚")
    
    # =============== S&P 500 è¿‡æ»¤ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰===============
    if use_sp500:
        # æ‰¾å‡ºæ•°æ®ä¸­å­˜åœ¨çš„ S&P 500 æˆåˆ†è‚¡
        sp500_in_data = [t for t in all_tickers if t in SP500_TICKERS]
        print(f"\nğŸ“Œ [S&P 500 æ¨¡å¼] åªä½¿ç”¨æ ¸å¿ƒæˆåˆ†è‚¡")
        print(f"    S&P 500 æˆåˆ†è‚¡å®šä¹‰: {len(SP500_TICKERS)} åª")
        print(f"    æ•°æ®ä¸­åŒ¹é…åˆ°: {len(sp500_in_data)} åª")
        
        if len(sp500_in_data) < 100:
            print(f"âš ï¸ è­¦å‘Šï¼šåŒ¹é…åˆ°çš„ S&P 500 æˆåˆ†è‚¡è¾ƒå°‘ ({len(sp500_in_data)} åª)")
            print("    å¯èƒ½åŸå› ï¼šæ•°æ®é›†ä¸­çš„è‚¡ç¥¨ä»£ç æ ¼å¼ä¸åŒï¼Œæˆ–æ•°æ®é›†ä¸åŒ…å«è¿™äº›è‚¡ç¥¨")
            print("    å°†ä½¿ç”¨å…¨é‡è‚¡ç¥¨...")
            tickers = all_tickers
        else:
            tickers = sp500_in_data
            # è¿‡æ»¤ä»·æ ¼æ•°æ®ï¼Œåªä¿ç•™ S&P 500 æˆåˆ†è‚¡
            df_price = df_price[df_price['Ticker'].isin(tickers)]
    else:
        tickers = all_tickers
        print(f"ğŸ“Œ [å…¨é‡æ¨¡å¼] ä½¿ç”¨æ‰€æœ‰ {len(tickers)} åªè‚¡ç¥¨")
    
    ticker2idx = {t: i for i, t in enumerate(tickers)}
    num_nodes = len(tickers)
    print(f"    æœ€ç»ˆä½¿ç”¨ {num_nodes} åªè‚¡ç¥¨æ„å»ºå›¾è°±ã€‚")

    if not os.path.exists(INPUT_NEWS):
        print(f"[WARN] æœªæ‰¾åˆ°æ–°é—»æ–‡ä»¶ {INPUT_NEWS}ï¼Œä¿å­˜å•ä½é˜µã€‚")
        adj_matrix = np.eye(num_nodes, dtype=np.float32)
        np.save(OUTPUT_GRAPH, adj_matrix)
        return

    df_news = pd.read_csv(INPUT_NEWS, low_memory=False)
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")
    
    # å¦‚æœä½¿ç”¨ S&P 500 æ¨¡å¼ï¼Œè¿‡æ»¤æ–°é—»æ•°æ®
    if use_sp500 and len(tickers) < len(all_tickers):
        before_filter = len(df_news)
        df_news = df_news[df_news['Ticker'].isin(tickers)].copy()
        print(f"    [S&P 500 è¿‡æ»¤] ä¿ç•™æ–°é—»: {before_filter} -> {len(df_news)}")

    # =========================== é˜²æ­¢"æœªæ¥ä¿¡æ¯"æ•°æ®æ³„éœ² ===========================
    try:
        if 'Date' in df_news.columns:
            df_news['Date'] = pd.to_datetime(df_news['Date'], errors='coerce')
            
            if df_news['Date'].dt.tz is not None:
                df_news['Date'] = df_news['Date'].dt.tz_localize(None)

            df_price_for_split = pd.read_csv(INPUT_MODEL_DATA, usecols=['Date'])
            df_price_for_split['Date'] = pd.to_datetime(df_price_for_split['Date'])
            unique_dates = sorted(df_price_for_split['Date'].unique())
            
            if len(unique_dates) >= 2:
                split_idx = int(len(unique_dates) * 0.8)
                split_idx = min(split_idx, len(unique_dates) - 1)
                split_date = unique_dates[split_idx]
                
                print(f"\n[é˜²æ³„éœ²] åˆ‡åˆ†æ—¥æœŸ split_date = {split_date}")
                before_news = len(df_news)
                df_news = df_news[df_news['Date'] < split_date].copy()
                print(f"[é˜²æ³„éœ²] è¿‡æ»¤åä¿ç•™æ–°é—»: {before_news} -> {len(df_news)}")
            else:
                print("[WARN] æ—¥æœŸä¸è¶³ï¼Œè·³è¿‡è¿‡æ»¤ã€‚")
    except Exception as e:
        print(f"[ERROR] æ—¶é—´è¿‡æ»¤å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…¨é‡æ–°é—»ï¼ˆå­˜åœ¨æ³„éœ²é£é™©ï¼‰ã€‚")

    # =========================== åˆ†å±‚é‡‡æ ·ï¼ˆå…³é”®ä¿®æ­£ï¼‰===========================
    df_news_sampled = stratified_sample_news(
        df_news, 
        max_per_ticker=max_per_ticker, 
        max_total=max_total
    )

    # è·å–æ–‡æœ¬åˆ—
    text_col = 'Headline' if 'Headline' in df_news_sampled.columns else 'Article_title'
    if text_col not in df_news_sampled.columns:
        cols = [c for c in df_news_sampled.columns if df_news_sampled[c].dtype == object]
        text_col = cols[0] if cols else None
    
    if text_col is None:
        print("[WARN] æ²¡æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œä¿å­˜å•ä½é˜µã€‚")
        np.save(OUTPUT_GRAPH, np.eye(num_nodes, dtype=np.float32))
        return

    # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µï¼ˆå•ä½é˜µ = è‡ªç¯ï¼‰
    adj_matrix = np.eye(num_nodes, dtype=np.float32)

    # =========================== åŠ è½½ LLM æ¨¡å‹ï¼ˆå¯é€‰ï¼‰===========================
    local_model = None
    local_tokenizer = None
    
    if use_llm:
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            print(f"\n[åŠ è½½ä¸­] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH} ...")
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"    è®¾å¤‡: {device}")

            local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
            # decoder-only æ¨¡å‹å¿…é¡»ä½¿ç”¨ left paddingï¼Œå¦åˆ™å¯èƒ½å½±å“ç”Ÿæˆç»“æœå¹¶äº§ç”Ÿè­¦å‘Š
            local_tokenizer.padding_side = "left"
            if getattr(local_tokenizer, "pad_token", None) is None:
                local_tokenizer.pad_token = local_tokenizer.eos_token

            dtype = torch.float16 if device == "cuda" else torch.float32
            model_kwargs = dict(
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True,
            )
            # å°è¯•å¯ç”¨æ›´å¿«çš„æ³¨æ„åŠ›å®ç°ï¼ˆè‹¥ç¯å¢ƒä¸æ”¯æŒä¼šè‡ªåŠ¨å›é€€ï¼‰
            if device == "cuda":
                model_kwargs["attn_implementation"] = "flash_attention_2"
            # å…¼å®¹ä¸åŒ transformers ç‰ˆæœ¬ï¼šä¼˜å…ˆä½¿ç”¨æ–°å‚æ•° dtype=ï¼ˆå¯æ¶ˆé™¤ torch_dtype deprecation æç¤ºï¼‰
            try:
                local_model = AutoModelForCausalLM.from_pretrained(
                    LOCAL_MODEL_PATH,
                    dtype=dtype,
                    **model_kwargs,
                )
            except Exception:
                # å›é€€ï¼šç§»é™¤ flash_attention_2 æˆ– dtype å‚æ•°å·®å¼‚
                model_kwargs.pop("attn_implementation", None)
                local_model = AutoModelForCausalLM.from_pretrained(
                    LOCAL_MODEL_PATH,
                    torch_dtype=dtype,
                    **model_kwargs,
                )

            # åŒæ­¥ pad_token_idï¼Œé¿å…generateé˜¶æ®µçš„paddingé—®é¢˜
            try:
                local_model.config.pad_token_id = local_tokenizer.pad_token_id
            except Exception:
                pass
            try:
                local_model.eval()
            except Exception:
                pass
            print("[OK] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(">>> é™çº§ä¸ºè§„åˆ™æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
            local_model = None

    # =========================== å¾ªç¯å»ºå›¾ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ï¼‰===========================
    print(f"\n>>> [Step 2] å¼€å§‹å»ºå›¾ (å…± {len(df_news_sampled)} æ¡æ–°é—»)...")
    print("=" * 70)
    
    # è¿›åº¦ä¿å­˜é…ç½®
    CHECKPOINT_INTERVAL = 10000
    checkpoint_path = OUTPUT_GRAPH.replace('.npy', '_checkpoint.npy')
    BATCH_SIZE = int(os.environ.get("LLM_BATCH_SIZE", str(LLM_BATCH_SIZE_DEFAULT)))
    MAX_INPUT_TOKENS = int(os.environ.get("LLM_MAX_INPUT_TOKENS", str(LLM_MAX_INPUT_TOKENS_DEFAULT)))
    MAX_NEW_TOKENS = int(os.environ.get("LLM_MAX_NEW_TOKENS", str(LLM_MAX_NEW_TOKENS_DEFAULT)))
    DO_SAMPLE = os.environ.get("LLM_DO_SAMPLE", "1" if LLM_DO_SAMPLE_DEFAULT else "0") == "1"
    
    edge_count = 0
    matched_tickers = set()
    
    if local_model:
        print(f"[æ‰¹å¤„ç†æ¨¡å¼] batch={BATCH_SIZE}, max_input_tokens={MAX_INPUT_TOKENS}, max_new_tokens={MAX_NEW_TOKENS}, do_sample={DO_SAMPLE}")
        
        # æ‰¹å¤„ç†LLMæ¨ç†
        batch_news = []
        batch_tickers = []
        
        for idx, row in tqdm(df_news_sampled.iterrows(), total=len(df_news_sampled), desc="Building Graph"):
            src_ticker = row.get('Ticker')
            if src_ticker not in ticker2idx:
                continue
                
            content = row.get(text_col, "")
            if not content or pd.isna(content):
                continue
            
            batch_news.append(str(content))
            batch_tickers.append(src_ticker)
            
            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–æœ€åä¸€æ‰¹
            if len(batch_news) >= BATCH_SIZE or idx == df_news_sampled.index[-1]:
                # æ‰¹é‡æ¨ç†ï¼ˆé‡åˆ°OOMè‡ªåŠ¨é™ä½batchå†é‡è¯•ï¼‰
                while True:
                    try:
                        batch_relations = extract_relations_with_llm_batch(
                            batch_news,
                            local_model,
                            local_tokenizer,
                            batch_size=BATCH_SIZE,
                            max_input_tokens=MAX_INPUT_TOKENS,
                            max_new_tokens=MAX_NEW_TOKENS,
                            do_sample=DO_SAMPLE,
                        )
                        break
                    except torch.cuda.OutOfMemoryError:
                        if BATCH_SIZE <= 4:
                            raise
                        torch.cuda.empty_cache()
                        BATCH_SIZE = max(4, BATCH_SIZE // 2)
                        print(f"\n[OOM] æ˜¾å­˜ä¸è¶³ï¼Œè‡ªåŠ¨é™ä½ batch_size -> {BATCH_SIZE} åç»§ç»­")
                
                # å¤„ç†ç»“æœ
                for src_ticker, relations in zip(batch_tickers, batch_relations):
                    if relations:
                        for r in relations:
                            # å…¼å®¹ï¼šr å¯èƒ½æ˜¯ dict æˆ– list/tupleï¼ˆLLM è¾“å‡ºå¶å‘è·‘åï¼‰
                            if isinstance(r, dict):
                                src, dst = r.get("src"), r.get("dst")
                            elif isinstance(r, (list, tuple)) and len(r) >= 2:
                                src, dst = r[0], r[1]
                            else:
                                continue
                            if src and dst and src in ticker2idx and dst in ticker2idx and src != dst:
                                i, j = ticker2idx[src], ticker2idx[dst]
                                if adj_matrix[i, j] == 0:
                                    edge_count += 1
                                adj_matrix[i, j] = 1.0
                                adj_matrix[j, i] = 1.0
                                matched_tickers.add(src)
                                matched_tickers.add(dst)
                
                # æ¸…ç©ºæ‰¹æ¬¡
                batch_news = []
                batch_tickers = []
                
                # è¿›åº¦ä¿å­˜
                if (idx + 1) % CHECKPOINT_INTERVAL == 0:
                    np.save(checkpoint_path, adj_matrix)
                    print(f"\n[è¿›åº¦ä¿å­˜] å·²å¤„ç† {idx+1}/{len(df_news_sampled)} æ¡ (è¾¹æ•°: {int((adj_matrix.sum()-num_nodes)/2)})")
    else:
        # è§„åˆ™æ¨¡å¼ï¼ˆä¸å˜ï¼‰
        for idx, row in tqdm(df_news_sampled.iterrows(), total=len(df_news_sampled), desc="Building Graph"):
            src_ticker = row.get('Ticker')
            if src_ticker not in ticker2idx:
                continue
                
            content = row.get(text_col, "")
            if not content or pd.isna(content):
                continue
            
            content = str(content)
            
            # è§„åˆ™åŒ¹é…
            for t in tickers:
                if t != src_ticker and len(str(t)) >= 3 and t.upper() in content.upper():
                    if t in ticker2idx:
                        i, j = ticker2idx[src_ticker], ticker2idx[t]
                        if adj_matrix[i, j] == 0:
                            edge_count += 1
                        adj_matrix[i, j] = 1.0
                        adj_matrix[j, i] = 1.0
                        matched_tickers.add(src_ticker)
                        matched_tickers.add(t)
            
            if (idx + 1) % CHECKPOINT_INTERVAL == 0:
                np.save(checkpoint_path, adj_matrix)
                print(f"\n[è¿›åº¦ä¿å­˜] å·²å¤„ç† {idx+1}/{len(df_news_sampled)} æ¡ (è¾¹æ•°: {int((adj_matrix.sum()-num_nodes)/2)})")

    # =========================== ä¿å­˜æœ€ç»ˆç»“æœ ===========================
    print("\n>>> [Step 3] ä¿å­˜æœ€ç»ˆç»“æœ...")
    np.save(OUTPUT_GRAPH, adj_matrix)
    
    # åˆ é™¤checkpointæ–‡ä»¶
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
        print(f"[æ¸…ç†] å·²åˆ é™¤ä¸´æ—¶checkpointæ–‡ä»¶")
    
    # =========================== è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ ===========================
    print("\n" + "=" * 70)
    print(">>> å›¾è°±ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)
    
    # è®¡ç®—å›¾è°±ç»Ÿè®¡
    total_edges = (adj_matrix.sum() - num_nodes) / 2  # å‡å»è‡ªç¯ï¼Œé™¤ä»¥2ï¼ˆæ— å‘å›¾ï¼‰
    density = total_edges / (num_nodes * (num_nodes - 1) / 2) if num_nodes > 1 else 0
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„åº¦
    degrees = adj_matrix.sum(axis=1) - 1  # å‡å»è‡ªç¯
    non_isolated = np.sum(degrees > 0)
    
    print(f"    èŠ‚ç‚¹æ•° (è‚¡ç¥¨æ•°): {num_nodes}")
    print(f"    è¾¹æ•° (è‚¡ç¥¨å…³ç³»): {int(total_edges)}")
    print(f"    å›¾å¯†åº¦: {density:.6f}")
    print(f"    æœ‰è¿æ¥çš„è‚¡ç¥¨æ•°: {non_isolated} / {num_nodes} ({non_isolated/num_nodes*100:.1f}%)")
    print(f"    å¹³å‡åº¦: {degrees.mean():.2f}")
    print(f"    æœ€å¤§åº¦: {int(degrees.max())}")
    print(f"    å­¤ç«‹èŠ‚ç‚¹æ•°: {num_nodes - non_isolated}")
    
    if non_isolated < num_nodes * 0.5:
        print("\nâš ï¸ è­¦å‘Šï¼šè¶…è¿‡ä¸€åŠçš„è‚¡ç¥¨æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼")
        print("   å»ºè®®ï¼š")
        print("   1. å¢åŠ  max_per_ticker å‚æ•°")
        print("   2. ä½¿ç”¨ LLM æ¨¡å¼ (use_llm=True) æå–æ›´å¤šå…³ç³»")
        print("   3. æ£€æŸ¥æ–°é—»æ•°æ®è´¨é‡")
    
    print(f"\n[OK] å·²ä¿å­˜è‡³ {OUTPUT_GRAPH}ï¼Œå½¢çŠ¶: {adj_matrix.shape}")
    print("=" * 70)
    
    return adj_matrix


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºè‚¡ç¥¨å…³ç³»å›¾è°±')
    # å…¼å®¹æ—§å‚æ•°ï¼š--use_llm / --no_llmï¼ŒåŒæ—¶æä¾›æ›´æ¸…æ™°çš„ --llm/--no-llm
    parser.add_argument('--use_llm', action='store_true', help='(å…¼å®¹) å¼ºåˆ¶å¯ç”¨LLM')
    parser.add_argument('--no_llm', action='store_true', help='(å…¼å®¹) å¼ºåˆ¶ç¦ç”¨LLMï¼ˆè§„åˆ™åŒ¹é…ï¼‰')
    parser.add_argument('--llm', action=argparse.BooleanOptionalAction, default=USE_LLM_DEFAULT,
                        help=f'æ˜¯å¦ä½¿ç”¨LLMï¼ˆé»˜è®¤: {USE_LLM_DEFAULT}ï¼‰')
    parser.add_argument('--max_per_ticker', type=int, default=MAX_NEWS_PER_TICKER, help='æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--max_total', type=int, default=MAX_TOTAL_NEWS, help='æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--all_stocks', action='store_true', help='ä½¿ç”¨å…¨é‡è‚¡ç¥¨ï¼ˆé»˜è®¤åªç”¨ S&P 500ï¼‰')
    
    args = parser.parse_args()
    
    # LLMå¼€å…³ï¼šé»˜è®¤å– --llm çš„å€¼ï¼Œä½†æ—§å‚æ•°å¯è¦†ç›–
    use_llm_mode = bool(args.llm)
    if args.no_llm:
        use_llm_mode = False
    if args.use_llm:
        use_llm_mode = True
    
    print("\n" + "=" * 70)
    print("ğŸ“Š è‚¡ç¥¨å…³ç³»å›¾è°±æ„å»ºå·¥å…·")
    print("=" * 70)
    print(f"é…ç½®:")
    print(f"  - è‚¡ç¥¨èŒƒå›´: {'å…¨é‡' if args.all_stocks else 'S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èï¼‰'}")
    print(f"  - å…³ç³»æå–: {'ğŸ§  LLMè¯­ä¹‰æå– (Qwen2.5-14B)' if use_llm_mode else 'ğŸ“‹ è§„åˆ™åŒ¹é…'}")
    print(f"  - æ¯è‚¡ç¥¨é‡‡æ ·: {args.max_per_ticker} æ¡æ–°é—»")
    print(f"  - æ€»é‡‡æ ·ä¸Šé™: {args.max_total} æ¡")
    if use_llm_mode:
        bs = int(os.environ.get("LLM_BATCH_SIZE", str(LLM_BATCH_SIZE_DEFAULT)))
        mi = int(os.environ.get("LLM_MAX_INPUT_TOKENS", str(LLM_MAX_INPUT_TOKENS_DEFAULT)))
        mn = int(os.environ.get("LLM_MAX_NEW_TOKENS", str(LLM_MAX_NEW_TOKENS_DEFAULT)))
        ds = os.environ.get("LLM_DO_SAMPLE", "1" if LLM_DO_SAMPLE_DEFAULT else "0")
        print(f"  - æ‰¹å¤„ç†å¤§å°: {bs} æ¡/æ‰¹ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡ LLM_BATCH_SIZE è°ƒæ•´ï¼‰")
        print(f"  - æ¨ç†å‚æ•°: max_input_tokens={mi}, max_new_tokens={mn}, do_sample={ds}")
    print("=" * 70 + "\n")
    
    build_dynamic_graph(
        use_llm=use_llm_mode, 
        max_per_ticker=args.max_per_ticker,
        max_total=args.max_total,
        use_sp500=not args.all_stocks  # é»˜è®¤ä½¿ç”¨ S&P 500
    )
