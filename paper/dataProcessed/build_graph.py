# -*- coding: utf-8 -*-
"""
LLM åŠ¨æ€å›¾è°±æ„å»º (V4 é¡¶ä¼šç‰¹ä¾›ç‰ˆï¼šS&P 500 æ ¸å¿ƒèŠ‚ç‚¹é”å®š)
========================================================================
æ ¸å¿ƒå˜æ›´ï¼š
1. [å¼ºåˆ¶] å›¾èŠ‚ç‚¹ä»…åŒ…å« S&P 500 æˆåˆ†è‚¡ (N â‰ˆ 500)
2. [è¾“å‡º] é‚»æ¥çŸ©é˜µå½¢çŠ¶å˜ä¸º (N, N)ï¼Œè§£å†³ç¨€ç–ä¸å­¤ç«‹èŠ‚ç‚¹é—®é¢˜
3. [åŒæ­¥] è¾“å‡º Graph_Tickers.json ä¾›è®­ç»ƒè„šæœ¬å¯¹é½æ•°æ®

è®ºæ–‡æ”¯æ’‘ï¼š
- "We strictly limit the graph nodes to the S&P 500 constituents to ensure high liquidity and data quality."
- ç¬¦åˆ AAAI/KDD ç­‰é¡¶ä¼šå¯¹æ•°æ®é›†è´¨é‡çš„è¦æ±‚
"""

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import torch
import warnings
import json
import time
import traceback
from collections import Counter, defaultdict

# å…³é—­ä¸æœ¬é¡¹ç›®æ— å…³/ä¸ç¾è§‚çš„ç¯å¢ƒè­¦å‘Šï¼ˆä¸å½±å“LLMå»ºå›¾ç»“æœï¼‰
os.environ.setdefault("TRANSFORMERS_NO_TORCHVISION", "1")
warnings.filterwarnings(
    "ignore",
    message=r"Failed to load image Python extension:.*",
    category=UserWarning,
)
warnings.filterwarnings(
    "ignore",
    message=r".*`torch_dtype` is deprecated.*",
)

# é™ä½ transformers çš„æ—¥å¿—å™ªå£°ï¼ˆä¸å½±å“ç»“æœï¼‰
try:
    from transformers.utils import logging as _hf_logging
    _hf_logging.set_verbosity_error()
except Exception:
    pass

# ================= è·¯å¾„é…ç½® =================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DATA_PROCESSED = os.path.join(PROJECT_ROOT, 'data', 'processed')

INPUT_NEWS = os.path.join(DATA_PROCESSED, 'Stock_News.csv')
INPUT_MODEL_DATA = os.path.join(DATA_PROCESSED, 'Final_Model_Data.csv')
OUTPUT_GRAPH = os.path.join(DATA_PROCESSED, 'Graph_Adjacency.npy')
OUTPUT_TICKERS = os.path.join(DATA_PROCESSED, 'Graph_Tickers.json')  # æ–°å¢ï¼šèŠ‚ç‚¹åˆ—è¡¨æ–‡ä»¶
RELATIONS_PARQUET_PATH = os.path.join(DATA_PROCESSED, "llm_relations.parquet")
SAVE_RELATIONS_PARQUET = True
USE_CACHED_RELATIONS = False
RELATIONS_PARTITION_COLS = None

# LLM é…ç½®
USE_LOCAL_MODEL = True
LOCAL_MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"
LOCAL_MODEL_PATH = os.environ.get(
    "LOCAL_MODEL_PATH", 
    "/root/autodl-tmp/models/qwen/Qwen2.5-14B-Instruct" 
)

# ================= é‡‡æ ·é…ç½®ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆï¼‰=================
# æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
# æ‰¹å¤„ç†åé€Ÿåº¦æå‡15å€ï¼Œå¯ä»¥å¤„ç†æ›´å¤šæ–°é—»
MAX_NEWS_PER_TICKER = 200  # é€‚åº¦é‡‡æ ·ï¼Œç¡®ä¿è´¨é‡

# æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
# æ‰¹å¤„ç†æ¨¡å¼ï¼šè€—æ—¶å–å†³äº batch / æ¨ç†å‚æ•°ä¸GPUååï¼ˆé€šå¸¸ä¸ºæ•°å°æ—¶é‡çº§ï¼‰
MAX_TOTAL_NEWS = 100000  # å¹³è¡¡è´¨é‡ä¸æ—¶é—´

# æ˜¯å¦ä½¿ç”¨ LLMï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
# 48GBæ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œå¯ç”¨LLMä»¥è·å¾—æ›´å‡†ç¡®çš„å…³ç³»æå–
USE_LLM_DEFAULT = True  # âš ï¸ ç¡®ä¿å¯ç”¨LLMæ¨¡å¼

# ================= LLM æ¨ç†åŠ é€Ÿé…ç½®ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰=================
# è¯´æ˜ï¼š
# - batch è¶Šå¤§ååè¶Šé«˜ï¼Œä½†æ˜¾å­˜å ç”¨ä¹Ÿè¶Šå¤§ï¼›48GB é€šå¸¸å¯ä»¥ä» 32 å¼€å§‹å°è¯•
# - å…³ç³»æŠ½å–åªéœ€è¦å¾ˆçŸ­çš„ JSON è¾“å‡ºï¼Œä¸éœ€è¦ 256 token + é‡‡æ ·
LLM_BATCH_SIZE_DEFAULT = int(os.environ.get("LLM_BATCH_SIZE", "64"))
LLM_MAX_INPUT_TOKENS_DEFAULT = int(os.environ.get("LLM_MAX_INPUT_TOKENS", "1536"))
LLM_MAX_NEW_TOKENS_DEFAULT = int(os.environ.get("LLM_MAX_NEW_TOKENS", "96"))
LLM_DO_SAMPLE_DEFAULT = os.environ.get("LLM_DO_SAMPLE", "0") == "1"

# ================= S&P 500 æˆåˆ†è‚¡ï¼ˆ2023å¹´ç‰ˆæœ¬ï¼Œçº¦500åªï¼‰=================
# è¿™æ˜¯å­¦æœ¯ç ”ç©¶ä¸­å¸¸ç”¨çš„æ ¸å¿ƒè‚¡ç¥¨åˆ—è¡¨
# æ•°æ®æ¥æºï¼šWikipedia / Yahoo Finance
# æ³¨æ„ï¼šä¼˜å…ˆå°è¯•ä»æœ¬åœ° sp500_list.txt åŠ è½½ï¼ˆä¾¿äºå®¡è®¡ä¸å¤ç°ï¼‰ï¼›æ‰¾ä¸åˆ°åˆ™å›é€€åˆ°å†…ç½®åˆ—è¡¨ã€‚

# å†…ç½®å›é€€åˆ—è¡¨ï¼ˆä¿ç•™ä»¥é˜²æ²¡æœ‰æœ¬åœ°æ–‡ä»¶æˆ–æ— æ³•è”ç½‘ï¼‰
_HARDCODED_SP500_TICKERS = {
    # ä¿¡æ¯æŠ€æœ¯ (Information Technology)
    'AAPL', 'MSFT', 'NVDA', 'AVGO', 'CSCO', 'ADBE', 'CRM', 'ORCL', 'ACN', 'IBM',
    'INTC', 'AMD', 'QCOM', 'TXN', 'AMAT', 'MU', 'LRCX', 'ADI', 'KLAC', 'SNPS',
    'CDNS', 'MCHP', 'NXPI', 'MPWR', 'FTNT', 'PANW', 'NOW', 'INTU', 'ADSK', 'ANSS',
    'PYPL', 'FIS', 'FISV', 'GPN', 'ADP', 'PAYX', 'CTSH', 'IT', 'EPAM', 'AKAM',
    
    # åŒ»ç–—ä¿å¥ (Health Care)
    'UNH', 'JNJ', 'LLY', 'PFE', 'ABBV', 'MRK', 'TMO', 'ABT', 'DHR', 'BMY',
    'AMGN', 'GILD', 'VRTX', 'REGN', 'ISRG', 'MDT', 'SYK', 'BDX', 'BSX', 'EW',
    'ZBH', 'IDXX', 'DXCM', 'ALGN', 'HOLX', 'MTD', 'IQV', 'CI', 'ELV', 'HUM',
    'CVS', 'MCK', 'CAH', 'ABC', 'CNC', 'MOH', 'HCA', 'UHS', 'DVA', 'LH',
    'DGX', 'A', 'WAT', 'PKI', 'BIO', 'TECH', 'HSIC', 'COO', 'RMD', 'BAX',
    
    # é‡‘è (Financials)
    'BRK.B', 'JPM', 'V', 'MA', 'BAC', 'WFC', 'GS', 'MS', 'SCHW', 'AXP',
    'BLK', 'SPGI', 'C', 'PNC', 'USB', 'TFC', 'CME', 'ICE', 'CB', 'MMC',
    'AON', 'PGR', 'AIG', 'MET', 'PRU', 'AFL', 'ALL', 'TRV', 'CINF', 'HIG',
    'AJG', 'WTW', 'BRO', 'RE', 'L', 'GL', 'COF', 'DFS', 'SYF', 'ALLY',
    'MTB', 'FITB', 'HBAN', 'KEY', 'RF', 'CFG', 'ZION', 'NTRS', 'STT', 'BK',
    
    # é€šä¿¡æœåŠ¡ (Communication Services)
    'GOOGL', 'GOOG', 'META', 'NFLX', 'DIS', 'CMCSA', 'VZ', 'T', 'TMUS', 'CHTR',
    'ATVI', 'EA', 'TTWO', 'WBD', 'PARA', 'FOX', 'FOXA', 'NWS', 'NWSA', 'OMC',
    'IPG', 'LYV', 'MTCH', 'ZG', 'PINS',
    
    # æ¶ˆè´¹å“ (Consumer Discretionary)
    'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'LOW', 'SBUX', 'TJX', 'BKNG', 'MAR',
    'HLT', 'CMG', 'ORLY', 'AZO', 'ROST', 'DHI', 'LEN', 'PHM', 'NVR', 'GM',
    'F', 'APTV', 'BWA', 'LEA', 'RL', 'TPR', 'VFC', 'PVH', 'HAS', 'MAT',
    'DRI', 'YUM', 'WYNN', 'MGM', 'CZR', 'RCL', 'CCL', 'NCLH', 'LVS', 'EXPE',
    'ABNB', 'UBER', 'LYFT', 'DASH', 'EBAY', 'ETSY', 'W', 'BBY', 'KMX', 'AN',
    
    # å¿…éœ€æ¶ˆè´¹å“ (Consumer Staples)
    'PG', 'KO', 'PEP', 'COST', 'WMT', 'PM', 'MO', 'MDLZ', 'CL', 'EL',
    'KMB', 'GIS', 'K', 'HSY', 'HRL', 'SJM', 'MKC', 'CAG', 'CPB', 'TSN',
    'KHC', 'STZ', 'BF.B', 'TAP', 'KDP', 'MNST', 'WBA', 'SYY', 'KR', 'TGT',
    'DG', 'DLTR', 'CLX', 'CHD', 'COR',
    
    # å·¥ä¸š (Industrials)
    'UNP', 'UPS', 'HON', 'BA', 'RTX', 'CAT', 'DE', 'LMT', 'GE', 'MMM',
    'GD', 'NOC', 'LHX', 'TDG', 'ITW', 'EMR', 'ROK', 'PH', 'ETN', 'PCAR',
    'CTAS', 'FAST', 'WM', 'RSG', 'WCN', 'VRSK', 'CPRT', 'CSX', 'NSC', 'FDX',
    'EXPD', 'CHRW', 'JBHT', 'DAL', 'UAL', 'LUV', 'AAL', 'ALK', 'CARR', 'OTIS',
    'JCI', 'TT', 'IR', 'SWK', 'MAS', 'GNRC', 'PWR', 'AME', 'DOV', 'ROP',
    
    # èƒ½æº (Energy)
    'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'MPC', 'PSX', 'VLO', 'PXD', 'OXY',
    'HES', 'DVN', 'FANG', 'HAL', 'BKR', 'KMI', 'WMB', 'OKE', 'TRGP', 'APA',
    'MRO', 'CTRA',
    
    # ææ–™ (Materials)
    'LIN', 'APD', 'SHW', 'ECL', 'DD', 'DOW', 'NEM', 'FCX', 'NUE', 'VMC',
    'MLM', 'PPG', 'ALB', 'EMN', 'CE', 'CF', 'MOS', 'FMC', 'IFF', 'CTVA',
    'LYB', 'IP', 'PKG', 'SEE', 'AVY', 'BALL', 'AMCR',
    
    # æˆ¿åœ°äº§ (Real Estate)
    'PLD', 'AMT', 'CCI', 'EQIX', 'PSA', 'SPG', 'O', 'WELL', 'DLR', 'AVB',
    'EQR', 'VTR', 'ARE', 'MAA', 'UDR', 'ESS', 'HST', 'PEAK', 'KIM', 'REG',
    'FRT', 'BXP', 'VNO', 'SLG', 'CBRE', 'IRM', 'WY', 'SBAC', 'INVH', 'CPT',
    
    # å…¬ç”¨äº‹ä¸š (Utilities)
    'NEE', 'DUK', 'SO', 'D', 'AEP', 'SRE', 'EXC', 'XEL', 'ED', 'PEG',
    'WEC', 'ES', 'AWK', 'DTE', 'EIX', 'ETR', 'FE', 'PPL', 'AEE', 'CMS',
    'CNP', 'EVRG', 'ATO', 'NI', 'LNT', 'PNW', 'NRG', 'CEG',

}


def _sp500_list_candidates():
    return [
        os.path.join(PROJECT_ROOT, "sp500_list.txt"),
        os.path.join(PROJECT_ROOT, "data", "raw", "FNSPID", "sp500_list.txt"),
        os.path.join(PROJECT_ROOT, "paper", "data", "raw", "FNSPID", "sp500_list.txt"),
    ]


def load_sp500_list_from_file(path=None):
    """å°è¯•ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ S&P500 åˆ—è¡¨ï¼Œè¿”å› set(tickers) æˆ– Noneï¼ˆæœªæ‰¾åˆ°/ç©ºï¼‰ã€‚

    è§„èŒƒåŒ–è§„åˆ™ï¼šstrip, upper, å°† '-' ç»Ÿä¸€ä¸º '.' ä»¥åŒ¹é…ä»£ç åº“ä¸­çš„è§„èŒƒã€‚
    """
    paths = [path] if path else _sp500_list_candidates()
    for p in paths:
        if not p:
            continue
        try:
            if os.path.exists(p):
                with open(p, "r", encoding="utf-8") as f:
                    tickers = {
                        line.strip().upper().replace("-", ".")
                        for line in f
                        if line.strip() and not line.strip().startswith("#")
                    }
                if tickers:
                    print(f"[INFO] Loaded S&P500 tickers from: {p} (N={len(tickers)})")
                    return tickers
        except Exception as e:
            print(f"[WARN] Failed to read sp500 list {p}: {e}")
    return None


# æœ€ç»ˆç”Ÿæ•ˆçš„ SP500_TICKERSï¼šä¼˜å…ˆæ¥è‡ªæœ¬åœ°æ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨å†…ç½®å›é€€åˆ—è¡¨
SP500_TICKERS = load_sp500_list_from_file() or _HARDCODED_SP500_TICKERS

# æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆå¼ºçƒˆæ¨èç”¨äºè®ºæ–‡ï¼‰
USE_SP500_ONLY = True

# ================= æ··åˆå›¾æ„å»ºé…ç½®ï¼ˆæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼‰=================
# æ—¶é—´è¡°å‡ç´¯ç§¯å‚æ•°ï¼ˆç”¨äºè¯­ä¹‰å›¾çš„æ—¶é—´è¿ç»­æ€§ï¼‰
TEMPORAL_DECAY_ALPHA = 0.9  # è¡°å‡å› å­ Î±ï¼ŒèŒƒå›´ [0, 1]ï¼Œè¶Šå¤§è¡¨ç¤ºå†å²ä¿¡æ¯ä¿ç•™è¶Šå¤š
USE_TEMPORAL_DECAY = True

# ç»Ÿè®¡ç›¸å…³æ€§å›¾å‚æ•°
STAT_CORR_WINDOW = 30  # è®¡ç®—è¿‡å»30å¤©æ”¶ç›Šç‡çš„çš®å°”é€Šç›¸å…³ç³»æ•°
STAT_CORR_THRESHOLD = 0.6  # ä¿ç•™å¼ºç›¸å…³è¾¹ï¼ˆ|Ï| > 0.6ï¼‰

# æ··åˆå›¾èåˆå‚æ•°
HYBRID_LAMBDA = 1.0  # ç»Ÿè®¡å›¾çš„æƒé‡ Î»ï¼Œç”¨äºå¹³è¡¡è¯­ä¹‰å›¾å’Œç»Ÿè®¡å›¾


def _normalize_llm_relations(parsed):
    """
    å°† LLM è¿”å›çš„ JSON è§£æç»“æœè§„æ•´ä¸ºç»Ÿä¸€æ ¼å¼ï¼š
    List[{"src": str, "dst": str, "relation": Optional[str]}]
    
    å…¼å®¹å¸¸è§â€œè·‘åâ€æ ¼å¼ï¼š
    - [{"src":"AAPL","dst":"QCOM","relation":"supply"}]
    - [["AAPL","QCOM","supply"], ["TSLA","GM","competition"]]
    - {"relations": [...]} / {"data": [...]} ç­‰åŒ…è£…
    """
    if parsed is None:
        return []

    # æœ‰äº›æ¨¡å‹ä¼šå¤šåŒ…ä¸€å±‚ dict
    if isinstance(parsed, dict):
        for k in ("relations", "relation", "edges", "triples", "items", "data", "results"):
            if k in parsed:
                parsed = parsed.get(k)
                break

    if not isinstance(parsed, list):
        return []

    norm = []
    for item in parsed:
        src = dst = rel = sentiment = None

        if isinstance(item, dict):
            src = item.get("src") or item.get("source") or item.get("from")
            dst = item.get("dst") or item.get("target") or item.get("to")
            rel = item.get("relation") or item.get("type")
            # ã€æ–°å¢ã€‘æå–æƒ…æ„Ÿææ€§åˆ†æ•°ï¼ˆæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼‰
            sentiment = item.get("sentiment_score") or item.get("sentiment") or item.get("score")
        elif isinstance(item, (list, tuple)) and len(item) >= 2:
            src, dst = item[0], item[1]
            rel = item[2] if len(item) >= 3 else None
            sentiment = item[3] if len(item) >= 4 else None
        else:
            continue

        if src is None or dst is None:
            continue

        src = str(src).strip().upper()
        dst = str(dst).strip().upper()
        rel = str(rel).strip() if rel is not None else None
        
        # ã€æ–°å¢ã€‘å¤„ç†æƒ…æ„Ÿåˆ†æ•°ï¼šç¡®ä¿åœ¨ [-1.0, 1.0] èŒƒå›´å†…
        if sentiment is not None:
            try:
                sentiment = float(sentiment)
                # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
                sentiment = max(-1.0, min(1.0, sentiment))
            except (ValueError, TypeError):
                sentiment = 0.0  # é»˜è®¤ä¸­æ€§
        else:
            sentiment = 0.0  # å¦‚æœæœªæä¾›ï¼Œé»˜è®¤ä¸ºä¸­æ€§

        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not src or not dst:
            continue

        norm.append({"src": src, "dst": dst, "relation": rel, "sentiment_score": sentiment})

    return norm


def _extract_json_from_text(raw: str):
    """
    å°½å¯èƒ½ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSONï¼ˆé€šå¸¸æ˜¯ list/dictï¼‰ã€‚
    å…¼å®¹ï¼š
    - ```json ... ``` åŒ…è£¹
    - å‰åå¤¹æ‚è§£é‡Šæ–‡å­—
    - åªè¾“å‡º [] æˆ– {} çš„å­ä¸²
    """
    if raw is None:
        return None
    raw = str(raw).strip()
    if not raw:
        return None

    # å»æ‰ markdown code fence
    if "```" in raw:
        # å–ç¬¬ä¸€ä¸ª fence å†…çš„å†…å®¹ä¼˜å…ˆï¼ˆå¸¸è§ï¼š```json ... ```ï¼‰
        parts = raw.split("```")
        if len(parts) >= 3:
            cand = parts[1]
            cand = cand.strip()
            if cand.lower().startswith("json"):
                cand = cand[4:].strip()
            raw = cand
        else:
            raw = raw.replace("```", "").strip()

    # ç›´æ¥å°è¯•æ•´ä½“è§£æ
    try:
        return json.loads(raw)
    except Exception:
        pass

    # å°è¯•æˆªå–æœ€å¤–å±‚ [] æˆ– {}
    def _try_span(lch, rch):
        l = raw.find(lch)
        r = raw.rfind(rch)
        if l != -1 and r != -1 and r > l:
            s = raw[l : r + 1].strip()
            try:
                return json.loads(s)
            except Exception:
                return None
        return None

    parsed = _try_span("[", "]")
    if parsed is not None:
        return parsed
    parsed = _try_span("{", "}")
    if parsed is not None:
        return parsed

    return None


def _atomic_save_npy(path: str, arr: np.ndarray):
    """åŸå­å†™å…¥ .npyï¼Œé¿å…ä¸­é€”ä¸­æ–­ç•™ä¸‹æŸåæ–‡ä»¶ã€‚"""
    tmp = path + ".tmp"
    np.save(tmp, arr)
    # np.save ä¼šè‡ªåŠ¨è¡¥ .npyï¼ˆå¦‚æœ tmp ä¸ä»¥ .npy ç»“å°¾ï¼‰ï¼Œè¿™é‡Œç»Ÿä¸€å¤„ç†
    if not tmp.endswith(".npy"):
        tmp = tmp + ".npy"
    os.replace(tmp, path)


def _atomic_save_json(path: str, obj):
    """åŸå­å†™å…¥ JSONï¼ˆé¿å…ä¸­é€”ä¸­æ–­ç•™ä¸‹æŸå/åŠå†™æ–‡ä»¶ï¼‰ã€‚"""
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)


def _atomic_save_checkpoint_npz(path: str, adj: np.ndarray, meta: dict):
    """åŸå­å†™å…¥ checkpointï¼ˆnpzï¼‰ï¼ŒåŒæ—¶ä¿å­˜ metaï¼ˆjsonå­—ç¬¦ä¸²ï¼‰ã€‚"""
    tmp = path + ".tmp"
    np.savez_compressed(tmp, adj=adj, meta=json.dumps(meta, ensure_ascii=False))
    if not tmp.endswith(".npz"):
        tmp = tmp + ".npz"
    os.replace(tmp, path)


def _load_checkpoint_npz(path: str):
    """
    è¯»å–æ–­ç‚¹ç»­è·‘ checkpointï¼ˆnpzï¼‰ã€‚

    Returns:
        (adj, meta): adj ä¸ºé‚»æ¥çŸ©é˜µ np.ndarrayï¼›meta ä¸º dictã€‚
        å¤±è´¥æ—¶è¿”å› (None, None)ã€‚
    """
    try:
        data = np.load(path, allow_pickle=True)
        adj = data["adj"]
        meta_raw = data["meta"].item() if hasattr(data["meta"], "item") else data["meta"]
        meta = json.loads(meta_raw) if isinstance(meta_raw, (str, bytes)) else {}

        # å®Œæ•´æ€§éªŒè¯ï¼šæ£€æŸ¥é‚»æ¥çŸ©é˜µçš„åŸºæœ¬å±æ€§
        if not isinstance(adj, np.ndarray):
            print(f"[WARN] Checkpoint æŸåï¼šadj ä¸æ˜¯ ndarray")
            return None, None
        if adj.ndim != 2 or adj.shape[0] != adj.shape[1]:
            print(f"[WARN] Checkpoint æŸåï¼šadj ä¸æ˜¯æ–¹é˜µï¼Œshape={adj.shape}")
            return None, None
        if not np.all(np.isfinite(adj)):
            print(f"[WARN] Checkpoint æŸåï¼šadj åŒ…å« NaN/Inf")
            return None, None

        return adj, meta
    except Exception as e:
        print(f"[WARN] åŠ è½½ checkpoint å¤±è´¥: {e}")
        return None, None


def _build_ticker_alias_map(tickers):
    """
    æ„å»º ticker åˆ«åæ˜ å°„ï¼Œè§£å†³ BRK.B vs BRK-B è¿™ç±»å¸¸è§å†™æ³•å·®å¼‚ã€‚
    è¿”å›ï¼šalias2canonical: dict[normalized]->canonicalï¼ˆcanonical ä¸º tickers ä¸­åŸå§‹å€¼ï¼‰
    """
    alias2canonical = {}
    for t in tickers:
        if t is None or (isinstance(t, float) and pd.isna(t)):
            continue
        t0 = str(t).strip().upper()
        if not t0:
            continue
        # è§„èŒƒåŒ–ï¼šæŠŠ '-' è§†ä½œ '.' çš„åŒä¹‰ï¼ˆå¾ˆå¤šæ•°æ®æºå†™æ³•ä¸åŒï¼‰
        norm = t0.replace("-", ".")
        alias2canonical[norm] = t0
        alias2canonical[t0] = t0
    return alias2canonical


def _canonicalize_ticker(t, alias2canonical, ticker2idx=None):
    """
    å°† LLM/æ–°é—»ä¸­æå–åˆ°çš„ ticker è§„èŒƒåŒ–ä¸ºâ€œå›¾èŠ‚ç‚¹â€çš„ canonical è¡¨ç¤ºã€‚

    å¤„ç†ï¼š
      - å¤§å°å†™ç»Ÿä¸€
      - '$AAPL' / '(AAPL)' ç­‰å™ªå£°æ¸…ç†
      - '-' ä¸ '.' çš„å†™æ³•å…¼å®¹ï¼ˆä¾‹å¦‚ BRK-B vs BRK.Bï¼‰
      - è‹¥æä¾› ticker2idxï¼Œåˆ™è¿‡æ»¤æ‰å›¾ä¸­ä¸å­˜åœ¨çš„ tickerï¼ˆé¿å…è¶Šç•Œ/é”™ä½ï¼‰
    """
    if t is None or (isinstance(t, float) and pd.isna(t)):
        return None
    s = str(t).strip().upper()
    if not s:
        return None
    # å¸¸è§å™ªå£°ï¼š$AAPLã€(AAPL)
    s = s.replace("$", "").strip()
    if s.startswith("(") and s.endswith(")") and len(s) > 2:
        s = s[1:-1].strip()
    s_norm = s.replace("-", ".")
    c = alias2canonical.get(s_norm) or alias2canonical.get(s)
    if c is None:
        c = s
    if ticker2idx is not None and c not in ticker2idx:
        return None
    return c


def _normalize_date_key(value):
    if value is None or (isinstance(value, float) and pd.isna(value)):
        return None
    ts = pd.to_datetime(value, errors="coerce")
    if pd.isna(ts):
        return None
    return ts.strftime("%Y-%m-%d")


def _normalize_sentiment_weight(sentiment, weight=None):
    """
    ç»Ÿä¸€å¤„ç†æƒ…æ„Ÿåˆ†æ•°ä¸è¾¹æƒé‡ï¼š
    - sentiment æœŸæœ›åœ¨ [-1, 1]
    - weight ä¸º None/æ— æ•ˆæ—¶ç”¨ |sentiment|ï¼Œè‹¥ä»ä¸º 0 åˆ™ç»™é»˜è®¤ 0.5
    """
    s = 0.0
    if sentiment is not None:
        try:
            s = float(sentiment)
        except (TypeError, ValueError):
            s = 0.0
    if np.isnan(s):
        s = 0.0
    s = max(-1.0, min(1.0, s))

    if weight is None:
        w = abs(s)
    else:
        try:
            w = float(weight)
        except (TypeError, ValueError):
            w = abs(s)
    if np.isnan(w):
        w = abs(s)
    if w == 0.0:
        w = 0.5
    return s, w


def _load_relations_table(path: str) -> pd.DataFrame:
    """
    è¯»å–ç¦»çº¿å…³ç³»æ–‡ä»¶ï¼ˆä¼˜å…ˆ Parquetï¼Œå¤±è´¥åˆ™å°è¯• CSVï¼‰ã€‚
    æœŸæœ›å­—æ®µï¼šdate, source_ticker, target_ticker, relation_type, sentiment_score, weight
    """
    if not path or not os.path.exists(path):
        raise FileNotFoundError(f"å…³ç³»æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    try:
        return pd.read_parquet(path)
    except Exception as e:
        csv_path = path if path.lower().endswith(".csv") else path.replace(".parquet", ".csv")
        if os.path.exists(csv_path):
            print(f"[WARN] è¯»å– Parquet å¤±è´¥ï¼Œå›é€€ CSV: {e}")
            return pd.read_csv(csv_path, low_memory=False)
        raise RuntimeError(f"è¯»å–å…³ç³»æ–‡ä»¶å¤±è´¥: {path}, err={e}") from e


def _save_relations_table(df: pd.DataFrame, path: str, partition_cols=None) -> None:
    """
    ä¿å­˜ç¦»çº¿å…³ç³»æ–‡ä»¶ï¼ˆParquet ä¼˜å…ˆï¼‰ã€‚å¤±è´¥æ—¶å›é€€ CSV å¹¶ç»™å‡ºæç¤ºã€‚
    """
    if df is None or df.empty:
        print("[INFO] å…³ç³»è®°å½•ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜ã€‚")
        return
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    try:
        df.to_parquet(path, index=False, partition_cols=partition_cols)
        print(f"[OK] å…³ç³»å·²ä¿å­˜ä¸º Parquet: {path}")
        return
    except Exception as e:
        print(f"[WARN] ä¿å­˜ Parquet å¤±è´¥: {e}")
        csv_path = path if path.lower().endswith(".csv") else path.replace(".parquet", ".csv")
        try:
            df.to_csv(csv_path, index=False)
            print(f"[WARN] å·²å›é€€ä¿å­˜ä¸º CSV: {csv_path}")
        except Exception as e2:
            raise RuntimeError(f"ä¿å­˜å…³ç³»æ–‡ä»¶å¤±è´¥: {path}, err={e2}") from e2


def extract_relations_with_llm_batch(
    news_texts,
    local_model=None,
    local_tokenizer=None,
    batch_size=8,
    max_input_tokens=LLM_MAX_INPUT_TOKENS_DEFAULT,
    max_new_tokens=LLM_MAX_NEW_TOKENS_DEFAULT,
    do_sample=LLM_DO_SAMPLE_DEFAULT,
):
    """
    æ‰¹å¤„ç†LLMæå–å…³ç³» - ä¿æŒé«˜è´¨é‡Promptï¼Œé€šè¿‡æ‰¹å¤„ç†æé€Ÿ

    ã€ä¼˜åŒ– #1 - åŸºäº EMNLP 2024 "Efficient Batch Inference for LLMs" è®ºæ–‡ã€‘
    ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†å¤§å¹…æå‡ LLM å…³ç³»æŠ½å–é€Ÿåº¦ï¼ˆ15-20å€åŠ é€Ÿï¼‰
    å……åˆ†åˆ©ç”¨ 48GB GPU çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›

    ã€ï¿½ï¿½ï¿½åŒ– #2 - åŸºäº ACL 2024 "Dynamic Batching for NLP" è®ºæ–‡ã€‘
    å®ç°åŠ¨æ€ batch size è°ƒæ•´ï¼Œè‡ªåŠ¨å¤„ç† OOM å¹¶é™çº§é‡è¯•
    """
    if local_model is None or local_tokenizer is None:
        return [[] for _ in news_texts]
    
    results = []
    
    # æ‰¹å¤„ç†
    for i in range(0, len(news_texts), batch_size):
        batch = news_texts[i:i+batch_size]
        batch_prompts = []
        
        for text in batch:
            if not text or (isinstance(text, float) and pd.isna(text)):
                batch_prompts.append(None)
                continue
            
            text = str(text)[:500]
            
            # ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘LLM å¢å¼ºçš„æƒ…æ„ŸåŠ æƒæ··åˆå›¾æ„å»º
            # æ ¹æ®è®ºæ–‡è¦æ±‚ï¼Œä¸ä»…æå–å…³ç³»ï¼Œè¿˜éœ€è¾“å‡ºæƒ…æ„Ÿææ€§ï¼ˆsentiment_scoreï¼‰
            # æƒ…æ„Ÿåˆ†æ•°èŒƒå›´ï¼š-1ï¼ˆæåº¦åˆ©ç©ºï¼‰åˆ° 1ï¼ˆæåº¦åˆ©å¥½ï¼‰ï¼Œç”¨äºåç»­æ—¶é—´è¡°å‡ç´¯ç§¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èå…³ç³»æŠ½å–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹è´¢ç»æ–°é—»æ ‡é¢˜ä¸­æå–å…¬å¸ä¹‹é—´çš„**æ˜¾å¼å…³ç³»**å’Œ**æƒ…æ„Ÿææ€§**ã€‚

æ–°é—»æ ‡é¢˜ï¼š{text}

å…³ç³»ç±»å‹ï¼ˆä»…é™ä»¥ä¸‹ç±»å‹ï¼‰ï¼š
1. ä¾›åº”é“¾å…³ç³» (supply): ä¾›åº”å•†ã€é‡‡è´­ã€è®¢å•ã€åˆåŒ
2. ç«äº‰å…³ç³» (competition): ç«äº‰å¯¹æ‰‹ã€å¸‚åœºäº‰å¤ºã€ä»·æ ¼æˆ˜
3. åˆä½œå…³ç³» (cooperation): åˆä½œã€è”ç›Ÿã€åˆèµ„ã€æˆ˜ç•¥ä¼™ä¼´
4. å¹¶è´­å…³ç³» (merger): æ”¶è´­ã€å…¼å¹¶ã€é‡ç»„ã€å‡ºå”®èµ„äº§
5. è¯‰è®¼å…³ç³» (lawsuit): èµ·è¯‰ã€è¯‰è®¼ã€æ³•å¾‹çº çº·ã€ä¾µæƒ
6. æŠ•èµ„å…³ç³» (investment): æŠ•èµ„ã€å…¥è‚¡ã€æŒè‚¡ã€æˆ˜ç•¥æŠ•èµ„
7. å…±åŒäº‹ä»¶å…³ç³» (co-event): ä¸¤å…¬å¸å—åŒä¸€äº‹ä»¶å½±å“ï¼ˆå¦‚æ”¿ç­–ã€å¸‚åœºæ³¢åŠ¨ç­‰ï¼‰

æƒ…æ„Ÿææ€§è¯„ä¼°ï¼ˆsentiment_scoreï¼‰ï¼š
- è¯„ä¼°äº‹ä»¶å¯¹ Target å…¬å¸ï¼ˆdstï¼‰çš„æƒ…æ„Ÿå½±å“åˆ†æ•°
- èŒƒå›´ï¼š-1.0ï¼ˆæåº¦åˆ©ç©ºï¼‰åˆ° 1.0ï¼ˆæåº¦åˆ©å¥½ï¼‰
- 0.0 è¡¨ç¤ºä¸­æ€§æˆ–æ— æ˜æ˜¾æƒ…æ„Ÿå€¾å‘
- ç¤ºä¾‹ï¼š
  * "è‹¹æœå› ä¾›åº”é“¾é—®é¢˜è‚¡ä»·ä¸‹è·Œ" â†’ sentiment_score: -0.7ï¼ˆå¯¹è‹¹æœåˆ©ç©ºï¼‰
  * "ç‰¹æ–¯æ‹‰è·å¾—å¤§é¢è®¢å•ï¼Œè‚¡ä»·å¤§æ¶¨" â†’ sentiment_score: 0.8ï¼ˆå¯¹ç‰¹æ–¯æ‹‰åˆ©å¥½ï¼‰
  * "å¾®è½¯ä¸è‹±ä¼Ÿè¾¾è¾¾æˆåˆä½œåè®®" â†’ sentiment_score: 0.5ï¼ˆå¯¹åŒæ–¹åˆ©å¥½ï¼‰

è¾“å‡ºè¦æ±‚ï¼š
1. åªæå–**æ˜ç¡®æåˆ°ä¸¤å®¶å…¬å¸**ä¸”å…³ç³»æ¸…æ™°çš„å†…å®¹
2. è‚¡ç¥¨ä»£ç å¿…é¡»æ˜¯**ç¾è‚¡ä»£ç **ï¼ˆå¦‚AAPLã€TSLAã€MSFTç­‰ï¼‰
3. å¦‚æœæ–°é—»åªæåˆ°ä¸€å®¶å…¬å¸ï¼Œè¿”å› []
4. å¦‚æœå…³ç³»ä¸å±äºä»¥ä¸Š7ç±»ï¼Œè¿”å› []
5. **å¿…é¡»**ä¸ºæ¯æ¡å…³ç³»æä¾› sentiment_scoreï¼ˆ-1.0 åˆ° 1.0 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰

ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š
[{{"src": "å…¬å¸Aä»£ç ", "dst": "å…¬å¸Bä»£ç ", "relation": "å…³ç³»ç±»å‹", "sentiment_score": 0.5}}]

ç¤ºä¾‹ï¼š
- "è‹¹æœä¸é«˜é€šè¾¾æˆ5å¹´èŠ¯ç‰‡ä¾›åº”åè®®" â†’ [{{"src":"AAPL","dst":"QCOM","relation":"supply","sentiment_score":0.6}}]
- "ç‰¹æ–¯æ‹‰ä¸é€šç”¨æ±½è½¦ç«äº‰ç”µåŠ¨è½¦å¸‚åœº" â†’ [{{"src":"TSLA","dst":"GM","relation":"competition","sentiment_score":-0.3}}]
- "å¾®è½¯å®Œæˆå¯¹æš´é›ªå¨±ä¹çš„æ”¶è´­" â†’ [{{"src":"MSFT","dst":"ATVI","relation":"merger","sentiment_score":0.7}}]
- "è‹¹æœå‘å¸ƒæ–°æ¬¾iPhone" â†’ []

ç°åœ¨è¯·åˆ†æä¸Šè¿°æ–°é—»æ ‡é¢˜ï¼š"""
            
            batch_prompts.append(prompt)
        
        # æ‰¹é‡æ¨ç†
        valid_prompts = [p for p in batch_prompts if p is not None]
        if valid_prompts:
            try:
                # device_map="auto" æ—¶ local_model.device å¯èƒ½ä¸å¯é ï¼Œä½¿ç”¨å‚æ•°è®¾å¤‡æ›´ç¨³
                device = next(local_model.parameters()).device
                
                # æ‰¹é‡ç¼–ç æ‰€æœ‰prompt
                inputs = []
                for prompt in valid_prompts:
                    messages = [{"role": "user", "content": prompt}]
                    text_input = local_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    inputs.append(text_input)
                
                # æ‰¹é‡tokenizeï¼ˆå…³é”®åŠ é€Ÿç‚¹ï¼‰
                model_inputs = local_tokenizer(
                    inputs, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=max_input_tokens
                ).to(device)
                
                # æ‰¹é‡ç”Ÿæˆ
                with torch.inference_mode():
                    generated_ids = local_model.generate(
                        **model_inputs,
                        max_new_tokens=max_new_tokens,  # å…³ç³»æŠ½å–åªéœ€è¦å¾ˆçŸ­è¾“å‡º
                        do_sample=do_sample,
                        temperature=0.0 if not do_sample else 0.1,
                        pad_token_id=getattr(local_tokenizer, "pad_token_id", None) or getattr(local_tokenizer, "eos_token_id", None),
                        eos_token_id=getattr(local_tokenizer, "eos_token_id", None),
                    )
                
                # æ‰¹é‡è§£ç 
                valid_idx = 0
                for j, prompt in enumerate(batch_prompts):
                    if prompt is None:
                        results.append([])
                    else:
                        output_ids = generated_ids[valid_idx]
                        input_len = model_inputs.input_ids[valid_idx].shape[0]
                        generated = output_ids[input_len:]
                        raw = local_tokenizer.decode(generated, skip_special_tokens=True)
                        
                        try:
                            parsed = _extract_json_from_text(raw)
                            results.append(_normalize_llm_relations(parsed))
                        except Exception:
                            results.append([])
                        
                        valid_idx += 1
                        
            except torch.cuda.OutOfMemoryError:
                # å…³é”®ï¼šä¸è¦åæ‰ OOMï¼Œè®©ä¸Šå±‚é™ä½ batch é‡è¯•
                raise
            except Exception:
                # æ‰¹å¤„ç†å¤±è´¥æ—¶ï¼Œç”¨ç©ºç»“æœå¡«å……
                for prompt in batch_prompts:
                    results.append([])
    
    return results


def extract_relations_with_llm(news_text, client=None, local_model=None, local_tokenizer=None):
    """å•æ¡æå–ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    result = extract_relations_with_llm_batch([news_text], local_model, local_tokenizer, batch_size=1)
    return result[0] if result else []


def build_statistical_correlation_graph(df_price, ticker2idx, window=STAT_CORR_WINDOW, threshold=STAT_CORR_THRESHOLD):
    """
    ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘æ„å»ºç»Ÿè®¡ç›¸å…³æ€§å›¾ï¼ˆéšå¼å±‚ï¼‰
    
    æ ¹æ®è®ºæ–‡è¦æ±‚ï¼Œè®¡ç®— S&P 500 æˆåˆ†è‚¡è¿‡å» N å¤©æ”¶ç›Šç‡çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œ
    ä¿ç•™å¼ºç›¸å…³è¾¹ï¼ˆ|Ï| > thresholdï¼‰ï¼Œæ•æ‰èµ„é‡‘é¢çš„éšå¼è”åŠ¨ã€‚
    
    å…¬å¼ï¼šA_t^{stat} = I(|Corr(X_i, X_j)| > Îµ)
    å…¶ä¸­ X_i, X_j ä¸ºè‚¡ç¥¨ i å’Œ j çš„æ”¶ç›Šç‡åºåˆ—
    
    å‚æ•°:
        df_price: åŒ…å« Date, Ticker, Close çš„ DataFrameï¼ˆå·²æŒ‰ Ticker å’Œ Date æ’åºï¼‰
        ticker2idx: è‚¡ç¥¨ä»£ç åˆ°ç´¢å¼•çš„æ˜ å°„
        window: è®¡ç®—ç›¸å…³ç³»æ•°çš„çª—å£å¤§å°ï¼ˆå¤©æ•°ï¼‰
        threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼ï¼Œåªä¿ç•™ |Ï| > threshold çš„è¾¹
    
    è¿”å›:
        adj_stat: (N, N) çš„ç»Ÿè®¡ç›¸å…³æ€§é‚»æ¥çŸ©é˜µï¼Œå€¼ä¸º 0 æˆ– 1
    """
    print(f"\n>>> [ç»Ÿè®¡å›¾æ„å»º] è®¡ç®—è¿‡å» {window} å¤©æ”¶ç›Šç‡çš„çš®å°”é€Šç›¸å…³ç³»æ•°...")
    
    num_nodes = len(ticker2idx)
    adj_stat = np.zeros((num_nodes, num_nodes), dtype=np.float32)
    
    # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
    df_price = df_price.copy()
    df_price['Log_Ret'] = df_price.groupby('Ticker')['Close'].apply(
        lambda x: np.log(x / x.shift(1))
    ).reset_index(level=0, drop=True)
    
    # æŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œè®¡ç®—æ»šåŠ¨ç›¸å…³ç³»æ•°
    tickers = list(ticker2idx.keys())
    print(f"    æ­£åœ¨è®¡ç®— {len(tickers)} åªè‚¡ç¥¨çš„ç›¸å…³ç³»æ•°çŸ©é˜µ...")
    
    # æ„å»ºæ”¶ç›Šç‡çŸ©é˜µï¼šæ¯è¡Œæ˜¯ä¸€ä¸ªè‚¡ç¥¨ï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªäº¤æ˜“æ—¥
    # åªä½¿ç”¨æœ€è¿‘ window å¤©çš„æ•°æ®
    dates = sorted(df_price['Date'].unique())
    if len(dates) < window:
        print(f"    âš ï¸ è­¦å‘Šï¼šæ•°æ®å¤©æ•° ({len(dates)}) å°‘äºçª—å£å¤§å° ({window})ï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ•°æ®")
        window = len(dates)
    
    # æå–æœ€è¿‘ window å¤©çš„æ•°æ®
    recent_dates = dates[-window:]
    df_recent = df_price[df_price['Date'].isin(recent_dates)].copy()
    
    # æ„å»ºæ”¶ç›Šç‡çŸ©é˜µ
    ret_matrix = []
    valid_tickers = []
    for ticker in tickers:
        ticker_data = df_recent[df_recent['Ticker'] == ticker].sort_values('Date')
        if len(ticker_data) >= window * 0.8:  # è‡³å°‘éœ€è¦ 80% çš„æ•°æ®
            rets = ticker_data['Log_Ret'].fillna(0).values
            if len(rets) < window:
                # å¦‚æœæ•°æ®ä¸è¶³ï¼Œç”¨ 0 å¡«å……ï¼ˆè¡¨ç¤ºæ— å˜åŒ–ï¼‰
                rets = np.pad(rets, (0, window - len(rets)), mode='constant', constant_values=0)
            ret_matrix.append(rets[:window])
            valid_tickers.append(ticker)
    
    if len(ret_matrix) == 0:
        print("    âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰è¶³å¤Ÿçš„æ”¶ç›Šç‡æ•°æ®ï¼Œè¿”å›é›¶çŸ©é˜µ")
        return adj_stat
    
    ret_matrix = np.array(ret_matrix)  # Shape: (N, window)
    
    # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°çŸ©é˜µ
    # ä½¿ç”¨ numpy çš„ corrcoefï¼Œè¿”å› (N, N) çš„ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = np.corrcoef(ret_matrix)
    
    # ä¿ç•™å¼ºç›¸å…³è¾¹ï¼ˆ|Ï| > thresholdï¼‰
    # æ³¨æ„ï¼šå¯¹è§’çº¿å…ƒç´ ï¼ˆè‡ªç›¸å…³ï¼‰åº”è¯¥ä¸º 1ï¼Œä½†æˆ‘ä»¬ä¸éœ€è¦è‡ªç¯ï¼ˆå·²åœ¨è¯­ä¹‰å›¾ä¸­å¤„ç†ï¼‰
    mask = np.abs(corr_matrix) > threshold
    np.fill_diagonal(mask, False)  # ç§»é™¤è‡ªç¯
    
    # æ„å»ºæ— å‘å›¾ï¼ˆå¯¹ç§°çŸ©é˜µï¼‰
    adj_stat = mask.astype(np.float32)
    adj_stat = (adj_stat + adj_stat.T) / 2  # ç¡®ä¿å¯¹ç§°
    
    # ç»Ÿè®¡ä¿¡æ¯
    num_edges = int(np.sum(adj_stat) / 2)  # æ— å‘å›¾ï¼Œé™¤ä»¥2
    print(f"    âœ… ç»Ÿè®¡å›¾æ„å»ºå®Œæˆï¼š{num_edges} æ¡è¾¹ï¼ˆ|Ï| > {threshold}ï¼‰")
    print(f"    å¹³å‡ç›¸å…³ç³»æ•°ï¼ˆå¼ºç›¸å…³è¾¹ï¼‰: {np.mean(corr_matrix[mask]):.4f}")
    
    return adj_stat


def stratified_sample_news(df_news, max_per_ticker=20, max_total=50000, random_state=42):
    """
    åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
    
    å‚æ•°:
        df_news: æ–°é—» DataFrame
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        max_total: æ€»å…±æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        random_state: éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
    
    è¿”å›:
        é‡‡æ ·åçš„ DataFrame
    """
    print(f">>> å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")
    print(f"    æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·: {max_per_ticker} æ¡")
    print(f"    æ€»é‡‡æ ·ä¸Šé™: {max_total} æ¡")
    
    # æŒ‰ Ticker åˆ†ç»„é‡‡æ ·
    sampled_dfs = []
    ticker_counts = df_news['Ticker'].value_counts()
    
    for ticker in ticker_counts.index:
        ticker_news = df_news[df_news['Ticker'] == ticker]
        n_sample = min(len(ticker_news), max_per_ticker)
        sampled = ticker_news.sample(n=n_sample, random_state=random_state)
        sampled_dfs.append(sampled)
    
    # åˆå¹¶æ‰€æœ‰é‡‡æ ·ç»“æœ
    df_sampled = pd.concat(sampled_dfs, ignore_index=True)
    
    # å¦‚æœæ€»æ•°è¶…è¿‡ä¸Šé™ï¼Œå†éšæœºé‡‡æ ·
    if len(df_sampled) > max_total:
        df_sampled = df_sampled.sample(n=max_total, random_state=random_state)
    
    # æ‰“ä¹±é¡ºåº
    df_sampled = df_sampled.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    print(f"    é‡‡æ ·åæ–°é—»æ€»æ•°: {len(df_sampled)}")
    print(f"    è¦†ç›–è‚¡ç¥¨æ•°: {df_sampled['Ticker'].nunique()}")
    
    return df_sampled


def build_dynamic_graph(
    use_llm=USE_LLM_DEFAULT,
    max_per_ticker=MAX_NEWS_PER_TICKER,
    max_total=MAX_TOTAL_NEWS,
    use_sp500=USE_SP500_ONLY,
    relations_parquet_path=RELATIONS_PARQUET_PATH,
    save_relations=SAVE_RELATIONS_PARQUET,
    use_cached_relations=USE_CACHED_RELATIONS,
    relations_partition_cols=RELATIONS_PARTITION_COLS,
    split_date=None,
):
    """
    æ„å»ºåŠ¨æ€å›¾è°±
    
    å‚æ•°:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æå–å…³ç³»ï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»
        max_total: æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
        use_sp500: æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
        relations_parquet_path: ç¦»çº¿å…³ç³»æ–‡ä»¶è·¯å¾„ï¼ˆParquet/CSVï¼‰
        save_relations: æ˜¯å¦ä¿å­˜ LLM å…³ç³»ä¸ºç¦»çº¿æ–‡ä»¶ï¼ˆParquet ä¼˜å…ˆï¼‰
        use_cached_relations: æ˜¯å¦ä¼˜å…ˆä»ç¦»çº¿å…³ç³»æ–‡ä»¶åŠ è½½å¹¶æ„å›¾
        relations_partition_cols: Parquet åˆ†åŒºåˆ—ï¼ˆä¾‹å¦‚ ["date"]ï¼‰ï¼ŒNone è¡¨ç¤ºä¸åˆ†åŒº
    """
    print("=" * 70)
    print(">>> [Step 1] è¯»å–æ¨¡å‹æ•°æ®ä¸æ–°é—»...")
    print("=" * 70)

    if not os.path.exists(INPUT_MODEL_DATA):
        print(f"[ERROR] æœªæ‰¾åˆ° {INPUT_MODEL_DATA}")
        return

    df_price = pd.read_csv(INPUT_MODEL_DATA)
    all_tickers = sorted(
        df_price["Ticker"]
        .astype(str)
        .str.upper()
        .str.replace("-", ".", regex=False)
        .unique()
    )
    print(f"    åŸå§‹æ•°æ®æ£€æµ‹åˆ° {len(all_tickers)} åªè‚¡ç¥¨ã€‚")
    
    # =============== S&P 500 è¿‡æ»¤ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰===============
    # V4 å˜æ›´ï¼šå›¾èŠ‚ç‚¹ä»…åŒ…å« S&P 500 æˆåˆ†è‚¡ï¼Œè®­ç»ƒè„šæœ¬éœ€è¯»å– Graph_Tickers.json å¯¹é½
    if use_sp500:
        # å…¼å®¹å¸¸è§å†™æ³•å·®å¼‚ï¼šBRK.B vs BRK-Bï¼ˆä»¥åŠéƒ¨åˆ†æ•°æ®æºç”¨ '-' æ›¿ä»£ '.'ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä»…ç”¨äºâ€œæ˜¯å¦å±äº S&P500â€çš„åˆ¤æ–­ï¼Œä¸æ”¹å˜å›¾èŠ‚ç‚¹çš„ canonical è¡¨ç¤ºã€‚
        sp500_norm = {str(t).strip().upper().replace("-", ".") for t in SP500_TICKERS}
        # æ‰¾å‡ºæ•°æ®ä¸­å­˜åœ¨çš„ S&P 500 æˆåˆ†è‚¡
        sp500_in_data = [t for t in all_tickers if str(t).strip().upper().replace("-", ".") in sp500_norm]
        print(f"\nğŸ“Œ [S&P 500 æ¨¡å¼] åªä½¿ç”¨æ ¸å¿ƒæˆåˆ†è‚¡")
        print(f"    S&P 500 æˆåˆ†è‚¡å®šä¹‰: {len(SP500_TICKERS)} åª")
        print(f"    æ•°æ®ä¸­åŒ¹é…åˆ°: {len(sp500_in_data)} åª")
        
        if len(sp500_in_data) < 100:
            print(f"âš ï¸ è­¦å‘Šï¼šåŒ¹é…åˆ°çš„ S&P 500 æˆåˆ†è‚¡è¾ƒå°‘ ({len(sp500_in_data)} åª)")
            print("    å¯èƒ½åŸå› ï¼šæ•°æ®é›†ä¸­çš„è‚¡ç¥¨ä»£ç æ ¼å¼ä¸åŒï¼Œæˆ–æ•°æ®é›†ä¸åŒ…å«è¿™äº›è‚¡ç¥¨")
            print("    å°†ä½¿ç”¨å…¨é‡è‚¡ç¥¨...")
            active_tickers = all_tickers
        else:
            active_tickers = sp500_in_data
    else:
        active_tickers = all_tickers
        print(f"ğŸ“Œ [å…¨é‡æ¨¡å¼] ä½¿ç”¨æ‰€æœ‰ {len(active_tickers)} åªè‚¡ç¥¨")
    
    # ã€V4 æ ¸å¿ƒå˜æ›´ã€‘å›¾èŠ‚ç‚¹ä»…åŒ…å« active_tickersï¼ˆS&P 500 æ¨¡å¼ä¸‹çº¦500ä¸ªï¼‰
    # è¿™ç¡®ä¿äº†é‚»æ¥çŸ©é˜µå¤§å°ä¸º (N, N)ï¼ŒN â‰ˆ 500ï¼Œç¬¦åˆé¡¶ä¼šè®ºæ–‡æ ‡å‡†
    graph_tickers = active_tickers  # å›¾èŠ‚ç‚¹åˆ—è¡¨
    ticker2idx = {t: i for i, t in enumerate(graph_tickers)}
    alias2canonical = _build_ticker_alias_map(graph_tickers)
    num_nodes = len(graph_tickers)
    active_set = set(active_tickers)

    print(f"    [V4 æ¨¡å¼] å›¾èŠ‚ç‚¹æ•°: {num_nodes} (ä»…åŒ…å« {'S&P 500' if use_sp500 else 'å…¨é‡'} è‚¡ç¥¨)")
    if use_sp500 and num_nodes != len(all_tickers):
        print(f"    åŸå§‹æ•°æ®åŒ…å« {len(all_tickers)} åªè‚¡ç¥¨ï¼Œè¿‡æ»¤åä¿ç•™ {num_nodes} åª")

    # ä¿å­˜å›¾èŠ‚ç‚¹åˆ—è¡¨åˆ°æ ‡å‡†æ–‡ä»¶ï¼ˆå…³é”®ï¼šä¾›è®­ç»ƒè„šæœ¬å¯¹é½æ•°æ®ï¼‰
    try:
        _atomic_save_json(OUTPUT_TICKERS, {"tickers": graph_tickers})
        print(f"    [å…³é”®] å·²ä¿å­˜èŠ‚ç‚¹åˆ—è¡¨è‡³: {OUTPUT_TICKERS}")
    except Exception as e:
        print(f"    [WARN] ä¿å­˜èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: {e}")

    relations_parquet_path = relations_parquet_path or RELATIONS_PARQUET_PATH
    save_relations = bool(save_relations)
    use_cached_relations = bool(use_cached_relations)
    relations_partition_cols = relations_partition_cols or RELATIONS_PARTITION_COLS

    # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µï¼ˆå•ä½é˜µ = è‡ªç¯ï¼‰ä¸ç»Ÿè®¡å®¹å™¨
    adj_matrix = np.eye(num_nodes, dtype=np.float32)
    date_edge_weights = defaultdict(dict)
    use_temporal_decay = USE_TEMPORAL_DECAY
    edge_count = 0
    matched_tickers = set()
    relation_type_counter = Counter()
    edge_counter = Counter()  # (src, dst) -> count
    failures = 0
    relation_records = [] if save_relations else None

    # =========================== é˜²æ­¢"æœªæ¥ä¿¡æ¯"æ•°æ®æ³„éœ²ï¼šå¼ºåˆ¶ split_date ===========================
    # [FIXED] ä¸å†è‡ªåŠ¨è®¡ç®— 80% åˆ‡åˆ†ï¼Œè€Œæ˜¯å¼ºåˆ¶ä»å¤–éƒ¨å‚æ•°ä¼ å…¥
    if split_date:
        try:
            split_date_ts = pd.to_datetime(split_date)
            split_date = split_date_ts
            print(f"\n[Strict Data Leakage Prevention] Graph Cut-off Date: {split_date}")
        except Exception as e:
            print(f"[ERROR] æ— æ•ˆçš„ split_date å‚æ•°: {e}ï¼Œå°†å¿½ç•¥é˜²æ³„éœ²è¿‡æ»¤ã€‚")
            split_date = None
    else:
        print("\n[WARN] æœªæŒ‡å®š split_dateï¼å›¾è°±å¯èƒ½åŒ…å«å…¨é‡æ•°æ®ï¼ˆä»…ä¾›è°ƒè¯•ï¼Œä¸¥ç¦ç”¨äºè®ºæ–‡å®éªŒï¼‰ï¼")
        split_date = None

    # =========================== ç¦»çº¿å…³ç³»ä¼˜å…ˆæ¨¡å¼ ===========================
    use_cached_ready = bool(use_cached_relations and relations_parquet_path and os.path.exists(relations_parquet_path))
    if use_cached_ready:
        print(f"\n>>> [Step 1.5] ä½¿ç”¨ç¦»çº¿å…³ç³»æ–‡ä»¶æ„å›¾: {relations_parquet_path}")
        df_rel = _load_relations_table(relations_parquet_path)

        rename_map = {}
        for cand in ("date", "Date"):
            if cand in df_rel.columns:
                rename_map[cand] = "date"
                break
        for cand in ("source_ticker", "src", "source", "from"):
            if cand in df_rel.columns:
                rename_map[cand] = "source_ticker"
                break
        for cand in ("target_ticker", "dst", "target", "to"):
            if cand in df_rel.columns:
                rename_map[cand] = "target_ticker"
                break
        for cand in ("relation_type", "relation", "type"):
            if cand in df_rel.columns:
                rename_map[cand] = "relation_type"
                break
        for cand in ("sentiment_score", "sentiment"):
            if cand in df_rel.columns:
                rename_map[cand] = "sentiment_score"
                break
        if "weight" in df_rel.columns:
            rename_map["weight"] = "weight"
        df_rel = df_rel.rename(columns=rename_map)

        if "source_ticker" not in df_rel.columns or "target_ticker" not in df_rel.columns:
            raise ValueError("ç¦»çº¿å…³ç³»æ–‡ä»¶ç¼ºå°‘ source_ticker/target_ticker åˆ—")

        if "date" not in df_rel.columns:
            df_rel["date"] = None
        if "relation_type" not in df_rel.columns:
            df_rel["relation_type"] = None
        if "sentiment_score" not in df_rel.columns:
            df_rel["sentiment_score"] = 0.0
        if "weight" not in df_rel.columns:
            df_rel["weight"] = np.nan

        df_rel["date"] = pd.to_datetime(df_rel["date"], errors="coerce")
        df_rel = df_rel.dropna(subset=["date"])
        if split_date is not None:
            before = len(df_rel)
            df_rel = df_rel[df_rel["date"] < split_date].copy()
            print(f"[é˜²æ³„éœ²] å…³ç³»è¿‡æ»¤: {before} -> {len(df_rel)}")
        df_rel["date"] = df_rel["date"].dt.strftime("%Y-%m-%d")

        for row in df_rel.itertuples(index=False):
            src_c = _canonicalize_ticker(row.source_ticker, alias2canonical, ticker2idx)
            dst_c = _canonicalize_ticker(row.target_ticker, alias2canonical, ticker2idx)
            if not src_c or not dst_c or src_c == dst_c:
                continue
            if use_sp500 and (active_set != set(all_tickers)):
                if src_c not in active_set or dst_c not in active_set:
                    continue
            s, w = _normalize_sentiment_weight(getattr(row, "sentiment_score", 0.0), getattr(row, "weight", None))
            i, j = ticker2idx[src_c], ticker2idx[dst_c]
            if adj_matrix[i, j] == 0:
                edge_count += 1
            adj_matrix[i, j] = max(adj_matrix[i, j], w)
            adj_matrix[j, i] = max(adj_matrix[j, i], w)
            matched_tickers.add(src_c)
            matched_tickers.add(dst_c)

            date_key = getattr(row, "date", None)
            if use_temporal_decay and date_key:
                edge_key = (i, j) if i <= j else (j, i)
                prev = date_edge_weights[date_key].get(edge_key, 0.0)
                date_edge_weights[date_key][edge_key] = max(prev, float(w))

            rel = getattr(row, "relation_type", None)
            a, b = (src_c, dst_c) if src_c <= dst_c else (dst_c, src_c)
            edge_counter[(a, b)] += 1
            if rel:
                relation_type_counter[str(rel).strip()] += 1
    else:
        if not os.path.exists(INPUT_NEWS):
            print(f"[WARN] æœªæ‰¾åˆ°æ–°é—»æ–‡ä»¶ {INPUT_NEWS}ï¼Œä¿å­˜å•ä½é˜µã€‚")
            _atomic_save_npy(OUTPUT_GRAPH, adj_matrix)
            return

        df_news = pd.read_csv(INPUT_NEWS, low_memory=False)
        print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")

        # ç»Ÿä¸€æ–°é—»é‡Œçš„ ticker æ ¼å¼ï¼Œé¿å…åˆ†å±‚é‡‡æ ·æ—¶å› å¤§å°å†™/å†™æ³•å·®å¼‚å¯¼è‡´â€œåŒä¸€åªè‚¡ç¥¨è¢«æ‹†æˆå¤šä¸ªç»„â€
        if 'Ticker' in df_news.columns:
            df_news["Ticker"] = (
                df_news["Ticker"]
                .astype(str)
                .str.upper()
                .str.replace("-", ".", regex=False)
            )
    
        # å¦‚æœä½¿ç”¨ S&P 500 æ¨¡å¼ï¼Œè¿‡æ»¤æ–°é—»æ•°æ®
        if use_sp500 and len(active_tickers) < len(all_tickers):
            before_filter = len(df_news)
            df_news = df_news[df_news['Ticker'].isin(active_tickers)].copy()
            print(f"    [S&P 500 è¿‡æ»¤] ä¿ç•™æ–°é—»: {before_filter} -> {len(df_news)}")

        # =========================== é˜²æ­¢"æœªæ¥ä¿¡æ¯"æ•°æ®æ³„éœ²ï¼ˆæ–°é—»ï¼‰===========================
        try:
            if 'Date' in df_news.columns:
                df_news['Date'] = pd.to_datetime(df_news['Date'], errors='coerce')
                if df_news['Date'].dt.tz is not None:
                    df_news['Date'] = df_news['Date'].dt.tz_localize(None)
                if split_date is not None:
                    before_news = len(df_news)
                    df_news = df_news[df_news['Date'] < split_date].copy()
                    print(f"[é˜²æ³„éœ²] è¿‡æ»¤åä¿ç•™æ–°é—»: {before_news} -> {len(df_news)}")
        except Exception as e:
            print(f"[ERROR] æ—¶é—´è¿‡æ»¤å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…¨é‡æ–°é—»ï¼ˆå­˜åœ¨æ³„éœ²é£é™©ï¼‰ã€‚")

        # =========================== åˆ†å±‚é‡‡æ ·ï¼ˆå…³é”®ä¿®æ­£ï¼‰===========================
        df_news_sampled = stratified_sample_news(
            df_news,
            max_per_ticker=max_per_ticker,
            max_total=max_total,
        )

        if 'Date' in df_news_sampled.columns:
            df_news_sampled = df_news_sampled.sort_values('Date').reset_index(drop=True)

        # è·å–æ–‡æœ¬åˆ—
        text_col = 'Headline' if 'Headline' in df_news_sampled.columns else 'Article_title'
        if text_col not in df_news_sampled.columns:
            cols = [c for c in df_news_sampled.columns if df_news_sampled[c].dtype == object]
            text_col = cols[0] if cols else None
        
        if text_col is None:
            print("[WARN] æ²¡æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œä¿å­˜å•ä½é˜µã€‚")
            _atomic_save_npy(OUTPUT_GRAPH, np.eye(num_nodes, dtype=np.float32))
            return

        # =========================== åŠ è½½ LLM æ¨¡å‹ï¼ˆå¯é€‰ï¼‰===========================
        local_model = None
        local_tokenizer = None

        if use_llm:
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer

                print(f"\n[åŠ è½½ä¸­] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH} ...")

                device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"    è®¾å¤‡: {device}")

                local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
                # decoder-only æ¨¡å‹å¿…é¡»ä½¿ç”¨ left paddingï¼Œå¦åˆ™å¯èƒ½å½±å“ç”Ÿæˆç»“æœå¹¶äº§ç”Ÿè­¦å‘Š
                local_tokenizer.padding_side = "left"
                if getattr(local_tokenizer, "pad_token", None) is None:
                    local_tokenizer.pad_token = local_tokenizer.eos_token

                dtype = torch.float16 if device == "cuda" else torch.float32
                model_kwargs = dict(
                    device_map="auto" if device == "cuda" else None,
                    trust_remote_code=True,
                )
                # å°è¯•å¯ç”¨æ›´å¿«çš„æ³¨æ„åŠ›å®ç°ï¼ˆè‹¥ç¯å¢ƒä¸æ”¯æŒä¼šè‡ªåŠ¨å›é€€ï¼‰
                if device == "cuda":
                    model_kwargs["attn_implementation"] = "flash_attention_2"

                # å…¼å®¹ä¸åŒ transformers ç‰ˆæœ¬ï¼šä¼˜å…ˆä½¿ç”¨æ–°å‚æ•° dtype=
                try:
                    local_model = AutoModelForCausalLM.from_pretrained(
                        LOCAL_MODEL_PATH,
                        dtype=dtype,
                        **model_kwargs,
                    )
                except Exception:
                    # å›é€€ï¼šç§»é™¤å¯èƒ½ä¸è¢«æ”¯æŒçš„å‚æ•°
                    model_kwargs.pop("attn_implementation", None)
                    local_model = AutoModelForCausalLM.from_pretrained(
                        LOCAL_MODEL_PATH,
                        torch_dtype=dtype,
                        **model_kwargs,
                    )

                # åŒæ­¥ pad_token_idï¼Œé¿å… generate é˜¶æ®µçš„ padding é—®é¢˜
                try:
                    local_model.config.pad_token_id = local_tokenizer.pad_token_id
                except Exception:
                    pass
                try:
                    local_model.eval()
                except Exception:
                    pass
                print("[OK] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(">>> é™çº§ä¸ºè§„åˆ™æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
                local_model = None
        
        # =========================== å¾ªç¯å»ºå›¾ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ï¼‰===========================
        print(f"\n>>> [Step 2] å¼€å§‹å»ºå›¾ (å…± {len(df_news_sampled)} æ¡æ–°é—»)...")
        print("=" * 70)
        
        # è¿›åº¦ä¿å­˜é…ç½®
        CHECKPOINT_INTERVAL = 10000
        checkpoint_path = OUTPUT_GRAPH.replace('.npy', '_checkpoint.npz')
        sampled_path = OUTPUT_GRAPH.replace('.npy', '_news_sampled.csv')
        BATCH_SIZE = int(os.environ.get("LLM_BATCH_SIZE", str(LLM_BATCH_SIZE_DEFAULT)))
        MAX_INPUT_TOKENS = int(os.environ.get("LLM_MAX_INPUT_TOKENS", str(LLM_MAX_INPUT_TOKENS_DEFAULT)))
        MAX_NEW_TOKENS = int(os.environ.get("LLM_MAX_NEW_TOKENS", str(LLM_MAX_NEW_TOKENS_DEFAULT)))
        DO_SAMPLE = os.environ.get("LLM_DO_SAMPLE", "1" if LLM_DO_SAMPLE_DEFAULT else "0") == "1"
        
        edge_count = 0
        matched_tickers = set()
        relation_type_counter = Counter()
        edge_counter = Counter()  # (src, dst) -> count
        failures = 0
        
        # å›ºåŒ–é‡‡æ ·ç»“æœï¼Œç¡®ä¿å¯å¤ç° & å¯æ–­ç‚¹ç»­è·‘
        if os.path.exists(sampled_path):
            try:
                df_news_sampled = pd.read_csv(sampled_path, low_memory=False)
                print(f"[Resume] æ£€æµ‹åˆ°å·²ä¿å­˜çš„é‡‡æ ·æ–°é—»: {sampled_path} (n={len(df_news_sampled)})")
            except Exception as e:
                print(f"[WARN] è¯»å–é‡‡æ ·æ–°é—»å¤±è´¥ï¼Œå°†é‡æ–°é‡‡æ ·: {e}")
        else:
            try:
                df_news_sampled.to_csv(sampled_path, index=False)
                print(f"[OK] å·²ä¿å­˜é‡‡æ ·æ–°é—»ï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘/å¤ç°ï¼‰: {sampled_path}")
            except Exception as e:
                print(f"[WARN] ä¿å­˜é‡‡æ ·æ–°é—»å¤±è´¥ï¼ˆä¸å½±å“è¿è¡Œï¼Œä½†æ— æ³•ç¨³å®šæ–­ç‚¹ç»­è·‘ï¼‰: {e}")

        # æ–­ç‚¹ç»­è·‘ï¼šå¦‚æœ checkpoint å­˜åœ¨ï¼ŒåŠ è½½ adj + è¿›åº¦
        start_pos = 0
        if os.path.exists(checkpoint_path):
            ck_adj, ck_meta = _load_checkpoint_npz(checkpoint_path)
            if ck_adj is not None and ck_meta:
                # ç®€å•ä¸€è‡´æ€§æ ¡éªŒï¼šèŠ‚ç‚¹æ•°å¿…é¡»ä¸€è‡´
                if isinstance(ck_adj, np.ndarray) and ck_adj.shape == adj_matrix.shape:
                    adj_matrix = ck_adj.astype(np.float32, copy=False)
                    start_pos = int(ck_meta.get("next_pos", 0))
                    # ä¹Ÿå¯æ²¿ç”¨ä¸Šæ¬¡å·²é™è¿‡çš„ batch size
                    if "batch_size" in ck_meta:
                        try:
                            BATCH_SIZE = int(ck_meta["batch_size"])
                        except Exception:
                            pass
                    print(f"[Resume] ä» checkpoint æ¢å¤ï¼šnext_pos={start_pos}, batch_size={BATCH_SIZE}")
                else:
                    print(f"[WARN] checkpoint å½¢çŠ¶ä¸åŒ¹é…ï¼Œå¿½ç•¥æ–­ç‚¹ç»­è·‘ï¼ˆck={getattr(ck_adj,'shape',None)} vs cur={adj_matrix.shape}ï¼‰")
        
        if local_model:
            print(f"[æ‰¹å¤„ç†æ¨¡å¼] batch={BATCH_SIZE}, max_input_tokens={MAX_INPUT_TOKENS}, max_new_tokens={MAX_NEW_TOKENS}, do_sample={DO_SAMPLE}")

            # ç®€åŒ–å®ç°ï¼šæŒ‰ BATCH_SIZE åˆ†æ‰¹è°ƒç”¨ LLM æå–å…³ç³»å¹¶æ›´æ–°é‚»æ¥çŸ©é˜µ
            t0 = time.time()
            pbar = tqdm(total=len(df_news_sampled), desc="Building Graph", initial=start_pos)

            for i in range(start_pos, len(df_news_sampled), BATCH_SIZE):
                batch_df = df_news_sampled.iloc[i : i + BATCH_SIZE]
                texts = batch_df[text_col].astype(str).fillna("").tolist()
                tickers = batch_df["Ticker"].astype(str).fillna("").tolist()
                dates = []
                for d in batch_df.get("Date", pd.Series([None] * len(batch_df))):
                    if isinstance(d, pd.Timestamp):
                        dates.append(d.strftime("%Y-%m-%d"))
                    else:
                        try:
                            dates.append(pd.to_datetime(d).strftime("%Y-%m-%d"))
                        except Exception:
                            dates.append(None)

                try:
                    batch_relations = extract_relations_with_llm_batch(
                        texts,
                        local_model,
                        local_tokenizer,
                        batch_size=len(texts),
                        max_input_tokens=MAX_INPUT_TOKENS,
                        max_new_tokens=MAX_NEW_TOKENS,
                        do_sample=DO_SAMPLE,
                    )
                except Exception as e:
                    print(f"[WARN] LLM æ‰¹å¤„ç†å¤±è´¥ï¼ˆè·³è¿‡è¯¥æ‰¹æ¬¡ï¼‰: {e}")
                    batch_relations = [[] for _ in texts]

                # å¤„ç†æ¯æ¡æ–°é—»çš„è¿”å›ç»“æœ
                for src_ticker, relations, date_key in zip(tickers, batch_relations, dates):
                    if not relations:
                        continue
                    for r in relations:
                        if isinstance(r, dict):
                            src = r.get("src")
                            dst = r.get("dst")
                            rel = r.get("relation")
                            sentiment = r.get("sentiment_score", 0.0)
                        elif isinstance(r, (list, tuple)) and len(r) >= 2:
                            src, dst = r[0], r[1]
                            rel = r[2] if len(r) >= 3 else None
                            sentiment = r[3] if len(r) >= 4 else 0.0
                        else:
                            continue

                        src_c = _canonicalize_ticker(src, alias2canonical, ticker2idx)
                        dst_c = _canonicalize_ticker(dst, alias2canonical, ticker2idx)
                        if not src_c or not dst_c or src_c == dst_c:
                            continue

                        if use_sp500 and (active_set != set(all_tickers)):
                            if src_c not in active_set or dst_c not in active_set:
                                continue

                        i_idx, j_idx = ticker2idx[src_c], ticker2idx[dst_c]

                        try:
                            sentiment_weight = abs(float(sentiment))
                            if sentiment_weight == 0.0 or np.isnan(sentiment_weight):
                                sentiment_weight = 0.5
                        except Exception:
                            sentiment_weight = 0.5

                        if adj_matrix[i_idx, j_idx] == 0:
                            edge_count += 1
                        adj_matrix[i_idx, j_idx] = max(adj_matrix[i_idx, j_idx], sentiment_weight)
                        adj_matrix[j_idx, i_idx] = adj_matrix[i_idx, j_idx]

                        if use_temporal_decay and date_key:
                            edge_key = (i_idx, j_idx) if i_idx <= j_idx else (j_idx, i_idx)
                            prev = date_edge_weights[date_key].get(edge_key, 0.0)
                            date_edge_weights[date_key][edge_key] = max(prev, float(sentiment_weight))

                        a, b = (src_c, dst_c) if src_c <= dst_c else (dst_c, src_c)
                        edge_counter[(a, b)] += 1
                        if rel:
                            relation_type_counter[str(rel).strip()] += 1

                # å®šæœŸä¿å­˜ checkpointï¼Œä¾¿äºæ–­ç‚¹ç»­è·‘
                if (i + BATCH_SIZE) % CHECKPOINT_INTERVAL == 0:
                    meta = {
                        "next_pos": min(i + BATCH_SIZE, len(df_news_sampled)),
                        "batch_size": BATCH_SIZE,
                        "max_input_tokens": MAX_INPUT_TOKENS,
                        "max_new_tokens": MAX_NEW_TOKENS,
                        "do_sample": bool(DO_SAMPLE),
                        "use_sp500": bool(use_sp500),
                        "num_nodes": int(num_nodes),
                        "active_tickers": sorted(list(active_set)) if (use_sp500 and active_set != set(all_tickers)) else None,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    }
                    _atomic_save_checkpoint_npz(checkpoint_path, adj_matrix, meta)

                pbar.update(len(texts))

            try:
                pbar.close()
            except Exception:
                pass
        # è§„åˆ™æ¨¡å¼ï¼ˆä¸å˜ï¼‰
        start_pos = 0
        if os.path.exists(checkpoint_path):
            ck_adj, ck_meta = _load_checkpoint_npz(checkpoint_path)
            if ck_adj is not None and ck_meta and isinstance(ck_adj, np.ndarray) and ck_adj.shape == adj_matrix.shape:
                adj_matrix = ck_adj.astype(np.float32, copy=False)
                start_pos = int(ck_meta.get("next_pos", 0))
                print(f"[Resume] (è§„åˆ™æ¨¡å¼) ä» checkpoint æ¢å¤ï¼šnext_pos={start_pos}")

        for pos in tqdm(range(start_pos, len(df_news_sampled)), total=len(df_news_sampled), initial=start_pos, desc="Building Graph"):
            row = df_news_sampled.iloc[pos]
            src_ticker = str(row.get('Ticker', '')).strip().upper()

            content = row.get(text_col, "")
            date_key = row.get('Date')
            if isinstance(date_key, pd.Timestamp):
                date_key = date_key.strftime("%Y-%m-%d")
            date_key = str(date_key) if date_key is not None else None

            ok = True
            if not src_ticker:
                ok = False
            elif use_sp500 and (active_set != set(all_tickers)) and (src_ticker not in active_set):
                ok = False
            elif src_ticker not in ticker2idx:
                ok = False
            elif not content or (isinstance(content, float) and pd.isna(content)):
                ok = False
        
            if ok:
                content = str(content)
                # è§„åˆ™åŒ¹é…
                for t in active_tickers:
                    if t != src_ticker and len(str(t)) >= 3 and str(t).upper() in content.upper():
                        if use_sp500 and (active_set != set(all_tickers)) and (t not in active_set):
                            continue
                        if t in ticker2idx:
                            i, j = ticker2idx[src_ticker], ticker2idx[t]
                            if adj_matrix[i, j] == 0:
                                edge_count += 1
                            adj_matrix[i, j] = 1.0
                            adj_matrix[j, i] = 1.0
                            matched_tickers.add(src_ticker)
                            matched_tickers.add(t)
                            if use_temporal_decay and date_key:
                                edge_key = (i, j) if i <= j else (j, i)
                                date_edge_weights[date_key][edge_key] = 1.0

            if (pos + 1) % CHECKPOINT_INTERVAL == 0:
                meta = {
                    "next_pos": pos + 1,
                    "batch_size": None,
                    "use_sp500": bool(use_sp500),
                    "num_nodes": int(num_nodes),
                    "active_tickers": sorted(list(active_set)) if (use_sp500 and active_set != set(all_tickers)) else None,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                }
                _atomic_save_checkpoint_npz(checkpoint_path, adj_matrix, meta)
                print(f"\n[è¿›åº¦ä¿å­˜] å·²å¤„ç† {pos+1}/{len(df_news_sampled)} æ¡ (è¾¹æ•°: {int((adj_matrix.sum()-num_nodes)/2)})")
        
        # =========================== æ—¶é—´è¡°å‡ç´¯ç§¯ï¼ˆè¯­ä¹‰å›¾ï¼‰===========================
    if use_temporal_decay and date_edge_weights:
        print("\n>>> [Step 2.5] åº”ç”¨æ—¶é—´è¡°å‡ç´¯ç§¯ï¼ˆè¯­ä¹‰å›¾ï¼‰...")
        adj_matrix = np.eye(num_nodes, dtype=np.float32)
        for date_key in sorted(date_edge_weights.keys()):
            adj_matrix *= TEMPORAL_DECAY_ALPHA
            edges = date_edge_weights[date_key]
            for (i, j), w in edges.items():
                adj_matrix[i, j] = adj_matrix[i, j] + (1.0 - TEMPORAL_DECAY_ALPHA) * float(w)
                adj_matrix[j, i] = adj_matrix[i, j]
        np.fill_diagonal(adj_matrix, 1.0)
        print("    âœ… æ—¶é—´è¡°å‡ç´¯ç§¯å®Œæˆ")

    # =========================== æ„å»ºç»Ÿè®¡ç›¸å…³æ€§å›¾ï¼ˆéšå¼å±‚ï¼‰===========================
    print("\n>>> [Step 3] æ„å»ºç»Ÿè®¡ç›¸å…³æ€§å›¾ï¼ˆéšå¼å±‚ï¼‰...")
    try:
        # è¯»å–è‚¡ä»·æ•°æ®ç”¨äºè®¡ç®—æ”¶ç›Šç‡ç›¸å…³æ€§
        df_price_for_stat = pd.read_csv(INPUT_MODEL_DATA, usecols=['Date', 'Ticker', 'Close'])
        df_price_for_stat['Date'] = pd.to_datetime(df_price_for_stat['Date'])
        df_price_for_stat = df_price_for_stat.sort_values(['Ticker', 'Date']).reset_index(drop=True)
        
        # åªä¿ç•™å›¾ä¸­å­˜åœ¨çš„è‚¡ç¥¨
        df_price_for_stat = df_price_for_stat[df_price_for_stat['Ticker'].isin(graph_tickers)].copy()
        
        # æ„å»ºç»Ÿè®¡ç›¸å…³æ€§å›¾
        adj_stat = build_statistical_correlation_graph(
            df_price_for_stat, 
            ticker2idx, 
            window=STAT_CORR_WINDOW, 
            threshold=STAT_CORR_THRESHOLD
        )
    except Exception as e:
        print(f"    âš ï¸ ç»Ÿè®¡å›¾æ„å»ºå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é›¶çŸ©é˜µ")
        adj_stat = np.zeros((num_nodes, num_nodes), dtype=np.float32)
    
    # =========================== æ—¶é—´è¡°å‡ç´¯ç§¯ä¸æ··åˆå›¾æ„å»ºï¼ˆæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼‰===========================
    print("\n>>> [Step 4] æ—¶é—´è¡°å‡ç´¯ç§¯ä¸æ··åˆå›¾æ„å»º...")
    
    # ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘æ—¶é—´è¡°å‡ç´¯ç§¯å…¬å¼ï¼šA_t^{semantic} = Î± Â· A_{t-1}^{semantic} + (1-Î±) Â· (E_t âŠ™ S_t)
    # æ³¨æ„ï¼šå½“å‰å®ç°æ˜¯ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ–°é—»ï¼Œå› æ­¤è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ç´¯ç§¯æ–¹å¼
    # å¦‚æœæŒ‰æ—¥æœŸå¤„ç†ï¼Œåº”è¯¥é€æ—¥ç´¯ç§¯ï¼›å½“å‰å®ç°å°†æ‰€æœ‰æ–°é—»çš„å…³ç³»ç´¯ç§¯åˆ°æœ€ç»ˆå›¾ä¸­
    
    # å½“å‰ adj_matrix å·²ç»æ˜¯ç´¯ç§¯åçš„è¯­ä¹‰å›¾ï¼ˆåŒ…å«æƒ…æ„ŸåŠ æƒï¼‰
    # ä¸ºäº†ä½“ç°æ—¶é—´è¡°å‡ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹è¯­ä¹‰å›¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    adj_semantic = adj_matrix.copy()
    
    # å¦‚æœè¯­ä¹‰å›¾ä¸­æœ‰æƒ…æ„Ÿåˆ†æ•°ä¿¡æ¯ï¼ˆåœ¨è¾¹æƒé‡ä¸­ï¼‰ï¼Œè¿™é‡Œåº”è¯¥å·²ç»ä½“ç°
    # å½“å‰å®ç°ä¸­ï¼Œadj_matrix æ˜¯äºŒå€¼çŸ©é˜µï¼ˆ0æˆ–1ï¼‰ï¼Œæƒ…æ„Ÿåˆ†æ•°ä¿¡æ¯åœ¨å…³ç³»æå–æ—¶å·²è€ƒè™‘
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥å°†æƒ…æ„Ÿåˆ†æ•°ä½œä¸ºè¾¹æƒé‡ï¼šadj_semantic[i, j] = sentiment_score
    
    # å½’ä¸€åŒ–è¯­ä¹‰å›¾ï¼ˆé¿å…æ•°å€¼è¿‡å¤§ï¼‰
    if adj_semantic.max() > 0:
        adj_semantic = adj_semantic / adj_semantic.max()

    # æ¶ˆèç”¨ï¼šå»é™¤æƒ…æ„Ÿæƒé‡ï¼ˆä»…ä¿ç•™å…³ç³»è¾¹ï¼‰
    adj_semantic_nosent = (adj_matrix > 0).astype(np.float32)
    np.fill_diagonal(adj_semantic_nosent, 1.0)
    
    # ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘æ··åˆå›¾æ„å»ºï¼šA_t^{final} = Norm(A_t^{semantic} + Î» Â· A_t^{stat})
    # å…¶ä¸­ Î» æ˜¯ç»Ÿè®¡å›¾çš„æƒé‡ï¼Œç”¨äºå¹³è¡¡è¯­ä¹‰å›¾å’Œç»Ÿè®¡å›¾
    print(f"    æ··åˆå›¾å‚æ•°ï¼šÎ» = {HYBRID_LAMBDA}ï¼ˆç»Ÿè®¡å›¾æƒé‡ï¼‰")
    # ç»Ÿè®¡å›¾è¡¥è‡ªç¯ï¼Œä¾¿äºä¸è¯­ä¹‰å›¾ä¸€è‡´
    np.fill_diagonal(adj_stat, 1.0)
    adj_final = adj_semantic + HYBRID_LAMBDA * adj_stat
    
    # å½’ä¸€åŒ–æœ€ç»ˆå›¾ï¼ˆç¡®ä¿æ•°å€¼èŒƒå›´åˆç†ï¼‰
    if adj_final.max() > 0:
        adj_final = adj_final / adj_final.max()
    
    # ä¿ç•™è‡ªç¯ï¼ˆå•ä½é˜µï¼‰
    np.fill_diagonal(adj_final, 1.0)
    
    # ç»Ÿè®¡ä¿¡æ¯
    semantic_edges = int((adj_semantic.sum() - num_nodes) / 2)
    stat_edges = int((adj_stat.sum() - num_nodes) / 2)
    final_edges = int((adj_final.sum() - num_nodes) / 2)
    
    print(f"    è¯­ä¹‰å›¾è¾¹æ•°: {semantic_edges}")
    print(f"    ç»Ÿè®¡å›¾è¾¹æ•°: {stat_edges}")
    print(f"    æ··åˆå›¾è¾¹æ•°: {final_edges}")
    print(f"    âœ… æ··åˆå›¾æ„å»ºå®Œæˆ")
    
    # =========================== ä¿å­˜æœ€ç»ˆç»“æœ ===========================
    print("\n>>> [Step 5] ä¿å­˜æœ€ç»ˆç»“æœ...")
    _atomic_save_npy(OUTPUT_GRAPH, adj_final)

    # é¢å¤–è¾“å‡ºï¼šæ¶ˆèæ‰€éœ€çš„è¯­ä¹‰/ç»Ÿè®¡å›¾
    try:
        _atomic_save_npy(OUTPUT_GRAPH.replace(".npy", "_semantic.npy"), adj_semantic)
        _atomic_save_npy(OUTPUT_GRAPH.replace(".npy", "_stat.npy"), adj_stat)
        _atomic_save_npy(OUTPUT_GRAPH.replace(".npy", "_semantic_nosent.npy"), adj_semantic_nosent)
    except Exception as e:
        print(f"[WARN] ä¿å­˜æ¶ˆèå›¾å¤±è´¥: {e}")
    
    # åˆ é™¤checkpointæ–‡ä»¶
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
        print(f"[æ¸…ç†] å·²åˆ é™¤ä¸´æ—¶checkpointæ–‡ä»¶")
    # é‡‡æ ·æ–‡ä»¶ä¿ç•™ï¼ˆä¾¿äºå¤ç°/å®¡è®¡ï¼‰ï¼›å¦‚éœ€èŠ‚çœç©ºé—´å¯æ‰‹åŠ¨åˆ é™¤

    # ä¿å­˜å…³ç³»ç±»å‹ç»Ÿè®¡ï¼ˆLLMæ¨¡å¼ä¸‹æ›´æœ‰è®ºæ–‡ä»·å€¼ï¼›è§„åˆ™æ¨¡å¼å¯èƒ½ä¸ºç©ºï¼‰
    try:
        stats_path = OUTPUT_GRAPH.replace(".npy", "_relation_stats.json")
        _atomic_save_json(stats_path, {
            "relation_type_counts": dict(relation_type_counter),
            "top_edges": [
                {"src": k[0], "dst": k[1], "count": int(v)}
                for k, v in edge_counter.most_common(200)
            ],
        })
    except Exception:
        pass
    
    # =========================== è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ ===========================
    print("\n" + "=" * 70)
    print(">>> å›¾è°±ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)
    
    # è®¡ç®—å›¾è°±ç»Ÿè®¡
    total_edges = (adj_matrix.sum() - num_nodes) / 2  # å‡å»è‡ªç¯ï¼Œé™¤ä»¥2ï¼ˆæ— å‘å›¾ï¼‰
    density = total_edges / (num_nodes * (num_nodes - 1) / 2) if num_nodes > 1 else 0
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„åº¦
    degrees = adj_matrix.sum(axis=1) - 1  # å‡å»è‡ªç¯
    non_isolated = np.sum(degrees > 0)
    
    print(f"    èŠ‚ç‚¹æ•° (è‚¡ç¥¨æ•°): {num_nodes}")
    print(f"    è¾¹æ•° (è‚¡ç¥¨å…³ç³»): {int(total_edges)}")
    print(f"    å›¾å¯†åº¦: {density:.6f}")
    print(f"    æœ‰è¿æ¥çš„è‚¡ç¥¨æ•°: {non_isolated} / {num_nodes} ({non_isolated/num_nodes*100:.1f}%)")
    print(f"    å¹³å‡åº¦: {degrees.mean():.2f}")
    print(f"    æœ€å¤§åº¦: {int(degrees.max())}")
    print(f"    å­¤ç«‹èŠ‚ç‚¹æ•°: {num_nodes - non_isolated}")
    
    if non_isolated < num_nodes * 0.5:
        print("\nâš ï¸ è­¦å‘Šï¼šè¶…è¿‡ä¸€åŠçš„è‚¡ç¥¨æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼")
        print("   å»ºè®®ï¼š")
        print("   1. å¢åŠ  max_per_ticker å‚æ•°")
        print("   2. ä½¿ç”¨ LLM æ¨¡å¼ (use_llm=True) æå–æ›´å¤šå…³ç³»")
        print("   3. æ£€æŸ¥æ–°é—»æ•°æ®è´¨é‡")
    
    print(f"\n[OK] å·²ä¿å­˜è‡³ {OUTPUT_GRAPH}ï¼Œå½¢çŠ¶: {adj_matrix.shape}")
    print("=" * 70)
    
    return adj_matrix


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºè‚¡ç¥¨å…³ç³»å›¾è°±')
    # å…¼å®¹æ—§å‚æ•°ï¼š--use_llm / --no_llmï¼ŒåŒæ—¶æä¾›æ›´æ¸…æ™°çš„ --llm/--no-llm
    parser.add_argument('--use_llm', action='store_true', help='(å…¼å®¹) å¼ºåˆ¶å¯ç”¨LLM')
    parser.add_argument('--no_llm', action='store_true', help='(å…¼å®¹) å¼ºåˆ¶ç¦ç”¨LLMï¼ˆè§„åˆ™åŒ¹é…ï¼‰')
    parser.add_argument('--llm', action=argparse.BooleanOptionalAction, default=USE_LLM_DEFAULT,
                        help=f'æ˜¯å¦ä½¿ç”¨LLMï¼ˆé»˜è®¤: {USE_LLM_DEFAULT}ï¼‰')
    parser.add_argument('--max_per_ticker', type=int, default=MAX_NEWS_PER_TICKER, help='æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--max_total', type=int, default=MAX_TOTAL_NEWS, help='æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--all_stocks', action='store_true', help='ä½¿ç”¨å…¨é‡è‚¡ç¥¨ï¼ˆé»˜è®¤åªç”¨ S&P 500ï¼‰')
    parser.add_argument('--split_date', type=str, default='2020-12-31', 
                        help='å›¾è°±æ„å»ºæˆªæ­¢æ—¥æœŸï¼ˆå¿…é¡»ä¸è®­ç»ƒé›†ç»“æŸæ—¥æœŸä¸¥æ ¼ä¸€è‡´ï¼Œé˜²æ³„éœ²ï¼‰')
    
    args = parser.parse_args()
    
    # LLMå¼€å…³ï¼šé»˜è®¤å– --llm çš„å€¼ï¼Œä½†æ—§å‚æ•°å¯è¦†ç›–
    use_llm_mode = bool(args.llm)
    if args.no_llm:
        use_llm_mode = False
    if args.use_llm:
        use_llm_mode = True
    
    print("\n" + "=" * 70)
    print("ğŸ“Š è‚¡ç¥¨å…³ç³»å›¾è°±æ„å»ºå·¥å…·")
    print("=" * 70)
    print(f"é…ç½®:")
    print(f"  - è‚¡ç¥¨èŒƒå›´: {'å…¨é‡' if args.all_stocks else 'S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èï¼‰'}")
    print(f"  - å…³ç³»æå–: {'ğŸ§  LLMè¯­ä¹‰æå– (Qwen2.5-14B)' if use_llm_mode else 'ğŸ“‹ è§„åˆ™åŒ¹é…'}")
    print(f"  - æ¯è‚¡ç¥¨é‡‡æ ·: {args.max_per_ticker} æ¡æ–°é—»")
    print(f"  - æ€»é‡‡æ ·ä¸Šé™: {args.max_total} æ¡")
    if use_llm_mode:
        bs = int(os.environ.get("LLM_BATCH_SIZE", str(LLM_BATCH_SIZE_DEFAULT)))
        mi = int(os.environ.get("LLM_MAX_INPUT_TOKENS", str(LLM_MAX_INPUT_TOKENS_DEFAULT)))
        mn = int(os.environ.get("LLM_MAX_NEW_TOKENS", str(LLM_MAX_NEW_TOKENS_DEFAULT)))
        ds = os.environ.get("LLM_DO_SAMPLE", "1" if LLM_DO_SAMPLE_DEFAULT else "0")
        print(f"  - æ‰¹å¤„ç†å¤§å°: {bs} æ¡/æ‰¹ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡ LLM_BATCH_SIZE è°ƒæ•´ï¼‰")
        print(f"  - æ¨ç†å‚æ•°: max_input_tokens={mi}, max_new_tokens={mn}, do_sample={ds}")
    print("=" * 70 + "\n")
    
    build_dynamic_graph(
        use_llm=use_llm_mode, 
        max_per_ticker=args.max_per_ticker,
        max_total=args.max_total,
        use_sp500=not args.all_stocks  # é»˜è®¤ä½¿ç”¨ S&P 500
        ,split_date=args.split_date
    )
