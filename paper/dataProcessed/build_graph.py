# -*- coding: utf-8 -*-
"""
LLM åŠ¨æ€å›¾è°±æ„å»º (ä¿®æ­£ç‰ˆ V3ï¼šæ”¯æŒ S&P 500 æ ¸å¿ƒè‚¡ç¥¨è¿‡æ»¤)
========================================================================
æ ¸å¿ƒä¿®æ­£ï¼š
1. [å…³é”®] æ”¯æŒåªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
2. [å…³é”®] ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
3. æ‰“ä¹±æ–°é—»é¡ºåºï¼Œé¿å…åªå¤„ç†æ’åºé å‰çš„è‚¡ç¥¨
4. æ·»åŠ å›¾è°±ç»Ÿè®¡ä¿¡æ¯è¾“å‡º

è®ºæ–‡å»ºè®®ï¼š
- ä½¿ç”¨ S&P 500 æˆåˆ†è‚¡æ˜¯é‡‘è/é‡åŒ–ç ”ç©¶çš„å­¦æœ¯æƒ¯ä¾‹
- å¤§å…¬å¸æ–°é—»è´¨é‡é«˜ï¼Œå…³ç³»æ›´æ˜ç¡®ï¼Œå›¾è°±æ›´æœ‰æ„ä¹‰
"""

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import torch

# ================= è·¯å¾„é…ç½® =================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DATA_PROCESSED = os.path.join(PROJECT_ROOT, 'data', 'processed')

INPUT_NEWS = os.path.join(DATA_PROCESSED, 'Stock_News.csv')
INPUT_MODEL_DATA = os.path.join(DATA_PROCESSED, 'Final_Model_Data.csv')
OUTPUT_GRAPH = os.path.join(DATA_PROCESSED, 'Graph_Adjacency.npy')

# LLM é…ç½®
USE_LOCAL_MODEL = True
LOCAL_MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"
LOCAL_MODEL_PATH = os.environ.get(
    "LOCAL_MODEL_PATH", 
    "/root/autodl-tmp/models/qwen/Qwen2.5-14B-Instruct" 
)

# ================= é‡‡æ ·é…ç½®ï¼ˆ48GBæ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰=================
# æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
MAX_NEWS_PER_TICKER = 100  # 48GBæ˜¾å­˜å……è¶³ï¼Œå¢åŠ é‡‡æ ·ä»¥æé«˜å›¾è°±è´¨é‡

# æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
MAX_TOTAL_NEWS = 100000  # å¢åŠ æ€»é‡ä»¥è·å¾—æ›´ä¸°å¯Œçš„å…³ç³»

# æ˜¯å¦ä½¿ç”¨ LLMï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
# 48GBæ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œå¯ç”¨LLMä»¥è·å¾—æ›´å‡†ç¡®çš„å…³ç³»æå–
USE_LLM_DEFAULT = True

# ================= S&P 500 æˆåˆ†è‚¡ï¼ˆ2023å¹´ç‰ˆæœ¬ï¼Œçº¦500åªï¼‰=================
# è¿™æ˜¯å­¦æœ¯ç ”ç©¶ä¸­å¸¸ç”¨çš„æ ¸å¿ƒè‚¡ç¥¨åˆ—è¡¨
# æ•°æ®æ¥æºï¼šWikipedia / Yahoo Finance
SP500_TICKERS = {
    # ä¿¡æ¯æŠ€æœ¯ (Information Technology)
    'AAPL', 'MSFT', 'NVDA', 'AVGO', 'CSCO', 'ADBE', 'CRM', 'ORCL', 'ACN', 'IBM',
    'INTC', 'AMD', 'QCOM', 'TXN', 'AMAT', 'MU', 'LRCX', 'ADI', 'KLAC', 'SNPS',
    'CDNS', 'MCHP', 'NXPI', 'MPWR', 'FTNT', 'PANW', 'NOW', 'INTU', 'ADSK', 'ANSS',
    'PYPL', 'FIS', 'FISV', 'GPN', 'ADP', 'PAYX', 'CTSH', 'IT', 'EPAM', 'AKAM',
    
    # åŒ»ç–—ä¿å¥ (Health Care)
    'UNH', 'JNJ', 'LLY', 'PFE', 'ABBV', 'MRK', 'TMO', 'ABT', 'DHR', 'BMY',
    'AMGN', 'GILD', 'VRTX', 'REGN', 'ISRG', 'MDT', 'SYK', 'BDX', 'BSX', 'EW',
    'ZBH', 'IDXX', 'DXCM', 'ALGN', 'HOLX', 'MTD', 'IQV', 'CI', 'ELV', 'HUM',
    'CVS', 'MCK', 'CAH', 'ABC', 'CNC', 'MOH', 'HCA', 'UHS', 'DVA', 'LH',
    'DGX', 'A', 'WAT', 'PKI', 'BIO', 'TECH', 'HSIC', 'COO', 'RMD', 'BAX',
    
    # é‡‘è (Financials)
    'BRK.B', 'JPM', 'V', 'MA', 'BAC', 'WFC', 'GS', 'MS', 'SCHW', 'AXP',
    'BLK', 'SPGI', 'C', 'PNC', 'USB', 'TFC', 'CME', 'ICE', 'CB', 'MMC',
    'AON', 'PGR', 'AIG', 'MET', 'PRU', 'AFL', 'ALL', 'TRV', 'CINF', 'HIG',
    'AJG', 'WTW', 'BRO', 'RE', 'L', 'GL', 'COF', 'DFS', 'SYF', 'ALLY',
    'MTB', 'FITB', 'HBAN', 'KEY', 'RF', 'CFG', 'ZION', 'NTRS', 'STT', 'BK',
    
    # é€šä¿¡æœåŠ¡ (Communication Services)
    'GOOGL', 'GOOG', 'META', 'NFLX', 'DIS', 'CMCSA', 'VZ', 'T', 'TMUS', 'CHTR',
    'ATVI', 'EA', 'TTWO', 'WBD', 'PARA', 'FOX', 'FOXA', 'NWS', 'NWSA', 'OMC',
    'IPG', 'LYV', 'MTCH', 'ZG', 'PINS',
    
    # æ¶ˆè´¹å“ (Consumer Discretionary)
    'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'LOW', 'SBUX', 'TJX', 'BKNG', 'MAR',
    'HLT', 'CMG', 'ORLY', 'AZO', 'ROST', 'DHI', 'LEN', 'PHM', 'NVR', 'GM',
    'F', 'APTV', 'BWA', 'LEA', 'RL', 'TPR', 'VFC', 'PVH', 'HAS', 'MAT',
    'DRI', 'YUM', 'WYNN', 'MGM', 'CZR', 'RCL', 'CCL', 'NCLH', 'LVS', 'EXPE',
    'ABNB', 'UBER', 'LYFT', 'DASH', 'EBAY', 'ETSY', 'W', 'BBY', 'KMX', 'AN',
    
    # å¿…éœ€æ¶ˆè´¹å“ (Consumer Staples)
    'PG', 'KO', 'PEP', 'COST', 'WMT', 'PM', 'MO', 'MDLZ', 'CL', 'EL',
    'KMB', 'GIS', 'K', 'HSY', 'HRL', 'SJM', 'MKC', 'CAG', 'CPB', 'TSN',
    'KHC', 'STZ', 'BF.B', 'TAP', 'KDP', 'MNST', 'WBA', 'SYY', 'KR', 'TGT',
    'DG', 'DLTR', 'CLX', 'CHD', 'COR',
    
    # å·¥ä¸š (Industrials)
    'UNP', 'UPS', 'HON', 'BA', 'RTX', 'CAT', 'DE', 'LMT', 'GE', 'MMM',
    'GD', 'NOC', 'LHX', 'TDG', 'ITW', 'EMR', 'ROK', 'PH', 'ETN', 'PCAR',
    'CTAS', 'FAST', 'WM', 'RSG', 'WCN', 'VRSK', 'CPRT', 'CSX', 'NSC', 'FDX',
    'EXPD', 'CHRW', 'JBHT', 'DAL', 'UAL', 'LUV', 'AAL', 'ALK', 'CARR', 'OTIS',
    'JCI', 'TT', 'IR', 'SWK', 'MAS', 'GNRC', 'PWR', 'AME', 'DOV', 'ROP',
    
    # èƒ½æº (Energy)
    'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'MPC', 'PSX', 'VLO', 'PXD', 'OXY',
    'HES', 'DVN', 'FANG', 'HAL', 'BKR', 'KMI', 'WMB', 'OKE', 'TRGP', 'APA',
    'MRO', 'CTRA',
    
    # ææ–™ (Materials)
    'LIN', 'APD', 'SHW', 'ECL', 'DD', 'DOW', 'NEM', 'FCX', 'NUE', 'VMC',
    'MLM', 'PPG', 'ALB', 'EMN', 'CE', 'CF', 'MOS', 'FMC', 'IFF', 'CTVA',
    'LYB', 'IP', 'PKG', 'SEE', 'AVY', 'BALL', 'AMCR',
    
    # æˆ¿åœ°äº§ (Real Estate)
    'PLD', 'AMT', 'CCI', 'EQIX', 'PSA', 'SPG', 'O', 'WELL', 'DLR', 'AVB',
    'EQR', 'VTR', 'ARE', 'MAA', 'UDR', 'ESS', 'HST', 'PEAK', 'KIM', 'REG',
    'FRT', 'BXP', 'VNO', 'SLG', 'CBRE', 'IRM', 'WY', 'SBAC', 'INVH', 'CPT',
    
    # å…¬ç”¨äº‹ä¸š (Utilities)
    'NEE', 'DUK', 'SO', 'D', 'AEP', 'SRE', 'EXC', 'XEL', 'ED', 'PEG',
    'WEC', 'ES', 'AWK', 'DTE', 'EIX', 'ETR', 'FE', 'PPL', 'AEE', 'CMS',
    'CNP', 'EVRG', 'ATO', 'NI', 'LNT', 'PNW', 'NRG', 'CEG',
}

# æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆå¼ºçƒˆæ¨èç”¨äºè®ºæ–‡ï¼‰
USE_SP500_ONLY = True


def extract_relations_with_llm(news_text, client=None, local_model=None, local_tokenizer=None):
    """åˆ©ç”¨ LLM æå–å…³ç³»"""
    if not news_text or (isinstance(news_text, float) and pd.isna(news_text)):
        return []
    
    text = str(news_text)[:500]

    prompt = f"""
è¯·ä»ä»¥ä¸‹è´¢ç»æ–°é—»ä¸­æå–å…¬å¸ä¹‹é—´çš„æ˜¾å¼å…³ç³»ï¼ˆå¦‚ï¼šä¾›åº”ã€ç«äº‰ã€åˆä½œã€æ¯å­å…¬å¸ã€è¯‰è®¼ï¼‰ã€‚
æ–°é—»å†…å®¹ï¼š{text}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›åˆ—è¡¨ï¼Œä¸è¦åŒ…å«å…¶ä»–åºŸè¯ï¼š
[{{"src": "å…¬å¸Aè‚¡ç¥¨ä»£ç ", "dst": "å…¬å¸Bè‚¡ç¥¨ä»£ç ", "relation": "å…³ç³»ç±»å‹"}}]
å¦‚æœæ— æ˜ç¡®å…³ç³»ï¼Œè¿”å› []ã€‚
"""

    try:
        if local_model is not None and local_tokenizer is not None:
            messages = [{"role": "user", "content": prompt}]
            text_input = local_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            device = local_model.device
            model_inputs = local_tokenizer([text_input], return_tensors="pt").to(device)
            
            with torch.no_grad():
                generated_ids = local_model.generate(
                    **model_inputs,
                    max_new_tokens=512,
                    temperature=0.1,
                    do_sample=True,
                )
            
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            raw = local_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
        elif client is not None:
            return []
        else:
            return []
        
        import json
        if "```" in raw:
            raw = raw.split("```")[1]
            if raw.startswith("json"):
                raw = raw[4:]
        return json.loads(raw)
        
    except Exception as e:
        return []


def stratified_sample_news(df_news, max_per_ticker=20, max_total=50000, random_state=42):
    """
    åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿æ¯ä¸ªè‚¡ç¥¨éƒ½æœ‰ä»£è¡¨æ€§çš„æ–°é—»
    
    å‚æ•°:
        df_news: æ–°é—» DataFrame
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        max_total: æ€»å…±æœ€å¤šé‡‡æ ·å¤šå°‘æ¡
        random_state: éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
    
    è¿”å›:
        é‡‡æ ·åçš„ DataFrame
    """
    print(f">>> å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")
    print(f"    æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·: {max_per_ticker} æ¡")
    print(f"    æ€»é‡‡æ ·ä¸Šé™: {max_total} æ¡")
    
    # æŒ‰ Ticker åˆ†ç»„é‡‡æ ·
    sampled_dfs = []
    ticker_counts = df_news['Ticker'].value_counts()
    
    for ticker in ticker_counts.index:
        ticker_news = df_news[df_news['Ticker'] == ticker]
        n_sample = min(len(ticker_news), max_per_ticker)
        sampled = ticker_news.sample(n=n_sample, random_state=random_state)
        sampled_dfs.append(sampled)
    
    # åˆå¹¶æ‰€æœ‰é‡‡æ ·ç»“æœ
    df_sampled = pd.concat(sampled_dfs, ignore_index=True)
    
    # å¦‚æœæ€»æ•°è¶…è¿‡ä¸Šé™ï¼Œå†éšæœºé‡‡æ ·
    if len(df_sampled) > max_total:
        df_sampled = df_sampled.sample(n=max_total, random_state=random_state)
    
    # æ‰“ä¹±é¡ºåº
    df_sampled = df_sampled.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    print(f"    é‡‡æ ·åæ–°é—»æ€»æ•°: {len(df_sampled)}")
    print(f"    è¦†ç›–è‚¡ç¥¨æ•°: {df_sampled['Ticker'].nunique()}")
    
    return df_sampled


def build_dynamic_graph(use_llm=USE_LLM_DEFAULT, max_per_ticker=MAX_NEWS_PER_TICKER, max_total=MAX_TOTAL_NEWS, use_sp500=USE_SP500_ONLY):
    """
    æ„å»ºåŠ¨æ€å›¾è°±
    
    å‚æ•°:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æå–å…³ç³»ï¼ˆFalse åˆ™ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰
        max_per_ticker: æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»
        max_total: æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»
        use_sp500: æ˜¯å¦åªä½¿ç”¨ S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰
    """
    print("=" * 70)
    print(">>> [Step 1] è¯»å–æ¨¡å‹æ•°æ®ä¸æ–°é—»...")
    print("=" * 70)

    if not os.path.exists(INPUT_MODEL_DATA):
        print(f"[ERROR] æœªæ‰¾åˆ° {INPUT_MODEL_DATA}")
        return

    df_price = pd.read_csv(INPUT_MODEL_DATA)
    all_tickers = sorted(df_price['Ticker'].unique())
    print(f"    åŸå§‹æ•°æ®æ£€æµ‹åˆ° {len(all_tickers)} åªè‚¡ç¥¨ã€‚")
    
    # =============== S&P 500 è¿‡æ»¤ï¼ˆæ¨èç”¨äºè®ºæ–‡ï¼‰===============
    if use_sp500:
        # æ‰¾å‡ºæ•°æ®ä¸­å­˜åœ¨çš„ S&P 500 æˆåˆ†è‚¡
        sp500_in_data = [t for t in all_tickers if t in SP500_TICKERS]
        print(f"\nğŸ“Œ [S&P 500 æ¨¡å¼] åªä½¿ç”¨æ ¸å¿ƒæˆåˆ†è‚¡")
        print(f"    S&P 500 æˆåˆ†è‚¡å®šä¹‰: {len(SP500_TICKERS)} åª")
        print(f"    æ•°æ®ä¸­åŒ¹é…åˆ°: {len(sp500_in_data)} åª")
        
        if len(sp500_in_data) < 100:
            print(f"âš ï¸ è­¦å‘Šï¼šåŒ¹é…åˆ°çš„ S&P 500 æˆåˆ†è‚¡è¾ƒå°‘ ({len(sp500_in_data)} åª)")
            print("    å¯èƒ½åŸå› ï¼šæ•°æ®é›†ä¸­çš„è‚¡ç¥¨ä»£ç æ ¼å¼ä¸åŒï¼Œæˆ–æ•°æ®é›†ä¸åŒ…å«è¿™äº›è‚¡ç¥¨")
            print("    å°†ä½¿ç”¨å…¨é‡è‚¡ç¥¨...")
            tickers = all_tickers
        else:
            tickers = sp500_in_data
            # è¿‡æ»¤ä»·æ ¼æ•°æ®ï¼Œåªä¿ç•™ S&P 500 æˆåˆ†è‚¡
            df_price = df_price[df_price['Ticker'].isin(tickers)]
    else:
        tickers = all_tickers
        print(f"ğŸ“Œ [å…¨é‡æ¨¡å¼] ä½¿ç”¨æ‰€æœ‰ {len(tickers)} åªè‚¡ç¥¨")
    
    ticker2idx = {t: i for i, t in enumerate(tickers)}
    num_nodes = len(tickers)
    print(f"    æœ€ç»ˆä½¿ç”¨ {num_nodes} åªè‚¡ç¥¨æ„å»ºå›¾è°±ã€‚")

    if not os.path.exists(INPUT_NEWS):
        print(f"[WARN] æœªæ‰¾åˆ°æ–°é—»æ–‡ä»¶ {INPUT_NEWS}ï¼Œä¿å­˜å•ä½é˜µã€‚")
        adj_matrix = np.eye(num_nodes, dtype=np.float32)
        np.save(OUTPUT_GRAPH, adj_matrix)
        return

    df_news = pd.read_csv(INPUT_NEWS, low_memory=False)
    print(f"    åŸå§‹æ–°é—»æ€»æ•°: {len(df_news)}")
    
    # å¦‚æœä½¿ç”¨ S&P 500 æ¨¡å¼ï¼Œè¿‡æ»¤æ–°é—»æ•°æ®
    if use_sp500 and len(tickers) < len(all_tickers):
        before_filter = len(df_news)
        df_news = df_news[df_news['Ticker'].isin(tickers)].copy()
        print(f"    [S&P 500 è¿‡æ»¤] ä¿ç•™æ–°é—»: {before_filter} -> {len(df_news)}")

    # =========================== é˜²æ­¢"æœªæ¥ä¿¡æ¯"æ•°æ®æ³„éœ² ===========================
    try:
        if 'Date' in df_news.columns:
            df_news['Date'] = pd.to_datetime(df_news['Date'], errors='coerce')
            
            if df_news['Date'].dt.tz is not None:
                df_news['Date'] = df_news['Date'].dt.tz_localize(None)

            df_price_for_split = pd.read_csv(INPUT_MODEL_DATA, usecols=['Date'])
            df_price_for_split['Date'] = pd.to_datetime(df_price_for_split['Date'])
            unique_dates = sorted(df_price_for_split['Date'].unique())
            
            if len(unique_dates) >= 2:
                split_idx = int(len(unique_dates) * 0.8)
                split_idx = min(split_idx, len(unique_dates) - 1)
                split_date = unique_dates[split_idx]
                
                print(f"\n[é˜²æ³„éœ²] åˆ‡åˆ†æ—¥æœŸ split_date = {split_date}")
                before_news = len(df_news)
                df_news = df_news[df_news['Date'] < split_date].copy()
                print(f"[é˜²æ³„éœ²] è¿‡æ»¤åä¿ç•™æ–°é—»: {before_news} -> {len(df_news)}")
            else:
                print("[WARN] æ—¥æœŸä¸è¶³ï¼Œè·³è¿‡è¿‡æ»¤ã€‚")
    except Exception as e:
        print(f"[ERROR] æ—¶é—´è¿‡æ»¤å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…¨é‡æ–°é—»ï¼ˆå­˜åœ¨æ³„éœ²é£é™©ï¼‰ã€‚")

    # =========================== åˆ†å±‚é‡‡æ ·ï¼ˆå…³é”®ä¿®æ­£ï¼‰===========================
    df_news_sampled = stratified_sample_news(
        df_news, 
        max_per_ticker=max_per_ticker, 
        max_total=max_total
    )

    # è·å–æ–‡æœ¬åˆ—
    text_col = 'Headline' if 'Headline' in df_news_sampled.columns else 'Article_title'
    if text_col not in df_news_sampled.columns:
        cols = [c for c in df_news_sampled.columns if df_news_sampled[c].dtype == object]
        text_col = cols[0] if cols else None
    
    if text_col is None:
        print("[WARN] æ²¡æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œä¿å­˜å•ä½é˜µã€‚")
        np.save(OUTPUT_GRAPH, np.eye(num_nodes, dtype=np.float32))
        return

    # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µï¼ˆå•ä½é˜µ = è‡ªç¯ï¼‰
    adj_matrix = np.eye(num_nodes, dtype=np.float32)

    # =========================== åŠ è½½ LLM æ¨¡å‹ï¼ˆå¯é€‰ï¼‰===========================
    local_model = None
    local_tokenizer = None
    
    if use_llm:
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            print(f"\n[åŠ è½½ä¸­] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH} ...")
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"    è®¾å¤‡: {device}")

            local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
            local_model = AutoModelForCausalLM.from_pretrained(
                LOCAL_MODEL_PATH,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
            print("[OK] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(">>> é™çº§ä¸ºè§„åˆ™æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
            local_model = None

    # =========================== å¾ªç¯å»ºå›¾ ===========================
    print(f"\n>>> [Step 2] å¼€å§‹å»ºå›¾ (å…± {len(df_news_sampled)} æ¡æ–°é—»)...")
    print("=" * 70)
    
    edge_count = 0
    matched_tickers = set()
    
    for _, row in tqdm(df_news_sampled.iterrows(), total=len(df_news_sampled), desc="Building Graph"):
        src_ticker = row.get('Ticker')
        if src_ticker not in ticker2idx:
            continue
            
        content = row.get(text_col, "")
        if not content or pd.isna(content):
            continue
        
        content = str(content)
        
        # LLM æ¨¡å¼
        relations = []
        if local_model:
            relations = extract_relations_with_llm(content, local_model=local_model, local_tokenizer=local_tokenizer)
        
        if relations:
            for r in relations:
                src, dst = r.get("src"), r.get("dst")
                if src and dst and src in ticker2idx and dst in ticker2idx and src != dst:
                    i, j = ticker2idx[src], ticker2idx[dst]
                    if adj_matrix[i, j] == 0:  # æ–°è¾¹
                        edge_count += 1
                    adj_matrix[i, j] = 1.0
                    adj_matrix[j, i] = 1.0
                    matched_tickers.add(src)
                    matched_tickers.add(dst)
        else:
            # è§„åˆ™å…œåº•ï¼šæ£€æŸ¥æ–°é—»ä¸­æ˜¯å¦æåˆ°å…¶ä»–è‚¡ç¥¨ä»£ç 
            for t in tickers:
                if t != src_ticker and len(str(t)) >= 2:
                    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„åŒ¹é…ï¼šè‚¡ç¥¨ä»£ç éœ€è¦ä½œä¸ºç‹¬ç«‹å•è¯å‡ºç°
                    # é¿å… "A" åŒ¹é…åˆ° "Apple" ç­‰è¯¯åŒ¹é…
                    if len(t) >= 3 and t.upper() in content.upper():
                        if t in ticker2idx:
                            i, j = ticker2idx[src_ticker], ticker2idx[t]
                            if adj_matrix[i, j] == 0:  # æ–°è¾¹
                                edge_count += 1
                            adj_matrix[i, j] = 1.0
                            adj_matrix[j, i] = 1.0
                            matched_tickers.add(src_ticker)
                            matched_tickers.add(t)

    # =========================== ä¿å­˜ç»“æœ ===========================
    print("\n>>> [Step 3] ä¿å­˜ç»“æœ...")
    np.save(OUTPUT_GRAPH, adj_matrix)
    
    # =========================== è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ ===========================
    print("\n" + "=" * 70)
    print(">>> å›¾è°±ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)
    
    # è®¡ç®—å›¾è°±ç»Ÿè®¡
    total_edges = (adj_matrix.sum() - num_nodes) / 2  # å‡å»è‡ªç¯ï¼Œé™¤ä»¥2ï¼ˆæ— å‘å›¾ï¼‰
    density = total_edges / (num_nodes * (num_nodes - 1) / 2) if num_nodes > 1 else 0
    
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„åº¦
    degrees = adj_matrix.sum(axis=1) - 1  # å‡å»è‡ªç¯
    non_isolated = np.sum(degrees > 0)
    
    print(f"    èŠ‚ç‚¹æ•° (è‚¡ç¥¨æ•°): {num_nodes}")
    print(f"    è¾¹æ•° (è‚¡ç¥¨å…³ç³»): {int(total_edges)}")
    print(f"    å›¾å¯†åº¦: {density:.6f}")
    print(f"    æœ‰è¿æ¥çš„è‚¡ç¥¨æ•°: {non_isolated} / {num_nodes} ({non_isolated/num_nodes*100:.1f}%)")
    print(f"    å¹³å‡åº¦: {degrees.mean():.2f}")
    print(f"    æœ€å¤§åº¦: {int(degrees.max())}")
    print(f"    å­¤ç«‹èŠ‚ç‚¹æ•°: {num_nodes - non_isolated}")
    
    if non_isolated < num_nodes * 0.5:
        print("\nâš ï¸ è­¦å‘Šï¼šè¶…è¿‡ä¸€åŠçš„è‚¡ç¥¨æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼")
        print("   å»ºè®®ï¼š")
        print("   1. å¢åŠ  max_per_ticker å‚æ•°")
        print("   2. ä½¿ç”¨ LLM æ¨¡å¼ (use_llm=True) æå–æ›´å¤šå…³ç³»")
        print("   3. æ£€æŸ¥æ–°é—»æ•°æ®è´¨é‡")
    
    print(f"\n[OK] å·²ä¿å­˜è‡³ {OUTPUT_GRAPH}ï¼Œå½¢çŠ¶: {adj_matrix.shape}")
    print("=" * 70)
    
    return adj_matrix


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºè‚¡ç¥¨å…³ç³»å›¾è°±')
    parser.add_argument('--use_llm', action='store_true', help='ä½¿ç”¨ LLM æå–å…³ç³»ï¼ˆé»˜è®¤ä½¿ç”¨è§„åˆ™åŒ¹é…ï¼‰')
    parser.add_argument('--max_per_ticker', type=int, default=MAX_NEWS_PER_TICKER, help='æ¯ä¸ªè‚¡ç¥¨æœ€å¤šé‡‡æ ·å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--max_total', type=int, default=MAX_TOTAL_NEWS, help='æ€»å…±æœ€å¤šå¤„ç†å¤šå°‘æ¡æ–°é—»')
    parser.add_argument('--all_stocks', action='store_true', help='ä½¿ç”¨å…¨é‡è‚¡ç¥¨ï¼ˆé»˜è®¤åªç”¨ S&P 500ï¼‰')
    
    args = parser.parse_args()
    
    print("\n" + "=" * 70)
    print("ğŸ“Š è‚¡ç¥¨å…³ç³»å›¾è°±æ„å»ºå·¥å…·")
    print("=" * 70)
    print(f"é…ç½®:")
    print(f"  - è‚¡ç¥¨èŒƒå›´: {'å…¨é‡' if args.all_stocks else 'S&P 500 æˆåˆ†è‚¡ï¼ˆæ¨èï¼‰'}")
    print(f"  - å…³ç³»æå–: {'LLM' if args.use_llm else 'è§„åˆ™åŒ¹é…'}")
    print(f"  - æ¯è‚¡ç¥¨é‡‡æ ·: {args.max_per_ticker} æ¡æ–°é—»")
    print(f"  - æ€»é‡‡æ ·ä¸Šé™: {args.max_total} æ¡")
    print("=" * 70 + "\n")
    
    build_dynamic_graph(
        use_llm=args.use_llm, 
        max_per_ticker=args.max_per_ticker,
        max_total=args.max_total,
        use_sp500=not args.all_stocks  # é»˜è®¤ä½¿ç”¨ S&P 500
    )
