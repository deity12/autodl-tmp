"""
PyTorch Datasetï¼šå°† `Final_Model_Data.csv` è½¬æ¢ä¸ºå¯è®­ç»ƒæ ·æœ¬ï¼ˆStep 4ï¼‰
====================================================

æœ¬æ¨¡å—æä¾› `FinancialDataset`ï¼Œç”¨äºï¼š
  - æ•°æ®æ¸…æ´—ï¼ˆè£å‰ªæç«¯å€¼ã€å¤„ç† NaN/Infï¼‰
  - æŒ‰æ—¶é—´ 80/20 åˆ‡åˆ† train/testï¼ˆé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰
  - ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆè®­ç»ƒé›† fitï¼Œæµ‹è¯•é›† transformï¼‰
  - ä»¥â€œåŒä¸€è‚¡ç¥¨â€ä¸ºå•ä½æ„é€ æ»‘åŠ¨çª—å£åºåˆ—æ ·æœ¬
  - è®¡ç®—è®­ç»ƒé›†æ³¢åŠ¨ç‡åˆ†ä½æ•° `vol_stats`ï¼ˆå¸¸ç”¨ p70 ä½œä¸ºé‡å­é—¨æ§é˜ˆå€¼ï¼‰
  - æ„å»º `ticker2idx`ï¼Œå¹¶åœ¨ `__getitem__` è¿”å› `node_indices` ä¾› GNN ä½¿ç”¨

è¾“å…¥ï¼š
  - `data/processed/Final_Model_Data.csv`ï¼ˆæ¥è‡ª `dataProcessed/align.py`ï¼‰

è¾“å‡ºï¼ˆæ¯æ¡æ ·æœ¬ï¼Œdictï¼‰ï¼š
  - `x`: (seq_len, input_dim) è¿‡å»è‹¥å¹²å¤©ç‰¹å¾
  - `y`: (1,) ç›®æ ‡æ—¥å¯¹æ•°æ”¶ç›Šç‡
  - `vol`: (1,) æ³¢åŠ¨ç‡ï¼ˆæœ€åä¸€æ—¥ï¼‰
  - `node_indices`: (,) è‚¡ç¥¨èŠ‚ç‚¹ç´¢å¼•ï¼ˆç”¨äºå›¾èšåˆï¼‰
  - `target_date`: str ç›®æ ‡æ—¥æœŸï¼ˆç”¨äºæŒ‰æ—¥æœŸæˆªé¢ IC/RankIC æˆ–æŒ‰æ—¥åˆ†ç»„ batchï¼‰
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, RobustScaler
import os

class FinancialDataset(Dataset):
    """
    é‡‘èæ•°æ®é›†ç±»ï¼Œæ”¹è¿›ç‚¹ï¼š
    1. ä½¿ç”¨ RobustScaler ä½œä¸ºå¯é€‰é¡¹ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
    2. è®¡ç®—å¹¶å­˜å‚¨æ³¢åŠ¨ç‡åˆ†ä½æ•°ï¼Œç”¨äºåŠ¨æ€è®¾ç½®é‡å­é˜ˆå€¼
    3. æ”¹è¿›çš„æ•°æ®æ¸…æ´—æµç¨‹
    4. æ·»åŠ æ•°æ®å¢å¼ºé€‰é¡¹ï¼ˆå¯é€‰ï¼‰
    """
    def __init__(self, csv_path, seq_len=30, pred_len=1, mode='train', scaler=None, 
                 vol_stats=None, use_robust_scaler=False):
        """
        å‚æ•°è¯´æ˜:
            csv_path: æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶ Final_Model_Data.csv çš„è·¯å¾„
            seq_len: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ï¼šè¿‡å»30å¤©çš„æ•°æ®ï¼‰
            pred_len: é¢„æµ‹æ—¶é•¿ï¼ˆä¾‹å¦‚ï¼šé¢„æµ‹æœªæ¥1å¤©ï¼‰
            mode: 'train'ï¼ˆè®­ç»ƒï¼‰æˆ– 'test'ï¼ˆæµ‹è¯•ï¼‰
            scaler: å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›ï¼‰
            vol_stats: æ³¢åŠ¨ç‡ç»Ÿè®¡ä¿¡æ¯ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›ï¼‰
            use_robust_scaler: æ˜¯å¦ä½¿ç”¨ RobustScalerï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½ {mode} æ•°æ®ï¼Œæ¥æºï¼š{csv_path}...")
        
        # [é”™è¯¯å¤„ç†] æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {csv_path}")
        
        self.df = pd.read_csv(csv_path)
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.mode = mode
        self.use_robust_scaler = use_robust_scaler

        # ã€å…³é”®å¯¹é½ã€‘ç»Ÿä¸€è‚¡ç¥¨ä»£ç æ ¼å¼ä¸ºå¤§å†™ï¼Œç¡®ä¿ä¸ build_graph.py è¾“å‡ºçš„ Graph_Adjacency_tickers.json ä¸€è‡´
        # é¿å…å‡ºç°å› å¤§å°å†™å·®å¼‚å¯¼è‡´çš„â€œå›¾è°±ç´¢å¼•é”™ä½â€ï¼ˆæœ€å±é™©ï¼šä¸ä¸€å®šæŠ¥é”™ï¼Œä½†ä¼šè®©è®­ç»ƒç»“æœå¤±çœŸï¼‰
        if 'Ticker' in self.df.columns:
            self.df['Ticker'] = self.df['Ticker'].astype(str).str.upper()
            # æ¸…ç†å¼‚å¸¸ tickerï¼ˆæå°‘æ•°æƒ…å†µä¸‹ä¼šå‡ºç° NaN -> "NAN"ï¼‰
            self.df = self.df[self.df['Ticker'] != 'NAN'].copy()
        
        # å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
        self.feature_cols = ['Open', 'Close', 'High', 'Low', 'Volume', 'Market_Close', 'Market_Vol', 'Volatility_20d']
        self.target_col = 'Log_Ret'
        
        # =======================================================
        # ğŸ›¡ï¸ ã€æ”¹è¿›ã€‘é²æ£’æ€§æ•°æ®æ¸…æ´—é˜²ç«å¢™
        # =======================================================
        
        # 1. è£å‰ªï¼ˆClippingï¼‰ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
        # æ—¥æ”¶ç›Šç‡ Â±50% å·²ç»æ˜¯æç«¯æƒ…å†µï¼ˆè‚¡ç¥¨æ¶¨åœ/è·Œåœï¼‰
        if self.target_col in self.df.columns:
            self.df[self.target_col] = self.df[self.target_col].clip(-0.5, 0.5)
            
        # 2. å¤„ç†æ— ç©·å¤§å€¼ï¼šå°† Inf å’Œ -Inf æ›¿æ¢ä¸º NaN
        self.df = self.df.replace([np.inf, -np.inf], np.nan)
        
        # 3. å¡«å……/åˆ é™¤ç¼ºå¤±å€¼
        numeric_cols = self.feature_cols + [self.target_col]
        
        # ä½¿ç”¨å‰å‘å¡«å……ä¿®å¤ç¼ºå¤±çš„ä»·æ ¼æ•°æ®
        self.df[numeric_cols] = self.df[numeric_cols].ffill()
        
        # åˆ é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        before_len = len(self.df)
        self.df = self.df.dropna(subset=numeric_cols)
        after_len = len(self.df)
        
        if before_len != after_len:
            print(f"âš ï¸ å·²æ¸…ç†å¹¶åˆ é™¤ {before_len - after_len} è¡ŒåŒ…å«æ— æ•ˆæ•°æ®ï¼ˆNaNï¼‰çš„è®°å½•")

        # 4. æ³¢åŠ¨ç‡ä¿®æ­£ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
        if 'Volatility_20d' in self.df.columns:
             self.df['Volatility_20d'] = self.df['Volatility_20d'].fillna(0).clip(0, 2.0)

        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼šæ— æ— ç©·å€¼ã€æ— ç¼ºå¤±å€¼ã€æç«¯å€¼å·²è£å‰ªã€‚")
        # =======================================================
        
        # 1. æŒ‰æ—¥æœŸæ’åº
        self.df['Date'] = pd.to_datetime(self.df['Date'])
        self.df = self.df.sort_values(['Ticker', 'Date']).reset_index(drop=True)

        # ã€GNN æ”¯æŒã€‘åœ¨åˆ’åˆ† train/test å‰ï¼Œç”¨å…¨é‡ Ticker æ„å»º ticker->idx æ˜ å°„
        # é¡ºåºä¸ build_graph.pyã€Final_Model_Data çš„ sorted(unique) ä¸€è‡´ï¼Œä¾¿äºé‚»æ¥çŸ©é˜µå¯¹é½
        all_tickers = sorted(self.df['Ticker'].unique())
        self.ticker2idx = {t: i for i, t in enumerate(all_tickers)}
        
        # 2. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
        dates = sorted(self.df['Date'].unique())
        if len(dates) < 2:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼šä»…æ‰¾åˆ° {len(dates)} ä¸ªæ—¥æœŸã€‚")
            
        split_idx = int(len(dates) * 0.8)
        split_idx = min(split_idx, len(dates) - 1)
        split_date = dates[split_idx]
        
        if mode == 'train':
            self.df = self.df[self.df['Date'] < split_date].copy()
        else:
            self.df = self.df[self.df['Date'] >= split_date].copy()
            
        # ã€å…³é”®ä¿®å¤ã€‘é‡ç½®ç´¢å¼•ï¼é¿å…åç»­æ»‘åŠ¨çª—å£å‡ºé”™
        self.df = self.df.reset_index(drop=True)
            
        # 3. æ ‡å‡†åŒ–
        if mode == 'train':
            if use_robust_scaler:
                # RobustScaler å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
                self.scaler = RobustScaler(quantile_range=(10, 90))
            else:
                self.scaler = StandardScaler()
            feature_array = self.df[self.feature_cols].values
            self.df[self.feature_cols] = self.scaler.fit_transform(feature_array)
        else:
            if scaler is None:
                raise ValueError("æµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ï¼ˆscalerï¼‰ã€‚")
            self.scaler = scaler
            feature_array = self.df[self.feature_cols].values
            self.df[self.feature_cols] = self.scaler.transform(feature_array)
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘è½¬æ¢ä¸º Numpy float32 ç±»å‹ï¼ˆèŠ‚çœå†…å­˜ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰
        self.data_x = self.df[self.feature_cols].values.astype(np.float32)
        self.data_y = self.df[self.target_col].values.astype(np.float32)
        self.data_vol = self.df['Volatility_20d'].values.astype(np.float32)
        
        # =======================================================
        # ã€æ–°å¢ã€‘è®¡ç®—æ³¢åŠ¨ç‡åˆ†ä½æ•°ï¼Œç”¨äºåŠ¨æ€è®¾ç½®é‡å­é˜ˆå€¼
        # =======================================================
        if mode == 'train':
            # åœ¨æ ‡å‡†åŒ–åçš„æ³¢åŠ¨ç‡ä¸Šè®¡ç®—åˆ†ä½æ•°
            vol_col_idx = self.feature_cols.index('Volatility_20d')
            vol_standardized = self.data_x[:, vol_col_idx]  # æ ‡å‡†åŒ–åçš„æ³¢åŠ¨ç‡
            self.vol_stats = {
                'mean': float(np.mean(vol_standardized)),
                'std': float(np.std(vol_standardized)),
                'p50': float(np.percentile(vol_standardized, 50)),  # ä¸­ä½æ•°
                'p60': float(np.percentile(vol_standardized, 60)),
                'p70': float(np.percentile(vol_standardized, 70)),  # æ¨èé˜ˆå€¼
                'p80': float(np.percentile(vol_standardized, 80)),
                'p90': float(np.percentile(vol_standardized, 90)),
                'min': float(np.min(vol_standardized)),
                'max': float(np.max(vol_standardized)),
            }
            print(f"ğŸ“Š æ³¢åŠ¨ç‡ç»Ÿè®¡ï¼ˆæ ‡å‡†åŒ–åï¼‰:")
            print(f"   mean={self.vol_stats['mean']:.3f}, std={self.vol_stats['std']:.3f}")
            print(f"   p50={self.vol_stats['p50']:.3f}, p70={self.vol_stats['p70']:.3f}, p90={self.vol_stats['p90']:.3f}")
            print(f"   â­ æ¨èé‡å­é˜ˆå€¼ q_threshold: {self.vol_stats['p70']:.3f} (70%åˆ†ä½æ•°)")
        else:
            if vol_stats is None:
                # å¦‚æœæµ‹è¯•æ—¶æ²¡æä¾› vol_statsï¼Œä½¿ç”¨é»˜è®¤å€¼
                self.vol_stats = {'p70': 0.5}
                print("âš ï¸ æµ‹è¯•æ¨¡å¼æœªæä¾› vol_statsï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼ 0.5")
            else:
                self.vol_stats = vol_stats
        
        # 4. æ„å»ºæ»‘åŠ¨çª—å£ç´¢å¼•ï¼ˆç¡®ä¿ä¸è·¨è‚¡ç¥¨æ‹¼æ¥åºåˆ—ï¼‰
        print("æ­£åœ¨æ„å»ºæ»‘åŠ¨çª—å£ç´¢å¼•...")
        self.indices = []
        self.target_dates = []  # ç”¨äºâ€œæŒ‰æ—¥æœŸåˆ†ç»„â€çš„rank/rankingè®­ç»ƒï¼ˆé¡¶ä¼šå¸¸è§åšæ³•ï¼‰
        
        # æŒ‰è‚¡ç¥¨ä»£ç ï¼ˆTickerï¼‰åˆ†ç»„ï¼Œä¿è¯æ¯ä¸ªåºåˆ—åªæ¥è‡ªåŒä¸€åªè‚¡ç¥¨
        groups = self.df.groupby('Ticker')
        for _, group in groups:
            group_len = len(group)
            if group_len > seq_len + pred_len:
                start_row = group.index[0]
                # é«˜æ•ˆåœ°æ·»åŠ æ‰€æœ‰æœ‰æ•ˆçš„èµ·å§‹ä½ç½®ç´¢å¼•
                for i in range(group_len - seq_len - pred_len + 1):
                    s = start_row + i
                    end_row = s + self.seq_len
                    target_row = end_row + self.pred_len - 1
                    # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿ target_row ä¸è¶…å‡º group èŒƒå›´
                    if target_row >= start_row + group_len:
                        continue
                    self.indices.append(s)
                    # ç›®æ ‡å¯¹é½ï¼šy æ¥è‡ª target_rowï¼Œå› æ­¤ç”¨äºæ’åº/RankIC çš„"æˆªé¢æ—¥æœŸ"åº”ä»¥ target_date åˆ†ç»„
                    self.target_dates.append(self.df['Date'].iloc[target_row])
                    
        print(f"{mode} æ•°æ®é›†å…±ç”Ÿæˆæ ·æœ¬æ•°: {len(self.indices)}")
        # ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼Œä¾¿äº sampler/groupbyï¼ˆDataLoader å¯¹ Timestamp çš„é»˜è®¤ collate è¡Œä¸ºä¸ç¨³å®šï¼‰
        self.target_dates = [pd.Timestamp(d).strftime("%Y-%m-%d") for d in self.target_dates]

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        start_row = self.indices[idx]
        end_row = start_row + self.seq_len
        target_row = end_row + self.pred_len - 1
        
        # ç›´æ¥ä½¿ç”¨ NumPy åˆ‡ç‰‡ï¼ˆé€Ÿåº¦æœ€å¿«ï¼‰
        x = self.data_x[start_row : end_row]       # å½¢çŠ¶: (åºåˆ—é•¿åº¦, ç‰¹å¾æ•°)
        y = self.data_y[target_row]                # æ ‡é‡ï¼ˆå•æ—¥å¯¹æ•°æ”¶ç›Šç‡ï¼‰
        vol = self.data_vol[end_row - 1]           # æ ‡é‡ï¼ˆæœ€åä¸€æ—¥çš„æ³¢åŠ¨ç‡ï¼‰
        # ã€GNN æ”¯æŒã€‘å½“å‰æ ·æœ¬å¯¹åº”çš„è‚¡ç¥¨åœ¨å›¾ä¸­çš„èŠ‚ç‚¹ç´¢å¼•ï¼Œä¾› QL_MATCC_GNN_Model åšå›¾èšåˆ
        ticker = self.df['Ticker'].iloc[start_row]
        node_idx = self.ticker2idx.get(ticker, 0)
        
        return {
            'x': torch.from_numpy(x),                   # è½¬æ¢ä¸º PyTorch å¼ é‡
            'y': torch.tensor([y], dtype=torch.float32),
            'vol': torch.tensor([vol], dtype=torch.float32),
            'node_indices': torch.tensor(node_idx, dtype=torch.long),
            'target_date': self.target_dates[idx],      # ç”¨äºæŒ‰æ—¥æœŸåšæˆªé¢æ’åº/RankIC loss
        }

# ================= æµ‹è¯•ä»£ç  =================
if __name__ == "__main__":
    CSV_PATH = './data/processed/Final_Model_Data.csv'
    
    # 1. æµ‹è¯•è®­ç»ƒæ•°æ®åŠ è½½å™¨
    print(">>> æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒæ•°æ®é›†...")
    train_dataset = FinancialDataset(CSV_PATH, seq_len=30, mode='train')
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True,
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    print("\n>>> æ£€æŸ¥è®­ç»ƒæ‰¹æ¬¡æ•°æ®...")
    for batch in train_loader:
        print("è¾“å…¥å¼ é‡å½¢çŠ¶:", batch['x'].shape)
        print("ç›®æ ‡å¼ é‡å½¢çŠ¶:", batch['y'].shape)
        print("æ³¢åŠ¨ç‡å¼ é‡å½¢çŠ¶:", batch['vol'].shape)
        print("èŠ‚ç‚¹ç´¢å¼•å½¢çŠ¶:", batch['node_indices'].shape)
        print("æ ·æœ¬æ•°æ®åŠ è½½æˆåŠŸï¼")
        
        if torch.cuda.is_available():
            print(f"æ•°æ®æ‰€åœ¨è®¾å¤‡: {batch['x'].device}ï¼ˆGPU å¯ç”¨ä½†å½“å‰æœªä½¿ç”¨ï¼‰")
        break

    # 2. æµ‹è¯•æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆä¼ å…¥ vol_statsï¼‰
    print("\n>>> æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†...")
    test_dataset = FinancialDataset(
        CSV_PATH, seq_len=30, mode='test', 
        scaler=train_dataset.scaler,
        vol_stats=train_dataset.vol_stats  # ä¼ å…¥è®­ç»ƒé›†çš„æ³¢åŠ¨ç‡ç»Ÿè®¡
    )
    
    # ã€ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºã€‘
    print(f"\n>>> æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_dataset)}")
    print(f"  - æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"  - è¾“å…¥åºåˆ—é•¿åº¦: {train_dataset.seq_len}")
    print(f"  - é¢„æµ‹æ­¥é•¿: {train_dataset.pred_len}")
    print(f"  - ç‰¹å¾ç»´åº¦æ•°: {len(train_dataset.feature_cols)}")
    print(f"  - æ³¢åŠ¨ç‡åˆ†ä½æ•° (p70): {train_dataset.vol_stats['p70']:.4f}")
    print(f"  - æ¨èé‡å­é˜ˆå€¼: {train_dataset.vol_stats['p70']:.4f}")