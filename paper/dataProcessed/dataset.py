"""
PyTorch Datasetï¼šå°† `Final_Model_Data.csv` è½¬æ¢ä¸ºå¯è®­ç»ƒæ ·æœ¬ï¼ˆStep 4ï¼‰
====================================================

ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘æ ¹æ®æ–°ç ”ç©¶æ–¹å‘ï¼Œæœ¬æ¨¡å—æ”¯æŒ Graph-RWKV æ¨¡å‹çš„æ•°æ®åŠ è½½ï¼š

æœ¬æ¨¡å—æä¾› `FinancialDataset`ï¼Œç”¨äºï¼š
  - æ•°æ®æ¸…æ´—ï¼ˆè£å‰ªæç«¯å€¼ã€å¤„ç† NaN/Infï¼‰
  - æŒ‰æ—¶é—´ 80/20 åˆ‡åˆ† train/testï¼ˆé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰
     ã€æ³¨æ„ã€‘å®Œæ•´æ»šåŠ¨çª—å£éªŒè¯éœ€åœ¨è¯„ä¼°è„šæœ¬ä¸­å®ç°ï¼ˆè§è®ºæ–‡ 3.3ï¼‰
  - ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆè®­ç»ƒé›† fitï¼Œæµ‹è¯•é›† transformï¼‰
  - ä»¥â€œåŒä¸€è‚¡ç¥¨â€ä¸ºå•ä½æ„é€ æ»‘åŠ¨çª—å£åºåˆ—æ ·æœ¬
  - è®¡ç®—è®­ç»ƒé›†æ³¢åŠ¨ç‡åˆ†ä½æ•° `vol_stats`ï¼ˆå·²æ³¨é‡Šï¼šæ–°æ–¹å‘ä¸ä½¿ç”¨é‡å­é—¨æ§ï¼‰
  - æ„å»º `ticker2idx`ï¼Œå¹¶åœ¨ `__getitem__` è¿”å› `node_indices` ä¾› GNN ä½¿ç”¨
     ã€å…³é”®ã€‘ç¡®ä¿ä¸ Graph_Tickers.json ä¸­çš„èŠ‚ç‚¹é¡ºåºä¸€è‡´ï¼Œé¿å…ç´¢å¼•é”™ä½

è¾“å…¥ï¼š
  - `paper/data/processed/Final_Model_Data.csv`ï¼ˆæ¥è‡ª `dataProcessed/align.py`ï¼‰
  - `paper/data/processed/Graph_Tickers.json`ï¼ˆæ¥è‡ª `dataProcessed/build_graph.py`ï¼Œç”¨äºèŠ‚ç‚¹å¯¹é½ï¼‰

è¾“å‡ºï¼ˆæ¯æ¡æ ·æœ¬ï¼Œdictï¼‰ï¼š
  - `x`: (seq_len, input_dim) è¿‡å»è‹¥å¹²å¤©ç‰¹å¾ï¼ˆè¾“å…¥ RWKV æ—¶é—´ç¼–ç å™¨ï¼‰
  - `y`: (1,) ç›®æ ‡æ—¥å¯¹æ•°æ”¶ç›Šç‡ï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰
  - `vol`: (1,) æ³¢åŠ¨ç‡ï¼ˆæœ€åä¸€æ—¥ï¼Œä¿ç•™ä»¥å…¼å®¹æ¥å£ï¼Œä½†æ–°æ–¹å‘ä¸­ä¸ä½¿ç”¨ï¼‰
  - `node_indices`: (,) è‚¡ç¥¨èŠ‚ç‚¹ç´¢å¼•ï¼ˆç”¨äº GAT ç©ºé—´èšåˆï¼‰
  - `target_date`: str ç›®æ ‡æ—¥æœŸï¼ˆç”¨äºæŒ‰æ—¥æœŸæˆªé¢ IC/RankIC æˆ–æŒ‰æ—¥åˆ†ç»„ batchï¼‰

ã€è®ºæ–‡å¯¹åº”ã€‘ï¼š
    - å¯¹åº”è®ºæ–‡ 3.1 æ•°æ®é›†å‡†å¤‡
    - æ”¯æŒ RWKV æ—¶é—´åºåˆ—ç¼–ç å™¨çš„è¾“å…¥æ ¼å¼
    - æ”¯æŒåŠ¨æ€å›¾æ³¨æ„åŠ›ç½‘ç»œçš„èŠ‚ç‚¹ç´¢å¼•å¯¹é½
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, RobustScaler
import os

class FinancialDataset(Dataset):
    """
    é‡‘èæ•°æ®é›†ç±»ï¼Œæ”¹è¿›ç‚¹ï¼š
    1. ä½¿ç”¨ RobustScaler ä½œä¸ºå¯é€‰é¡¹ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
    2. è®¡ç®—å¹¶å­˜å‚¨æ³¢åŠ¨ç‡åˆ†ä½æ•°ï¼Œç”¨äºåŠ¨æ€è®¾ç½®é‡å­é˜ˆå€¼
    3. æ”¹è¿›çš„æ•°æ®æ¸…æ´—æµç¨‹
    4. æ·»åŠ æ•°æ®å¢å¼ºé€‰é¡¹ï¼ˆå¯é€‰ï¼‰
    """
    def __init__(
        self,
        csv_path,
        features_path=None,
        seq_len=30,
        pred_len=1,
        mode='train',
        scaler=None,
        vol_stats=None,
        use_robust_scaler=False,
        start_date=None,
        end_date=None,
        use_date_split=True,
        feature_cols=None,
        feature_columns_path=None,
    ):
        """
        å‚æ•°è¯´æ˜:
            csv_path: æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶ Final_Model_Data.csv çš„è·¯å¾„
            seq_len: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ï¼šè¿‡å»30å¤©çš„æ•°æ®ï¼‰
            pred_len: é¢„æµ‹æ—¶é•¿ï¼ˆä¾‹å¦‚ï¼šé¢„æµ‹æœªæ¥1å¤©ï¼‰
            mode: 'train'ï¼ˆè®­ç»ƒï¼‰æˆ– 'test'ï¼ˆæµ‹è¯•ï¼‰
            scaler: å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›ï¼‰
            vol_stats: æ³¢åŠ¨ç‡ç»Ÿè®¡ä¿¡æ¯ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›ï¼‰
            use_robust_scaler: æ˜¯å¦ä½¿ç”¨ RobustScalerï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½ {mode} æ•°æ®ï¼Œæ¥æºï¼š{csv_path}...")
        
        # [é”™è¯¯å¤„ç†] æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {csv_path}")
        
        self.df = pd.read_csv(csv_path)
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.mode = mode
        self.use_robust_scaler = use_robust_scaler
        self.features_path = features_path
        self.split_date = None
        self.start_date = start_date
        self.end_date = end_date

        # ã€å…³é”®å¯¹é½ã€‘ç»Ÿä¸€è‚¡ç¥¨ä»£ç æ ¼å¼ä¸ºå¤§å†™ï¼Œç¡®ä¿ä¸ build_graph.py è¾“å‡ºçš„ Graph_Tickers.json ä¸€è‡´
        # é¿å…å‡ºç°å› å¤§å°å†™å·®å¼‚å¯¼è‡´çš„â€œå›¾è°±ç´¢å¼•é”™ä½â€ï¼ˆæœ€å±é™©ï¼šä¸ä¸€å®šæŠ¥é”™ï¼Œä½†ä¼šè®©è®­ç»ƒç»“æœå¤±çœŸï¼‰
        if 'Ticker' in self.df.columns:
            # ä¸ build_graph / filter_sp500 ç»Ÿä¸€ï¼šå¤§å°å†™å½’ä¸€ + '-'/' .' å½’ä¸€ï¼ˆå¦‚ BRK-B -> BRK.Bï¼‰
            self.df['Ticker'] = (
                self.df['Ticker']
                .astype(str)
                .str.upper()
                .str.replace("-", ".", regex=False)
            )
            # æ¸…ç†å¼‚å¸¸ tickerï¼ˆæå°‘æ•°æƒ…å†µä¸‹ä¼šå‡ºç° NaN -> "NAN"ï¼‰
            self.df = self.df[self.df['Ticker'] != 'NAN'].copy()
        
        # å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
        default_feature_cols = [
            'Open', 'Close', 'High', 'Low', 'Volume',
            'Market_Close', 'Market_Vol', 'Volatility_20d',
        ]
        if feature_cols is not None:
            self.feature_cols = list(feature_cols)
        else:
            if feature_columns_path is None:
                feature_columns_path = os.path.join(os.path.dirname(csv_path), 'feature_columns.json')
            if os.path.exists(feature_columns_path):
                try:
                    import json
                    with open(feature_columns_path, 'r', encoding='utf-8') as f:
                        self.feature_cols = json.load(f)
                    print(f"    [ç‰¹å¾] ä» {feature_columns_path} è¯»å–ç‰¹å¾åˆ—ï¼Œå…± {len(self.feature_cols)} ç»´")
                except Exception as e:
                    print(f"    [WARN] è¯»å–ç‰¹å¾åˆ—å¤±è´¥: {e}ï¼Œå›é€€é»˜è®¤ 8 ç»´ç‰¹å¾")
                    self.feature_cols = default_feature_cols
            else:
                self.feature_cols = default_feature_cols
        # [DEBUG] å¼ºåˆ¶é”å®šä¸ºåŸºç¡€ç‰¹å¾ï¼Œæ’é™¤ Alpha158 å¹²æ‰°
        self.feature_cols = ['Open', 'Close', 'High', 'Low', 'Volume']
        print(f"âš ï¸ [DEBUG] åº•çº¿æµ‹è¯•ï¼šå·²å¼ºåˆ¶é”å®šç‰¹å¾ä¸º {self.feature_cols}")
        self.target_col = 'Log_Ret'

        # å¦‚æœç‰¹å¾åˆ—ä¸åœ¨ä¸» CSV ä¸­ï¼Œåˆ™å°è¯•ä»å¤–éƒ¨ç‰¹å¾æ–‡ä»¶ï¼ˆParquetï¼‰åˆå¹¶è¿›æ¥ã€‚
        # è¿™ä¸â€œAlpha158-like å› å­å•ç‹¬è½ç›˜ä¸º Parquetâ€æµç¨‹å¯¹é½ã€‚
        missing_cols = [c for c in self.feature_cols if c not in self.df.columns]
        if missing_cols:
            # é»˜è®¤åœ¨åŒç›®å½•æŸ¥æ‰¾ï¼šsp500_alpha158_features.parquet
            feat_path = features_path
            if feat_path is None:
                candidate = os.path.join(os.path.dirname(csv_path), "sp500_alpha158_features.parquet")
                if os.path.exists(candidate):
                    feat_path = candidate

            if feat_path and os.path.exists(feat_path):
                try:
                    df_feat = pd.read_parquet(feat_path)
                    # ã€ä¿®å¤ã€‘ç¡®ä¿ä¸¤è¾¹çš„ Date åˆ—ç±»å‹ä¸€è‡´
                    self.df["Date"] = pd.to_datetime(self.df["Date"], errors="coerce")
                    df_feat["Date"] = pd.to_datetime(df_feat["Date"], errors="coerce")
                    df_feat["Ticker"] = (
                        df_feat["Ticker"]
                        .astype(str)
                        .str.upper()
                        .str.replace("-", ".", regex=False)
                    )
                    # å·¦è¿æ¥ï¼šä¿ç•™ä¸»æ•°æ®çš„äº¤æ˜“æ—¥ä¸æ ·æœ¬å®šä¹‰
                    self.df = self.df.merge(df_feat, on=["Date", "Ticker"], how="left")
                except Exception as e:
                    raise ValueError(f"ç‰¹å¾åˆ—ç¼ºå¤±ä¸”å¤–éƒ¨ç‰¹å¾æ–‡ä»¶è¯»å–/åˆå¹¶å¤±è´¥: {feat_path}, err={e}") from e

                missing_cols = [c for c in self.feature_cols if c not in self.df.columns]

            if missing_cols:
                raise ValueError(f"ç‰¹å¾åˆ—ä¸å­˜åœ¨: {missing_cols}")
        
        # =======================================================
        # ğŸ›¡ï¸ ã€æ”¹è¿›ã€‘é²æ£’æ€§æ•°æ®æ¸…æ´—é˜²ç«å¢™
        # =======================================================
        
        # 1. è£å‰ªï¼ˆClippingï¼‰ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
        # æ—¥æ”¶ç›Šç‡ Â±50% å·²ç»æ˜¯æç«¯æƒ…å†µï¼ˆè‚¡ç¥¨æ¶¨åœ/è·Œåœï¼‰
        if self.target_col in self.df.columns:
            self.df[self.target_col] = self.df[self.target_col].clip(-0.5, 0.5)
            
        # 2. å¤„ç†æ— ç©·å¤§å€¼ï¼šå°† Inf å’Œ -Inf æ›¿æ¢ä¸º NaN
        self.df = self.df.replace([np.inf, -np.inf], np.nan)
        
        # 3. å¡«å……/åˆ é™¤ç¼ºå¤±å€¼
        numeric_cols = self.feature_cols + [self.target_col]
        
        # ä½¿ç”¨å‰å‘å¡«å……ä¿®å¤ç¼ºå¤±çš„ä»·æ ¼æ•°æ®
        self.df[numeric_cols] = self.df[numeric_cols].ffill()
        
        # åˆ é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        before_len = len(self.df)
        self.df = self.df.dropna(subset=numeric_cols)
        after_len = len(self.df)
        
        if before_len != after_len:
            print(f"âš ï¸ å·²æ¸…ç†å¹¶åˆ é™¤ {before_len - after_len} è¡ŒåŒ…å«æ— æ•ˆæ•°æ®ï¼ˆNaNï¼‰çš„è®°å½•")

        # 4. æ³¢åŠ¨ç‡ä¿®æ­£ï¼šä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
        if 'Volatility_20d' in self.df.columns:
             self.df['Volatility_20d'] = self.df['Volatility_20d'].fillna(0).clip(0, 2.0)

        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼šæ— æ— ç©·å€¼ã€æ— ç¼ºå¤±å€¼ã€æç«¯å€¼å·²è£å‰ªã€‚")
        # =======================================================
        
        # 1. æŒ‰æ—¥æœŸæ’åº
        self.df['Date'] = pd.to_datetime(self.df['Date'])
        self.df = self.df.sort_values(['Ticker', 'Date']).reset_index(drop=True)

        # ã€GNN æ”¯æŒ V4ã€‘è¯»å–å›¾è°±èŠ‚ç‚¹åˆ—è¡¨ä»¥ç¡®ä¿ç´¢å¼•å¯¹é½
        # å¦‚æœ Graph_Tickers.json å­˜åœ¨ï¼Œä½¿ç”¨å…¶ä¸­çš„èŠ‚ç‚¹é¡ºåºï¼›å¦åˆ™ä½¿ç”¨æ•°æ®ä¸­çš„å…¨é‡ ticker
        import json
        graph_tickers_path = os.path.join(os.path.dirname(csv_path), 'Graph_Tickers.json')
        if os.path.exists(graph_tickers_path):
            try:
                with open(graph_tickers_path, 'r') as f:
                    graph_data = json.load(f)
                    graph_tickers = graph_data.get('tickers', [])
                print(f"    [V4 å¯¹é½] ä» Graph_Tickers.json è¯»å– {len(graph_tickers)} ä¸ªå›¾èŠ‚ç‚¹")
                self.ticker2idx = {t: i for i, t in enumerate(graph_tickers)}
                # è¿‡æ»¤æ•°æ®ï¼šåªä¿ç•™å›¾ä¸­å­˜åœ¨çš„è‚¡ç¥¨
                self.df = self.df[self.df['Ticker'].isin(set(graph_tickers))].copy()
                print(f"    [V4 å¯¹é½] è¿‡æ»¤åæ•°æ®åŒ…å« {self.df['Ticker'].nunique()} åªè‚¡ç¥¨")
            except Exception as e:
                print(f"    [WARN] è¯»å– Graph_Tickers.json å¤±è´¥: {e}ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„å…¨é‡ ticker")
                all_tickers = sorted(self.df['Ticker'].unique())
                self.ticker2idx = {t: i for i, t in enumerate(all_tickers)}
        else:
            print(f"    [INFO] Graph_Tickers.json ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„å…¨é‡ ticker")
            all_tickers = sorted(self.df['Ticker'].unique())
            self.ticker2idx = {t: i for i, t in enumerate(all_tickers)}
        
        # 2. æ—¥æœŸè¿‡æ»¤ä¸åˆ‡åˆ†
        if start_date or end_date:
            if start_date:
                self.df = self.df[self.df['Date'] >= pd.to_datetime(start_date)].copy()
            if end_date:
                self.df = self.df[self.df['Date'] <= pd.to_datetime(end_date)].copy()
        elif use_date_split:
            dates = sorted(self.df['Date'].unique())
            if len(dates) < 2:
                raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼šä»…æ‰¾åˆ° {len(dates)} ä¸ªæ—¥æœŸã€‚")
                
            split_idx = int(len(dates) * 0.8)
            split_idx = min(split_idx, len(dates) - 1)
            split_date = dates[split_idx]
            self.split_date = pd.to_datetime(split_date)
            
            if mode == 'train':
                self.df = self.df[self.df['Date'] < split_date].copy()
            else:
                self.df = self.df[self.df['Date'] >= split_date].copy()
            
        # ã€å…³é”®ä¿®å¤ã€‘é‡ç½®ç´¢å¼•ï¼é¿å…åç»­æ»‘åŠ¨çª—å£å‡ºé”™
        self.df = self.df.reset_index(drop=True)
            
        # 3. æ ‡å‡†åŒ–
        if mode == 'train':
            if use_robust_scaler:
                # RobustScaler å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
                self.scaler = RobustScaler(quantile_range=(10, 90))
            else:
                self.scaler = StandardScaler()
            feature_array = self.df[self.feature_cols].values
            self.df[self.feature_cols] = self.scaler.fit_transform(feature_array)
        else:
            if scaler is None:
                raise ValueError("æµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ï¼ˆscalerï¼‰ã€‚")
            self.scaler = scaler
            feature_array = self.df[self.feature_cols].values
            self.df[self.feature_cols] = self.scaler.transform(feature_array)
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘è½¬æ¢ä¸º Numpy float32 ç±»å‹ï¼ˆèŠ‚çœå†…å­˜ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰
        self.data_x = self.df[self.feature_cols].values.astype(np.float32)
        self.data_y = self.df[self.target_col].values.astype(np.float32)
        self.data_vol = self.df['Volatility_20d'].values.astype(np.float32)
        
        # =======================================================
        # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨é‡å­é—¨æ§ï¼Œvol_stats è®¡ç®—å·²æ³¨é‡Š
        # =======================================================
        # ä¸ºäº†å…¼å®¹æ¥å£ï¼Œä¿ç•™ vol_stats ä½†è®¾ä¸ºç©ºå­—å…¸
        if mode == 'train':
            # ã€å·²æ³¨é‡Šã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨é‡å­é—¨æ§ï¼Œä¸å†éœ€è¦è®¡ç®—æ³¢åŠ¨ç‡åˆ†ä½æ•°
            # vol_col_idx = self.feature_cols.index('Volatility_20d')
            # vol_standardized = self.data_x[:, vol_col_idx]
            # self.vol_stats = {...}
            self.vol_stats = {}  # ç©ºå­—å…¸ä»¥å…¼å®¹æ¥å£
        else:
            # å…¼å®¹æ¥å£ï¼Œä½†æ–°æ–¹å‘ä¸­ä¸ä½¿ç”¨
            self.vol_stats = vol_stats if vol_stats is not None else {}
        
        # 4. æ„å»ºæ»‘åŠ¨çª—å£ç´¢å¼•ï¼ˆç¡®ä¿ä¸è·¨è‚¡ç¥¨æ‹¼æ¥åºåˆ—ï¼‰
        print("æ­£åœ¨æ„å»ºæ»‘åŠ¨çª—å£ç´¢å¼•...")
        self.indices = []
        self.target_dates = []  # ç”¨äºâ€œæŒ‰æ—¥æœŸåˆ†ç»„â€çš„rank/rankingè®­ç»ƒï¼ˆé¡¶ä¼šå¸¸è§åšæ³•ï¼‰
        
        # æŒ‰è‚¡ç¥¨ä»£ç ï¼ˆTickerï¼‰åˆ†ç»„ï¼Œä¿è¯æ¯ä¸ªåºåˆ—åªæ¥è‡ªåŒä¸€åªè‚¡ç¥¨
        groups = self.df.groupby('Ticker')
        for _, group in groups:
            group_len = len(group)
            if group_len > seq_len + pred_len:
                start_row = group.index[0]
                # é«˜æ•ˆåœ°æ·»åŠ æ‰€æœ‰æœ‰æ•ˆçš„èµ·å§‹ä½ç½®ç´¢å¼•
                for i in range(group_len - seq_len - pred_len + 1):
                    s = start_row + i
                    end_row = s + self.seq_len
                    target_row = end_row + self.pred_len - 1
                    # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿ target_row ä¸è¶…å‡º group èŒƒå›´
                    if target_row >= start_row + group_len:
                        continue
                    self.indices.append(s)
                    # ç›®æ ‡å¯¹é½ï¼šy æ¥è‡ª target_rowï¼Œå› æ­¤ç”¨äºæ’åº/RankIC çš„"æˆªé¢æ—¥æœŸ"åº”ä»¥ target_date åˆ†ç»„
                    self.target_dates.append(self.df['Date'].iloc[target_row])
                    
        print(f"{mode} æ•°æ®é›†å…±ç”Ÿæˆæ ·æœ¬æ•°: {len(self.indices)}")
        # ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼Œä¾¿äº sampler/groupbyï¼ˆDataLoader å¯¹ Timestamp çš„é»˜è®¤ collate è¡Œä¸ºä¸ç¨³å®šï¼‰
        self.target_dates = [pd.Timestamp(d).strftime("%Y-%m-%d") for d in self.target_dates]

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        start_row = self.indices[idx]
        end_row = start_row + self.seq_len
        target_row = end_row + self.pred_len - 1
        
        # ç›´æ¥ä½¿ç”¨ NumPy åˆ‡ç‰‡ï¼ˆé€Ÿåº¦æœ€å¿«ï¼‰
        x = self.data_x[start_row : end_row]       # å½¢çŠ¶: (åºåˆ—é•¿åº¦, ç‰¹å¾æ•°)
        y = self.data_y[target_row]                # æ ‡é‡ï¼ˆå•æ—¥å¯¹æ•°æ”¶ç›Šç‡ï¼‰
        vol = self.data_vol[end_row - 1]           # æ ‡é‡ï¼ˆæœ€åä¸€æ—¥çš„æ³¢åŠ¨ç‡ï¼‰
        # ã€GNN æ”¯æŒã€‘å½“å‰æ ·æœ¬å¯¹åº”çš„è‚¡ç¥¨åœ¨å›¾ä¸­çš„èŠ‚ç‚¹ç´¢å¼•ï¼Œä¾› QL_MATCC_GNN_Model åšå›¾èšåˆ
        ticker = self.df['Ticker'].iloc[start_row]
        node_idx = self.ticker2idx.get(ticker, 0)
        
        return {
            'x': torch.from_numpy(x),                   # è½¬æ¢ä¸º PyTorch å¼ é‡
            'y': torch.tensor([y], dtype=torch.float32),
            'vol': torch.tensor([vol], dtype=torch.float32),
            'node_indices': torch.tensor(node_idx, dtype=torch.long),
            'target_date': self.target_dates[idx],      # ç”¨äºæŒ‰æ—¥æœŸåšæˆªé¢æ’åº/RankIC loss
        }

# ================= æµ‹è¯•ä»£ç  =================
if __name__ == "__main__":
    CSV_PATH = "./paper/data/processed/Final_Model_Data.csv"
    
    # 1. æµ‹è¯•è®­ç»ƒæ•°æ®åŠ è½½å™¨
    print(">>> æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒæ•°æ®é›†...")
    train_dataset = FinancialDataset(CSV_PATH, seq_len=30, mode='train')
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True,
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    print("\n>>> æ£€æŸ¥è®­ç»ƒæ‰¹æ¬¡æ•°æ®...")
    for batch in train_loader:
        print("è¾“å…¥å¼ é‡å½¢çŠ¶:", batch['x'].shape)
        print("ç›®æ ‡å¼ é‡å½¢çŠ¶:", batch['y'].shape)
        print("æ³¢åŠ¨ç‡å¼ é‡å½¢çŠ¶:", batch['vol'].shape)
        print("èŠ‚ç‚¹ç´¢å¼•å½¢çŠ¶:", batch['node_indices'].shape)
        print("æ ·æœ¬æ•°æ®åŠ è½½æˆåŠŸï¼")
        
        if torch.cuda.is_available():
            print(f"æ•°æ®æ‰€åœ¨è®¾å¤‡: {batch['x'].device}ï¼ˆGPU å¯ç”¨ä½†å½“å‰æœªä½¿ç”¨ï¼‰")
        break

    # 2. æµ‹è¯•æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆä¼ å…¥ vol_statsï¼‰
    print("\n>>> æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†...")
    test_dataset = FinancialDataset(
        CSV_PATH, seq_len=30, mode='test', 
        scaler=train_dataset.scaler,
        vol_stats=train_dataset.vol_stats  # ä¼ å…¥è®­ç»ƒé›†çš„æ³¢åŠ¨ç‡ç»Ÿè®¡
    )
    
    # ã€ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºã€‘
    print(f"\n>>> æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_dataset)}")
    print(f"  - æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"  - è¾“å…¥åºåˆ—é•¿åº¦: {train_dataset.seq_len}")
    print(f"  - é¢„æµ‹æ­¥é•¿: {train_dataset.pred_len}")
    print(f"  - ç‰¹å¾ç»´åº¦æ•°: {len(train_dataset.feature_cols)}")
    p70 = train_dataset.vol_stats.get('p70') if hasattr(train_dataset, "vol_stats") else None
    if p70 is not None:
        print(f"  - æ³¢åŠ¨ç‡åˆ†ä½æ•° (p70): {p70:.4f}")
        print(f"  - æ¨èé‡å­é˜ˆå€¼: {p70:.4f}")
    else:
        print("  - æ³¢åŠ¨ç‡åˆ†ä½æ•° (p70): N/A")
        print("  - æ¨èé‡å­é˜ˆå€¼: N/A")
