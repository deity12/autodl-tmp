import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import os

class FinancialDataset(Dataset):
    def __init__(self, csv_path, seq_len=30, pred_len=1, mode='train', scaler=None):
        """
        å‚æ•°è¯´æ˜:
            csv_path: æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶ Final_Model_Data.csv çš„è·¯å¾„
            seq_len: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ï¼šè¿‡å»30å¤©çš„æ•°æ®ï¼‰
            pred_len: é¢„æµ‹æ—¶é•¿ï¼ˆä¾‹å¦‚ï¼šé¢„æµ‹æœªæ¥1å¤©ï¼‰
            mode: 'train'ï¼ˆè®­ç»ƒï¼‰æˆ– 'test'ï¼ˆæµ‹è¯•ï¼‰
            scaler: å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›ï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½ {mode} æ•°æ®ï¼Œæ¥æºï¼š{csv_path}...")
        
        # [é”™è¯¯å¤„ç†] æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {csv_path}")
        
        self.df = pd.read_csv(csv_path)
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.mode = mode
        
        # å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
        self.feature_cols = ['Open', 'Close', 'High', 'Low', 'Volume', 'Market_Close', 'Market_Vol', 'Volatility_20d']
        self.target_col = 'Log_Ret'
        
        # =======================================================
        # ğŸ›¡ï¸ ã€æ–°å¢ã€‘é²æ£’æ€§æ•°æ®æ¸…æ´—é˜²ç«å¢™ï¼ˆé˜²æ­¢å¼‚å¸¸å€¼ç ´åæ¨¡å‹ï¼‰
        # =======================================================
        
        # 1. è£å‰ªï¼ˆClippingï¼‰ï¼šè§£å†³æç«¯å¼‚å¸¸å€¼é—®é¢˜ï¼ˆä¾‹å¦‚ Log_Ret = 14.92ï¼‰
        # å¼ºåˆ¶å°†æ”¶ç›Šç‡é™åˆ¶åœ¨ -100% (-1.0) åˆ° +100% (1.0) ä¹‹é—´
        if self.target_col in self.df.columns:
            self.df[self.target_col] = self.df[self.target_col].clip(-1.0, 1.0)
            
        # 2. å¤„ç†æ— ç©·å¤§å€¼ï¼šå°† Inf å’Œ -Inf æ›¿æ¢ä¸º NaN
        self.df = self.df.replace([np.inf, -np.inf], np.nan)
        
        # 3. å¡«å……/åˆ é™¤ç¼ºå¤±å€¼ï¼š
        # å®šä¹‰éœ€è¦æ£€æŸ¥çš„æ•°å€¼å‹åˆ—
        numeric_cols = self.feature_cols + [self.target_col]
        
        # ä½¿ç”¨å‰å‘å¡«å……ï¼ˆforward fillï¼‰ä¿®å¤ç¼ºå¤±çš„ä»·æ ¼æ•°æ®ï¼ˆä¾‹å¦‚ä¹‹å‰å‘ç°çš„20è¡Œç¼ºå¤±ï¼‰
        self.df[numeric_cols] = self.df[numeric_cols].ffill()
        
        # å¦‚æœä»æœ‰ç¼ºå¤±å€¼ï¼ˆä¾‹å¦‚åœ¨æ•°æ®æœ€å¼€å§‹çš„ä½ç½®ï¼‰ï¼Œåˆ™ç›´æ¥åˆ é™¤è¿™äº›è¡Œ
        before_len = len(self.df)
        self.df = self.df.dropna(subset=numeric_cols)
        after_len = len(self.df)
        
        if before_len != after_len:
            print(f"âš ï¸ å·²æ¸…ç†å¹¶åˆ é™¤ {before_len - after_len} è¡ŒåŒ…å«æ— æ•ˆæ•°æ®ï¼ˆNaNï¼‰çš„è®°å½•")

        # 4. æ³¢åŠ¨ç‡ä¿®æ­£ï¼šé˜²æ­¢é‡å­å±‚è®¡ç®—æº¢å‡º
        if 'Volatility_20d' in self.df.columns:
             self.df['Volatility_20d'] = self.df['Volatility_20d'].fillna(0).clip(0, 5.0)

        print("âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼šæ— æ— ç©·å€¼ã€æ— ç¼ºå¤±å€¼ã€æç«¯å€¼å·²è£å‰ªã€‚")
        # =======================================================
        
        # 1. æŒ‰æ—¥æœŸæ’åº
        self.df['Date'] = pd.to_datetime(self.df['Date'])
        self.df = self.df.sort_values(['Ticker', 'Date']).reset_index(drop=True)

        # ã€GNN æ”¯æŒã€‘åœ¨åˆ’åˆ† train/test å‰ï¼Œç”¨å…¨é‡ Ticker æ„å»º ticker->idx æ˜ å°„
        # é¡ºåºä¸ build_graph.pyã€Final_Model_Data çš„ sorted(unique) ä¸€è‡´ï¼Œä¾¿äºé‚»æ¥çŸ©é˜µå¯¹é½
        all_tickers = sorted(self.df['Ticker'].unique())
        self.ticker2idx = {t: i for i, t in enumerate(all_tickers)}
        
        # 2. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
        dates = sorted(self.df['Date'].unique())
        if len(dates) < 2:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼šä»…æ‰¾åˆ° {len(dates)} ä¸ªæ—¥æœŸã€‚")
            
        split_idx = int(len(dates) * 0.8)
        split_idx = min(split_idx, len(dates) - 1)
        split_date = dates[split_idx]
        
        if mode == 'train':
            self.df = self.df[self.df['Date'] < split_date].copy()
        else:
            self.df = self.df[self.df['Date'] >= split_date].copy()
            
        # ã€å…³é”®ä¿®å¤ã€‘é‡ç½®ç´¢å¼•ï¼é¿å…åç»­æ»‘åŠ¨çª—å£å‡ºé”™
        self.df = self.df.reset_index(drop=True)
            
        # 3. æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨ StandardScalerï¼‰
        # ä½¿ç”¨ NumPy æ•°ç»„è¿›è¡Œæ ‡å‡†åŒ–ï¼Œé¿å… DataFrame çš„é¢å¤–å¼€é”€
        if mode == 'train':
            self.scaler = StandardScaler()
            feature_array = self.df[self.feature_cols].values
            self.df[self.feature_cols] = self.scaler.fit_transform(feature_array)
        else:
            if scaler is None:
                raise ValueError("æµ‹è¯•æ¨¡å¼ä¸‹å¿…é¡»æä¾›å·²æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ï¼ˆscalerï¼‰ã€‚")
            self.scaler = scaler
            feature_array = self.df[self.feature_cols].values
            self.df[self.feature_cols] = self.scaler.transform(feature_array)
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘è½¬æ¢ä¸º Numpy float32 ç±»å‹ï¼ˆèŠ‚çœå†…å­˜ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰
        self.data_x = self.df[self.feature_cols].values.astype(np.float32)
        self.data_y = self.df[self.target_col].values.astype(np.float32)
        self.data_vol = self.df['Volatility_20d'].values.astype(np.float32)
        
        # 4. æ„å»ºæ»‘åŠ¨çª—å£ç´¢å¼•ï¼ˆç¡®ä¿ä¸è·¨è‚¡ç¥¨æ‹¼æ¥åºåˆ—ï¼‰
        print("æ­£åœ¨æ„å»ºæ»‘åŠ¨çª—å£ç´¢å¼•...")
        self.indices = []
        
        # æŒ‰è‚¡ç¥¨ä»£ç ï¼ˆTickerï¼‰åˆ†ç»„ï¼Œä¿è¯æ¯ä¸ªåºåˆ—åªæ¥è‡ªåŒä¸€åªè‚¡ç¥¨
        groups = self.df.groupby('Ticker')
        for _, group in groups:
            group_len = len(group)
            if group_len > seq_len + pred_len:
                start_row = group.index[0]
                # é«˜æ•ˆåœ°æ·»åŠ æ‰€æœ‰æœ‰æ•ˆçš„èµ·å§‹ä½ç½®ç´¢å¼•
                for i in range(group_len - seq_len - pred_len + 1):
                    self.indices.append(start_row + i)
                    
        print(f"{mode} æ•°æ®é›†å…±ç”Ÿæˆæ ·æœ¬æ•°: {len(self.indices)}")

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        start_row = self.indices[idx]
        end_row = start_row + self.seq_len
        target_row = end_row + self.pred_len - 1
        
        # ç›´æ¥ä½¿ç”¨ NumPy åˆ‡ç‰‡ï¼ˆé€Ÿåº¦æœ€å¿«ï¼‰
        x = self.data_x[start_row : end_row]       # å½¢çŠ¶: (åºåˆ—é•¿åº¦, ç‰¹å¾æ•°)
        y = self.data_y[target_row]                # æ ‡é‡ï¼ˆå•æ—¥å¯¹æ•°æ”¶ç›Šç‡ï¼‰
        vol = self.data_vol[end_row - 1]           # æ ‡é‡ï¼ˆæœ€åä¸€æ—¥çš„æ³¢åŠ¨ç‡ï¼‰
        # ã€GNN æ”¯æŒã€‘å½“å‰æ ·æœ¬å¯¹åº”çš„è‚¡ç¥¨åœ¨å›¾ä¸­çš„èŠ‚ç‚¹ç´¢å¼•ï¼Œä¾› QL_MATCC_GNN_Model åšå›¾èšåˆ
        ticker = self.df['Ticker'].iloc[start_row]
        node_idx = self.ticker2idx.get(ticker, 0)
        
        return {
            'x': torch.from_numpy(x),                   # è½¬æ¢ä¸º PyTorch å¼ é‡
            'y': torch.tensor([y], dtype=torch.float32),
            'vol': torch.tensor([vol], dtype=torch.float32),
            'node_indices': torch.tensor(node_idx, dtype=torch.long),
        }

# ================= æµ‹è¯•ä»£ç  =================
if __name__ == "__main__":
    CSV_PATH = './data/processed/Final_Model_Data.csv'
    
    # 1. æµ‹è¯•è®­ç»ƒæ•°æ®åŠ è½½å™¨
    print(">>> æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒæ•°æ®é›†...")
    train_dataset = FinancialDataset(CSV_PATH, seq_len=30, mode='train')
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=32, 
        shuffle=True,
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘DataLoader è®¾ç½®
        num_workers=0,  # Windows ç³»ç»Ÿå»ºè®®è®¾ä¸º 0ï¼Œé¿å…å¤šè¿›ç¨‹å†²çª
        pin_memory=True if torch.cuda.is_available() else False,
        # prefetch_factor åœ¨ num_workers=0 æ—¶ä¸å¯ç”¨ï¼Œæ•…çœç•¥
    )
    
    print("\n>>> æ£€æŸ¥è®­ç»ƒæ‰¹æ¬¡æ•°æ®...")
    for batch in train_loader:
        print("è¾“å…¥å¼ é‡å½¢çŠ¶:", batch['x'].shape)
        print("ç›®æ ‡å¼ é‡å½¢çŠ¶:", batch['y'].shape)
        print("æ³¢åŠ¨ç‡å¼ é‡å½¢çŠ¶:", batch['vol'].shape)
        print("æ ·æœ¬æ•°æ®åŠ è½½æˆåŠŸï¼")
        
        if torch.cuda.is_available():
            print(f"æ•°æ®æ‰€åœ¨è®¾å¤‡: {batch['x'].device}ï¼ˆGPU å¯ç”¨ä½†å½“å‰æœªä½¿ç”¨ï¼‰")
        break

    # 2. æµ‹è¯•æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("\n>>> æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†...")
    test_dataset = FinancialDataset(CSV_PATH, seq_len=30, mode='test', scaler=train_dataset.scaler)
    
    # ã€ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºã€‘
    print(f"\n>>> æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_dataset)}")
    print(f"  - æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"  - è¾“å…¥åºåˆ—é•¿åº¦: {train_dataset.seq_len}")
    print(f"  - é¢„æµ‹æ­¥é•¿: {train_dataset.pred_len}")
    print(f"  - ç‰¹å¾ç»´åº¦æ•°: {len(train_dataset.feature_cols)}")