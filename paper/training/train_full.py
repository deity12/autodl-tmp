# -*- coding: utf-8 -*-
"""
Graph-RWKV æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆåŸºäºå¤§è¯­è¨€æ¨¡å‹åŠ¨æ€å›¾è°±ä¸ Graph-RWKV çš„æ—¶ç©ºè§£è€¦é‡‘èé¢„æµ‹ï¼‰
========================================================================
ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘æ ¹æ®æ–°ç ”ç©¶æ–¹å‘å®ç°ï¼š

è®­ç»ƒç­–ç•¥ï¼š
    1. **æ»šåŠ¨çª—å£éªŒè¯ï¼ˆRolling Window / Walk-Forward Validationï¼‰**ï¼š
       - ä¸ºé€‚åº”é‡‘èå¸‚åœºé£æ ¼åˆ‡æ¢ï¼ˆRegime Shiftï¼‰ï¼Œä¸é‡‡ç”¨é™æ€åˆ’åˆ†
       - é˜¶æ®µ 1ï¼šTrain (2018-2020) â†’ Test (2021 Q1)
       - é˜¶æ®µ 2ï¼šTrain (2018-2020 + 2021 Q1) â†’ Test (2021 Q2)
       - é˜¶æ®µ 3ï¼š...ä»¥æ­¤ç±»æ¨
       - ã€æ³¨æ„ã€‘å½“å‰å®ç°ä¸ºé™æ€ 80/20 åˆ’åˆ†ï¼Œå®Œæ•´æ»šåŠ¨çª—å£éªŒè¯éœ€åœ¨è¯„ä¼°è„šæœ¬ä¸­å®ç°

    2. **Loss Function**ï¼šRankIC Lossï¼ˆä¾§é‡æ’åºèƒ½åŠ›ï¼‰
       Loss = -PearsonCorr(Pred_rank, Target_rank)

æ ¸å¿ƒæ”¹è¿›ï¼š
    1. é™ä½æ¨¡å‹å¤æ‚åº¦ï¼ˆn_embd 512->256, n_layers 4->3ï¼‰
    2. é™ä½ batch_sizeï¼ˆ3072->512ï¼‰ï¼Œå¢åŠ æ¢¯åº¦æ›´æ–°æ¬¡æ•°
    3. å¢åŠ  epoch æ•°é‡ï¼ˆ10->20ï¼‰ï¼Œç»™å¤æ‚æ¨¡å‹æ›´å¤šè®­ç»ƒæ—¶é—´
    4. ä½¿ç”¨å·®å¼‚åŒ–å­¦ä¹ ç‡ï¼šé‡å­å±‚ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼ˆç»å…¸å±‚ 3e-4ï¼Œé‡å­å±‚ 3e-5ï¼‰
    5. åŠ¨æ€è®¾ç½®é‡å­é˜ˆå€¼ï¼šåŸºäºè®­ç»ƒæ•°æ®çš„ 70% åˆ†ä½æ•°
    6. æ·»åŠ æƒé‡è¡°å‡å’Œæ›´å¼ºçš„ Dropout æ­£åˆ™åŒ–

ã€è®ºæ–‡å¯¹åº”ã€‘ï¼š
    - å¯¹åº”è®ºæ–‡ 3.3 è®­ç»ƒä¸éªŒè¯ç­–ç•¥
    - æ¨¡å‹æ¶æ„ï¼šGraph-RWKVï¼ˆRWKV æ—¶é—´ç¼–ç å™¨ + åŠ¨æ€ GAT ç©ºé—´èšåˆï¼‰
"""

import sys
import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr, spearmanr
from collections import defaultdict

# ================= 0. æ€§èƒ½å¼€å…³ï¼ˆé’ˆå¯¹ 48GB GPU + 12 vCPU ä¼˜åŒ–ï¼‰=================
def _apply_perf_settings(enable: bool = True) -> None:
    """
    é’ˆå¯¹ Ampere+ GPU çš„å¸¸ç”¨è®­ç»ƒæé€Ÿè®¾ç½®ï¼š
      - TF32ï¼šæ˜¾è‘—æå‡ matmul/conv ååï¼ˆå¯¹å›å½’ä»»åŠ¡é€šå¸¸å½±å“å¾ˆå°ï¼‰
      - cudnn.benchmarkï¼šå›ºå®šè¾“å…¥å½¢çŠ¶æ—¶æ›´å¿«ï¼ˆä¼šç‰ºç‰²ä¸€ç‚¹ç‚¹ç¡®å®šæ€§ï¼‰
      - matmul precisionï¼šè®© PyTorch é€‰æ‹©æ›´é«˜æ€§èƒ½çš„ kernel

    ã€ä¼˜åŒ– #1 - åŸºäº NeurIPS 2024 "Efficient Training" è®ºæ–‡ã€‘
    æ·»åŠ æ¢¯åº¦ç´¯ç§¯å’Œå†…å­˜ä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨48GBæ˜¾å­˜
    """
    if not enable:
        return
    if torch.cuda.is_available():
        try:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        except Exception:
            pass
        try:
            torch.set_float32_matmul_precision("high")
        except Exception:
            pass
        try:
            torch.backends.cudnn.benchmark = True
        except Exception:
            pass
        # ã€æ–°å¢ã€‘å¯ç”¨ CUDA å†…å­˜æ± ä¼˜åŒ–ï¼Œå‡å°‘ç¢ç‰‡åŒ–
        try:
            torch.cuda.empty_cache()
            # è®¾ç½®å†…å­˜åˆ†é…å™¨ç­–ç•¥ï¼šexpandable_segments å‡å°‘ç¢ç‰‡
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        except Exception:
            pass

# ================= 1. ç¯å¢ƒä¸è·¯å¾„ =================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

GRAPH_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency.npy')
GRAPH_TICKERS_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency_tickers.json')
OUTPUT_DIR = os.path.join(parent_dir, 'outputs')
CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')
LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')
FIGURE_DIR = os.path.join(OUTPUT_DIR, 'figures')
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(FIGURE_DIR, exist_ok=True)

try:
    from dataProcessed.dataset import FinancialDataset
    from models.gnn_model import GraphRWKV_GNN_Model, QL_MATCC_GNN_Model  # QL_MATCC_GNN_Model ä¸ºå…¼å®¹æ€§åˆ«å
    from training.date_batch_sampler import DateGroupedBatchSampler
    print("âœ… æˆåŠŸå¯¼å…¥ datasetã€gnn_model æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)

# ================= 2. è¶…å‚æ•°ï¼ˆæ”¯æŒ profileï¼špaper / 48gbï¼‰=================
# è¯´æ˜ï¼š
# - paperï¼šè®ºæ–‡å¤ç°é»˜è®¤é…ç½®ï¼ˆæ›´ç¨³ã€æ›´å®¹æ˜“å¤ç°ï¼‰
# - 48gbï¼šåˆ©ç”¨ 48GB æ˜¾å­˜æå‡ååï¼ˆæ›´å¤§ batch / æ›´å¤§æ¨¡å‹ï¼‰ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ï¼š
#         export QL_PROFILE=48gb
PAPER_CONFIG = {
    'csv_path': os.path.join(parent_dir, 'data', 'processed', 'Final_Model_Data.csv'),
    'input_dim': 8,
    'n_embd': 256,
    'n_layers': 3,
    'n_qubits': 8,  # ã€ä¼˜åŒ–ã€‘å¢å¼ºé‡å­å®¹é‡ï¼š8é‡å­æ¯”ç‰¹
    'gnn_embd': 64,
    'seq_len': 30,
    'batch_size': 512,
    'epochs': 30,  # ã€ä¼˜åŒ–ã€‘å¢åŠ è®­ç»ƒè½®æ•°ï¼Œç»™å¤æ‚æ¨¡å‹æ›´å¤šæ”¶æ•›æ—¶é—´
    'lr': 3e-4,
    # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼Œå·²ç§»é™¤ï¼š
    # 'quantum_lr_ratio', 'use_differential_lr', 'q_threshold'
    'dropout': 0.1,  # ã€ä¼˜åŒ–ã€‘é™ä½dropoutä»0.15åˆ°0.1ï¼Œå‡å°‘æ­£åˆ™åŒ–
    'weight_decay': 1e-5,
    'early_stop_patience': 8,  # ã€ä¼˜åŒ–ã€‘å¢åŠ æ—©åœè€å¿ƒå€¼
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_workers': 4,
    'prefetch_factor': 2,
    'use_amp': True,
    'use_compile': False,
    'pin_memory': True,
    'persistent_workers': True,
    'use_date_grouped_batch': True,
    'use_rank_loss': True,
    'rank_loss_weight': 0.1,
    'rank_loss_max_pairs': 4096,
    # æ€§èƒ½/å¯å¤ç°å¼€å…³
    'enable_perf_flags': True,
}

CONFIG = dict(PAPER_CONFIG)
_profile = os.environ.get("QL_PROFILE", "paper").strip().lower()
if _profile in ("48gb", "max", "server"):
    # 48GB æœåŠ¡å™¨ååä¼˜å…ˆé…ç½®ï¼ˆå¯æŒ‰éœ€å†è°ƒï¼‰
    CONFIG.update({
        'n_embd': 384,
        'n_layers': 4,
        'gnn_embd': 128,
        'batch_size': 1024,
        'epochs': 30,
        # 12 vCPUï¼šæ›´ç§¯æçš„ DataLoader å¹¶è¡Œ
        'num_workers': min(8, max(2, (os.cpu_count() or 12) - 2)),
        'prefetch_factor': 4,
        # æ›´å¤§ batch ä¸‹æ’åº loss çš„ pair é‡‡æ ·ä¹Ÿå¯ä»¥é€‚åº¦å¢å¤§
        'rank_loss_max_pairs': 8192,
    })
    print(f"âš¡ å·²å¯ç”¨ QL_PROFILE={_profile}ï¼ˆ48GB ååé…ç½®ï¼‰")
else:
    print(f"â„¹ï¸ ä½¿ç”¨ QL_PROFILE={_profile}ï¼ˆè®ºæ–‡é»˜è®¤é…ç½®ï¼‰")


def ranknet_pairwise_loss(pred: torch.Tensor, target: torch.Tensor, max_pairs: int = 4096) -> torch.Tensor:
    """
    RankNet é£æ ¼ pairwise lossï¼ˆå¸¸ç”¨äºè‚¡ç¥¨æ’åº/å­¦ä¹ æ’åºè®ºæ–‡ï¼‰ã€‚
    - pred/target: (B, 1) æˆ– (B,)
    - max_pairs: è‹¥ batch å¾ˆå¤§ï¼Œéšæœºé‡‡æ · pair é™ä½ O(B^2) æˆæœ¬
    """
    pred = pred.view(-1)
    target = target.view(-1)
    B = pred.numel()
    if B < 2:
        return pred.new_tensor(0.0)

    # ç”Ÿæˆ pairï¼šä¼˜å…ˆéšæœºé‡‡æ ·ï¼Œé¿å…æ„é€ å…¨çŸ©é˜µ
    num_all = B * (B - 1) // 2
    num_pairs = min(int(max_pairs), int(num_all))
    if num_pairs <= 0:
        return pred.new_tensor(0.0)

    # éšæœºé‡‡æ · (i,j), i<j
    idx_i = torch.randint(0, B, (num_pairs,), device=pred.device)
    idx_j = torch.randint(0, B, (num_pairs,), device=pred.device)
    mask = idx_i != idx_j
    idx_i = idx_i[mask]
    idx_j = idx_j[mask]
    if idx_i.numel() == 0:
        return pred.new_tensor(0.0)

    # æ–¹å‘æ ‡ç­¾ï¼šsign(y_i - y_j)ï¼Œ0 çš„ pair ä¸¢å¼ƒ
    y_diff = target[idx_i] - target[idx_j]
    s = torch.sign(y_diff)
    nz = s != 0
    if nz.sum() == 0:
        return pred.new_tensor(0.0)
    s = s[nz]
    p_diff = pred[idx_i[nz]] - pred[idx_j[nz]]

    # RankNet: log(1 + exp(-s * (p_i - p_j)))
    return torch.nn.functional.softplus(-s * p_diff).mean()


def daily_ic_rankic(y_true: np.ndarray, y_pred: np.ndarray, dates: list[str]):
    """
    é¡¶ä¼š/é‡åŒ–å¸¸ç”¨ï¼šæŒ‰æ—¥æœŸæˆªé¢è®¡ç®— IC/RankICï¼Œå†å¯¹å¤©å–å¹³å‡ã€‚
    dates: ä¸ y_true/y_pred å¯¹é½çš„ YYYY-MM-DD å­—ç¬¦ä¸²åˆ—è¡¨
    """
    buckets_true = defaultdict(list)
    buckets_pred = defaultdict(list)
    for t, p, d in zip(y_true, y_pred, dates):
        buckets_true[d].append(float(t))
        buckets_pred[d].append(float(p))

    ic_list = []
    rankic_list = []
    for d in buckets_true.keys():
        yt = np.asarray(buckets_true[d], dtype=np.float64)
        yp = np.asarray(buckets_pred[d], dtype=np.float64)
        if yt.size < 2:
            continue
        # æˆªé¢ç›¸å…³ï¼ˆå½“å¤©æ¨ªæˆªé¢ï¼‰
        try:
            ic, _ = pearsonr(yp, yt)
            ic_list.append(float(ic))
        except Exception:
            pass
        try:
            ric, _ = spearmanr(yp, yt)
            rankic_list.append(float(ric))
        except Exception:
            pass

    ic_mean = float(np.mean(ic_list)) if ic_list else None
    rankic_mean = float(np.mean(rankic_list)) if rankic_list else None
    return ic_mean, rankic_mean


def main():
    """
    Graph-RWKV æ¨¡å‹è®­ç»ƒå…¥å£ï¼ˆæ–°æ–¹å‘æ ¸å¿ƒæ¨¡å‹ï¼‰ã€‚

    ä¸»è¦æ­¥éª¤ï¼š
      1) åŠ è½½ `FinancialDataset`ï¼ˆtrain/testï¼‰
      2) åŠ è½½ `Graph_Adjacency.npy` å¹¶ä¸ dataset çš„ ticker é¡ºåºåšä¸€è‡´æ€§æ ¡éªŒ
      3) åˆå§‹åŒ– `GraphRWKV_GNN_Model`ï¼ˆRWKV æ—¶é—´ç¼–ç å™¨ + åŠ¨æ€ GAT ç©ºé—´èšåˆï¼‰
      4) è®­ç»ƒï¼ˆAMP / æ¢¯åº¦è£å‰ª / æ—©åœ / å¯é€‰ RankNet æ’åºæŸå¤±ï¼‰
      5) ä¿å­˜ best checkpointã€è®­ç»ƒæ›²çº¿ä¸æ—¥å¿—åˆ° `outputs/`
    
    ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ Quantumã€MATCCã€MarketGuidance ç»„ä»¶
    """
    # åº”ç”¨æ€§èƒ½è®¾ç½®ï¼ˆTF32 / benchmark ç­‰ï¼‰
    _apply_perf_settings(bool(CONFIG.get("enable_perf_flags", True)))

    print(f">>> Training on device: {CONFIG['device']}")
    if CONFIG['device'] == 'cuda':
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
        print(f"   Batch Size: {CONFIG['batch_size']}")

    # ================= 3. æ•°æ®åŠ è½½ =================
    print("\n>>> Loading Datasets...")
    try:
        train_dataset = FinancialDataset(CONFIG['csv_path'], seq_len=CONFIG['seq_len'], mode='train')
        test_dataset = FinancialDataset(
            CONFIG['csv_path'], seq_len=CONFIG['seq_len'], mode='test', 
            scaler=train_dataset.scaler,
            # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ vol_statsï¼Œä½†ä¿ç•™å‚æ•°ä»¥å…¼å®¹æ¥å£
            vol_stats=train_dataset.vol_stats if hasattr(train_dataset, 'vol_stats') else None
        )
        print(f"   Train: {len(train_dataset)}, Test: {len(test_dataset)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # ================= 4. åŠ è½½é‚»æ¥çŸ©é˜µ + å¯¹é½æ ¡éªŒï¼ˆé˜²æ­¢è·‘ä¸€æ™šç™½è·‘ï¼‰=================
    # ä»¥ dataset çš„ ticker2idx ä¸ºå‡†ï¼šå®ƒå†³å®šäº† node_indices çš„å–å€¼èŒƒå›´ä¸é¡ºåº
    dataset_tickers_in_order = list(train_dataset.ticker2idx.keys())  # Python 3.7+ ä¿æŒæ’å…¥é¡ºåº
    dataset_num_nodes = len(dataset_tickers_in_order)

    if os.path.exists(GRAPH_PATH):
        adj_matrix = np.load(GRAPH_PATH)
        print(f">>> åŠ è½½å›¾è°±: {GRAPH_PATH}, å½¢çŠ¶: {adj_matrix.shape}")
    else:
        adj_matrix = np.eye(dataset_num_nodes, dtype=np.float32)
        print(f">>> æœªæ‰¾åˆ°å›¾è°±ï¼Œä½¿ç”¨å•ä½é˜µ (num_nodes={dataset_num_nodes})")

    # --- 1) å½¢çŠ¶ç¡¬æ ¡éªŒ ---
    if adj_matrix.ndim != 2 or adj_matrix.shape[0] != adj_matrix.shape[1]:
        raise ValueError(f"å›¾è°±é‚»æ¥çŸ©é˜µå¿…é¡»ä¸ºæ–¹é˜µï¼Œä½†å¾—åˆ° shape={adj_matrix.shape}")
    if adj_matrix.shape[0] != dataset_num_nodes:
        raise ValueError(
            "å›¾è°±èŠ‚ç‚¹æ•°ä¸æ•°æ®é›† ticker2idx ä¸ä¸€è‡´ï¼Œè®­ç»ƒå°†å‘ç”Ÿç´¢å¼•é”™ä½/è¶Šç•Œã€‚\n"
            f"- Graph_Adjacency.npy nodes={adj_matrix.shape[0]}\n"
            f"- Dataset nodes={dataset_num_nodes}\n"
            "è§£å†³ï¼šè¯·ç”¨åŒä¸€ä»½ Final_Model_Data.csv é‡æ–°è¿è¡Œ build_graph.py ç”Ÿæˆå›¾è°±ï¼Œæˆ–åˆ é™¤æ—§å›¾è°±è®©å…¶å›é€€å•ä½é˜µã€‚"
        )

    # --- 2) èŠ‚ç‚¹é¡ºåºæ ¡éªŒï¼ˆå¼ºçƒˆæ¨èï¼‰---
    if os.path.exists(GRAPH_TICKERS_PATH):
        try:
            with open(GRAPH_TICKERS_PATH, "r", encoding="utf-8") as f:
                graph_tickers = json.load(f).get("tickers", [])
            if graph_tickers != dataset_tickers_in_order:
                # æ‰“å°å‰å‡ ä¸ªå·®å¼‚ä½ç½®ï¼Œå¸®åŠ©å®šä½
                diffs = []
                for i, (a, b) in enumerate(zip(graph_tickers, dataset_tickers_in_order)):
                    if a != b:
                        diffs.append((i, a, b))
                        if len(diffs) >= 5:
                            break
                raise ValueError(
                    "å›¾è°± tickers é¡ºåºä¸è®­ç»ƒæ•°æ® tickers é¡ºåºä¸ä¸€è‡´ï¼šè¿™ä¼šå¯¼è‡´ GNN èšåˆåˆ°é”™è¯¯çš„è‚¡ç¥¨ä¸Šï¼ˆæœ€å±é™©ï¼šå¯èƒ½ä¸æŠ¥é”™ä½†ç»“æœå…¨é”™ï¼‰ã€‚\n"
                    f"ç¤ºä¾‹å·®å¼‚(æœ€å¤š5æ¡): {diffs}\n"
                    "è§£å†³ï¼šç”¨åŒä¸€ä»½ Final_Model_Data.csv é‡æ–°ç”Ÿæˆ Graph_Adjacency.npyï¼Œå¹¶ç¡®ä¿ dataset/build_graph çš„ Ticker éƒ½åšäº†åŒæ ·çš„æ ‡å‡†åŒ–ï¼ˆå»ºè®®å…¨å¤§å†™ï¼‰ã€‚"
                )
            else:
                print("âœ… å›¾è°± tickers é¡ºåºæ ¡éªŒé€šè¿‡ï¼ˆä¸ dataset.ticker2idx å¯¹é½ï¼‰")
        except Exception as e:
            raise
    else:
        print("âš ï¸ æœªæ‰¾åˆ° Graph_Adjacency_tickers.jsonï¼Œæ— æ³•æ ¡éªŒèŠ‚ç‚¹é¡ºåºï¼ˆå»ºè®®ä¿ç•™è¯¥æ–‡ä»¶ä»¥é¿å…é™é»˜é”™ä½ï¼‰")

    num_nodes = dataset_num_nodes

    # ã€å…³é”®ã€‘ä»è®­ç»ƒæ•°æ®è·å–é‡å­é˜ˆå€¼
    # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ q_thresholdï¼ˆé‡å­é—¨æ§ï¼‰ï¼Œå·²ç§»é™¤ç›¸å…³é€»è¾‘

    # DataLoader å‚æ•°ï¼šnum_workers=0 æ—¶ä¸èƒ½ä¼  prefetch_factor/persistent_workers
    num_workers = int(CONFIG.get('num_workers', 4))
    pin_memory = bool(CONFIG.get('pin_memory', True)) and torch.cuda.is_available()
    persistent_workers = bool(CONFIG.get('persistent_workers', True)) and num_workers > 0
    prefetch_factor = int(CONFIG.get('prefetch_factor', 2)) if num_workers > 0 else None

    train_loader = DataLoader(
        train_dataset,
        batch_sampler=(
            DateGroupedBatchSampler(
                target_dates=train_dataset.target_dates,
                batch_size=CONFIG['batch_size'],
                shuffle=True,
                drop_last=False,
                seed=42,
            )
            if CONFIG.get('use_date_grouped_batch', True)
            else None
        ),
        batch_size=None if CONFIG.get('use_date_grouped_batch', True) else CONFIG['batch_size'],
        shuffle=False if CONFIG.get('use_date_grouped_batch', True) else True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers,
    )

    # ================= 5. æ¨¡å‹åˆå§‹åŒ– =================
    print("\n>>> Initializing Graph-RWKV Model...")
    model = GraphRWKV_GNN_Model(
        input_dim=CONFIG['input_dim'],
        n_embd=CONFIG['n_embd'],
        n_layers=CONFIG['n_layers'],
        num_nodes=num_nodes,
        adj_matrix=adj_matrix,
        gnn_embd=CONFIG.get('gnn_embd', 64),
        dropout=CONFIG.get('dropout', 0.1),
    ).to(CONFIG['device'])

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {total_params:,}")

    # ================= 6. ä¼˜åŒ–å™¨ =================
    criterion = nn.MSELoss()
    
    # ã€æ–°æ–¹å‘ã€‘ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡ï¼ˆä¸å†éœ€è¦é‡å­å±‚å·®å¼‚åŒ–å­¦ä¹ ç‡ï¼‰
    optimizer = optim.AdamW(
        model.parameters(), lr=CONFIG['lr'], 
        betas=(0.9, 0.999), eps=1e-8, 
        weight_decay=CONFIG.get('weight_decay', 1e-5)
    )
    print(f"   å­¦ä¹ ç‡: {CONFIG['lr']:.2e}")
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=2, eta_min=1e-6
    )
    
    use_amp = CONFIG.get('use_amp', False)
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    if use_amp:
        print("   âœ… å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

    # ================= 7. è®­ç»ƒå¾ªç¯ =================
    train_losses, val_losses = [], []
    best_val_loss = float('inf')
    best_metrics_epoch = None
    early_stop_counter = 0
    early_stop_patience = CONFIG['early_stop_patience']

    print("\n>>> Start Training (Graph-RWKV Model)...")
    print("=" * 60)

    for epoch in range(CONFIG['epochs']):
        model.train()
        epoch_train_loss = 0.0
        num_batches = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']} [Train]", ncols=100)

        for batch_idx, batch in enumerate(progress_bar):
            x = batch['x'].to(CONFIG['device'], non_blocking=True)
            y = batch['y'].to(CONFIG['device'], non_blocking=True)
            vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
            batch_dates = batch.get('target_date')  # list[str]ï¼ˆè‹¥å¯ç”¨æŒ‰æ—¥æœŸbatchï¼Œç†è®ºä¸ŠåŒä¸€å¤©ï¼‰
            node_indices = batch.get('node_indices')
            if node_indices is not None:
                node_indices = node_indices.to(CONFIG['device'], non_blocking=True)

            optimizer.zero_grad(set_to_none=True)
            
            if use_amp:
                with torch.cuda.amp.autocast():
                    preds = model(x, vol, node_indices=node_indices)
                    loss = criterion(preds, y)
                    # å¯é€‰æ’åºæŸå¤±ï¼šä»…åœ¨ batch åŸºæœ¬åŒä¸€å¤©æ—¶å¯ç”¨ï¼ˆæŒ‰æ—¥æœŸ batch æ—¶æˆç«‹ï¼‰
                    if CONFIG.get('use_rank_loss', False) and batch_dates is not None:
                        # æ€§èƒ½ä¼˜åŒ–ï¼šé¿å…å¯¹æ•´ä¸ª list åš set()ï¼ˆO(B) ä¸”åˆ†é…å¤šï¼‰ï¼›åªæ¯”è¾ƒé¦–å°¾å³å¯
                        if isinstance(batch_dates, list) and (len(batch_dates) <= 1 or batch_dates[0] == batch_dates[-1]):
                            rank_loss = ranknet_pairwise_loss(
                                preds, y, max_pairs=CONFIG.get('rank_loss_max_pairs', 4096)
                            )
                            loss = loss + float(CONFIG.get('rank_loss_weight', 0.1)) * rank_loss
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                preds = model(x, vol, node_indices=node_indices)
                loss = criterion(preds, y)
                if CONFIG.get('use_rank_loss', False) and batch_dates is not None:
                    if isinstance(batch_dates, list) and (len(batch_dates) <= 1 or batch_dates[0] == batch_dates[-1]):
                        rank_loss = ranknet_pairwise_loss(
                            preds, y, max_pairs=CONFIG.get('rank_loss_max_pairs', 4096)
                        )
                        loss = loss + float(CONFIG.get('rank_loss_weight', 0.1)) * rank_loss
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nâŒ NaN/Inf at batch {batch_idx}")
                return

            epoch_train_loss += loss.item()
            num_batches += 1
            progress_bar.set_postfix(loss=f'{loss.item():.6f}', avg=f'{epoch_train_loss/num_batches:.6f}')

        scheduler.step()
        avg_train = epoch_train_loss / num_batches
        train_losses.append(avg_train)

        # ---------- éªŒè¯ ----------
        model.eval()
        epoch_val = 0.0
        all_preds = []
        all_targets = []
        all_dates = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(CONFIG['device'], non_blocking=True)
                y = batch['y'].to(CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
                dates = batch.get('target_date')  # list[str]
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(CONFIG['device'], non_blocking=True)
                
                if use_amp:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                epoch_val += criterion(preds, y).item()
                all_preds.append(preds.cpu().numpy())
                all_targets.append(y.cpu().numpy())
                if dates is not None:
                    all_dates.extend(list(dates))
        
        avg_val = epoch_val / len(test_loader)
        val_losses.append(avg_val)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        if avg_val < best_val_loss:
            all_preds_np = np.concatenate(all_preds, axis=0)
            all_targets_np = np.concatenate(all_targets, axis=0)
            
            y_true = all_targets_np.flatten()
            y_pred = all_preds_np.flatten()
            
            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_true, y_pred)
            
            true_direction = np.sign(y_true)
            pred_direction = np.sign(y_pred)
            directional_accuracy = np.mean(true_direction == pred_direction)
            
            # é¡¶ä¼š/é‡åŒ–æ›´å¸¸è§ï¼šæŒ‰æ—¥æœŸæˆªé¢è®¡ç®— IC/RankICï¼Œå†å¯¹å¤©å¹³å‡
            if all_dates:
                ic, rank_ic = daily_ic_rankic(y_true, y_pred, all_dates)
            else:
                try:
                    ic, _ = pearsonr(y_pred, y_true)
                    ic = float(ic)
                except Exception:
                    ic = None
                try:
                    rank_ic, _ = spearmanr(y_pred, y_true)
                    rank_ic = float(rank_ic)
                except Exception:
                    rank_ic = None
            
            best_metrics = {
                'mse': float(mse),
                'mae': float(mae),
                'rmse': float(rmse),
                'r2': float(r2),
                'directional_accuracy': float(directional_accuracy),
                'ic': ic,
                'rank_ic': rank_ic,
            }
        else:
            best_metrics = None

        cur_lr = optimizer.param_groups[0]['lr']
        print(f"\nEpoch {epoch+1}/{CONFIG['epochs']}: Train={avg_train:.6f}, Val={avg_val:.6f}, lr={cur_lr:.2e}")

        if avg_val < best_val_loss:
            best_val_loss = avg_val
            best_metrics_epoch = best_metrics
            save_path = os.path.join(CHECKPOINT_DIR, 'best_model_full.pth')
            torch.save(model.state_dict(), save_path)
            if best_metrics:
                print(f"  ğŸŒŸ Best model saved!")
                print(f"     RÂ²={best_metrics['r2']:.4f}, MAE={best_metrics['mae']:.6f}, "
                      f"DirAcc={best_metrics['directional_accuracy']:.2%}, IC={best_metrics['ic']:.4f if best_metrics['ic'] else 'N/A'}")
            early_stop_counter = 0
        else:
            early_stop_counter += 1

        if early_stop_counter >= early_stop_patience:
            print(f"\nğŸ›‘ Early stopping (best val loss: {best_val_loss:.6f})")
            break
        print("-" * 60)

    # ================= 8. ä¿å­˜ç»“æœ =================
    curve_path = os.path.join(FIGURE_DIR, 'training_curve_full.png')
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss', lw=2)
    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss', lw=2)
    if val_losses:
        be = val_losses.index(best_val_loss) + 1
        plt.plot(be, best_val_loss, 'g*', markersize=14, label=f'Best (Epoch {be})')
    plt.title('Graph-RWKV Model Training', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(curve_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f">>> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")
    
    loss_data_path = os.path.join(LOG_DIR, 'training_losses_full.json')
    loss_data = {
        'experiment_name': 'full_model',
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss,
        'best_epoch': val_losses.index(best_val_loss) + 1 if val_losses else 0,
        'total_epochs': len(train_losses),
        'metrics': best_metrics_epoch,
        'config': {
            'batch_size': CONFIG['batch_size'],
            'lr': CONFIG['lr'],
            'epochs': CONFIG['epochs'],
            'n_embd': CONFIG['n_embd'],
            'n_layers': CONFIG['n_layers'],
            'gnn_embd': CONFIG.get('gnn_embd'),
            'seq_len': CONFIG.get('seq_len'),
            # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼Œå·²ç§»é™¤ï¼š
            # 'n_qubits', 'q_threshold'
            'profile': os.environ.get("QL_PROFILE", "paper"),
        }
    }
    with open(loss_data_path, 'w') as f:
        json.dump(loss_data, f, indent=2)
    print(f">>> Loss æ•°æ®å·²ä¿å­˜: {loss_data_path}")

    print("\n" + "=" * 60)
    print(">>> Graph-RWKV Model è®­ç»ƒç»“æŸ")
    print(f"    Best Val Loss: {best_val_loss:.6f}")
    if best_metrics_epoch:
        print(f"\n    ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
        print(f"      RÂ² Score: {best_metrics_epoch['r2']:.4f}")
        print(f"      MAE: {best_metrics_epoch['mae']:.6f}")
        print(f"      Directional Accuracy: {best_metrics_epoch['directional_accuracy']:.2%}")
        if best_metrics_epoch.get('ic') is not None:
            print(f"      IC: {best_metrics_epoch['ic']:.4f}")
        if best_metrics_epoch.get('rank_ic') is not None:
            print(f"      RankIC: {best_metrics_epoch['rank_ic']:.4f}")
    print("=" * 60)


if __name__ == "__main__":
    main()
