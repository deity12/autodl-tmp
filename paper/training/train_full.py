# -*- coding: utf-8 -*-
"""
Graph-RWKV æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆåŸºäºå¤§è¯­è¨€æ¨¡å‹åŠ¨æ€å›¾è°±ä¸ Graph-RWKV çš„æ—¶ç©ºè§£è€¦é‡‘èé¢„æµ‹ï¼‰
========================================================================
ã€æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‘æ ¹æ®æ–°ç ”ç©¶æ–¹å‘å®ç°ï¼š

è®­ç»ƒç­–ç•¥ï¼š
    1. **æ»šåŠ¨çª—å£éªŒè¯ï¼ˆRolling Window / Walk-Forward Validationï¼‰**ï¼š
       - ä¸ºé€‚åº”é‡‘èå¸‚åœºé£æ ¼åˆ‡æ¢ï¼ˆRegime Shiftï¼‰ï¼Œä¸é‡‡ç”¨é™æ€åˆ’åˆ†
       - é˜¶æ®µ 1ï¼šTrain (2018-2020) â†’ Test (2021 Q1)
       - é˜¶æ®µ 2ï¼šTrain (2018-2020 + 2021 Q1) â†’ Test (2021 Q2)
       - é˜¶æ®µ 3ï¼š...ä»¥æ­¤ç±»æ¨
       - ã€æ³¨æ„ã€‘å½“å‰å®ç°ä¸ºé™æ€ 80/20 åˆ’åˆ†ï¼Œå®Œæ•´æ»šåŠ¨çª—å£éªŒè¯éœ€åœ¨è¯„ä¼°è„šæœ¬ä¸­å®ç°

    2. **Loss Function**ï¼šRankIC Lossï¼ˆä¾§é‡æ’åºèƒ½åŠ›ï¼‰
       Loss = -PearsonCorr(Pred_rank, Target_rank)

æ ¸å¿ƒæ”¹è¿›ï¼š
    1. é™ä½æ¨¡å‹å¤æ‚åº¦ï¼ˆn_embd 512->256, n_layers 4->3ï¼‰
    2. é™ä½ batch_sizeï¼ˆ3072->512ï¼‰ï¼Œå¢åŠ æ¢¯åº¦æ›´æ–°æ¬¡æ•°
    3. å¢åŠ  epoch æ•°é‡ï¼ˆ10->20ï¼‰ï¼Œç»™å¤æ‚æ¨¡å‹æ›´å¤šè®­ç»ƒæ—¶é—´
    4. ä½¿ç”¨å·®å¼‚åŒ–å­¦ä¹ ç‡ï¼šé‡å­å±‚ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼ˆç»å…¸å±‚ 3e-4ï¼Œé‡å­å±‚ 3e-5ï¼‰
    5. åŠ¨æ€è®¾ç½®é‡å­é˜ˆå€¼ï¼šåŸºäºè®­ç»ƒæ•°æ®çš„ 70% åˆ†ä½æ•°
    6. æ·»åŠ æƒé‡è¡°å‡å’Œæ›´å¼ºçš„ Dropout æ­£åˆ™åŒ–

ã€è®ºæ–‡å¯¹åº”ã€‘ï¼š
    - å¯¹åº”è®ºæ–‡ 3.3 è®­ç»ƒä¸éªŒè¯ç­–ç•¥
    - æ¨¡å‹æ¶æ„ï¼šGraph-RWKVï¼ˆRWKV æ—¶é—´ç¼–ç å™¨ + åŠ¨æ€ GAT ç©ºé—´èšåˆï¼‰
"""

import sys
import os
import json
import pickle
import shutil
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr, spearmanr
from collections import defaultdict

# ================= 0. æ€§èƒ½å¼€å…³ï¼ˆé’ˆå¯¹ 48GB GPU + 12 vCPU ä¼˜åŒ–ï¼‰=================
def _apply_perf_settings(enable: bool = True) -> None:
    """
    é’ˆå¯¹ Ampere+ GPU çš„å¸¸ç”¨è®­ç»ƒæé€Ÿè®¾ç½®ï¼š
      - TF32ï¼šæ˜¾è‘—æå‡ matmul/conv ååï¼ˆå¯¹å›å½’ä»»åŠ¡é€šå¸¸å½±å“å¾ˆå°ï¼‰
      - cudnn.benchmarkï¼šå›ºå®šè¾“å…¥å½¢çŠ¶æ—¶æ›´å¿«ï¼ˆä¼šç‰ºç‰²ä¸€ç‚¹ç‚¹ç¡®å®šæ€§ï¼‰
      - matmul precisionï¼šè®© PyTorch é€‰æ‹©æ›´é«˜æ€§èƒ½çš„ kernel

    ã€ä¼˜åŒ– #1 - åŸºäº NeurIPS 2024 "Efficient Training" è®ºæ–‡ã€‘
    æ·»åŠ æ¢¯åº¦ç´¯ç§¯å’Œå†…å­˜ä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨48GBæ˜¾å­˜
    """
    if not enable:
        return
    if torch.cuda.is_available():
        try:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        except Exception:
            pass
        try:
            torch.set_float32_matmul_precision("high")
        except Exception:
            pass
        try:
            torch.backends.cudnn.benchmark = True
        except Exception:
            pass
        # ã€æ–°å¢ã€‘å¯ç”¨ CUDA å†…å­˜æ± ä¼˜åŒ–ï¼Œå‡å°‘ç¢ç‰‡åŒ–
        try:
            torch.cuda.empty_cache()
            # è®¾ç½®å†…å­˜åˆ†é…å™¨ç­–ç•¥ï¼šexpandable_segments å‡å°‘ç¢ç‰‡
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        except Exception:
            pass


def _json_dump(path: str, obj) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False, default=str)


def _save_run_artifacts(output_dir: str, experiment_name: str, train_dataset, config: dict) -> str:
    """
    ä¿å­˜ä¸â€œæ¶ˆèå¯¹æ¯”/å¯å¤ç°â€å¼ºç›¸å…³çš„äº§ç‰©å¿«ç…§ï¼šscaler/ç‰¹å¾åˆ—/tickeré¡ºåº/é…ç½®/å›¾æ–‡ä»¶å¼•ç”¨ç­‰ã€‚
    """
    results_dir = os.path.join(output_dir, "results")
    run_dir = os.path.join(results_dir, f"artifacts_{experiment_name}")
    os.makedirs(run_dir, exist_ok=True)

    # 1) è®­ç»ƒé…ç½®å¿«ç…§ï¼ˆå®Œæ•´ CONFIGï¼‰
    _json_dump(os.path.join(run_dir, "config_full.json"), dict(config))

    # 2) ç‰¹å¾åˆ—å¿«ç…§ï¼ˆé¿å… feature_columns.json è¢«åç»­å®éªŒè¦†ç›–ï¼‰
    _json_dump(os.path.join(run_dir, "feature_columns_used.json"), list(getattr(train_dataset, "feature_cols", [])))

    # 3) èŠ‚ç‚¹é¡ºåºå¿«ç…§ï¼ˆä¸å›¾é‚»æ¥ã€node_indices å¯¹é½çš„å…³é”®ï¼‰
    ticker2idx = getattr(train_dataset, "ticker2idx", {})
    tickers_in_order = list(ticker2idx.keys()) if isinstance(ticker2idx, dict) else []
    _json_dump(os.path.join(run_dir, "tickers_in_order.json"), tickers_in_order)
    _json_dump(os.path.join(run_dir, "ticker2idx.json"), ticker2idx)

    # 4) æ•°æ®åˆ‡åˆ†ä¿¡æ¯ï¼ˆç”¨äº walk-forward / æ—¶é—´åˆ‡åˆ†å¤ç°ï¼‰
    split_info = {
        "mode": getattr(train_dataset, "mode", None),
        "start_date": getattr(train_dataset, "start_date", None),
        "end_date": getattr(train_dataset, "end_date", None),
        "split_date": getattr(train_dataset, "split_date", None),
        "csv_path": str(config.get("csv_path", "")),
        "features_path": getattr(train_dataset, "features_path", None),
        "feature_columns_path": str(config.get("feature_columns_path", "")),
    }
    _json_dump(os.path.join(run_dir, "data_split.json"), split_info)

    # 5) scalerï¼ˆæ•°å€¼æ ‡å‡†åŒ–ï¼‰
    try:
        with open(os.path.join(run_dir, "scaler.pkl"), "wb") as f:
            pickle.dump(getattr(train_dataset, "scaler", None), f)
    except Exception as e:
        _json_dump(os.path.join(run_dir, "scaler_error.json"), {"error": str(e)})

    # 6) å…³é”®è¾“å…¥æ–‡ä»¶å¼•ç”¨ï¼ˆå¯é€‰å¤åˆ¶ä¸€ä»½ï¼Œé¿å…åç»­è¢«è¦†ç›–ï¼‰
    for key, dst_name in [
        ("graph_path", "Graph_Adjacency.npy"),
        ("graph_tickers_path", "Graph_Tickers.json"),
        ("feature_columns_path", "feature_columns.json"),
    ]:
        src = str(config.get(key, "") or "")
        if src and os.path.exists(src):
            try:
                shutil.copy2(src, os.path.join(run_dir, dst_name))
            except Exception:
                pass

    return run_dir

# ================= 1. ç¯å¢ƒä¸è·¯å¾„ =================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

GRAPH_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency.npy')
GRAPH_TICKERS_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Tickers.json')
GRAPH_TICKERS_PATH_LEGACY = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency_tickers.json')
OUTPUT_DIR = os.path.join(parent_dir, 'outputs')
CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')
LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')
FIGURE_DIR = os.path.join(OUTPUT_DIR, 'figures')
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(FIGURE_DIR, exist_ok=True)

try:
    from dataProcessed.dataset import FinancialDataset
    from models.gnn_model import GraphRWKV_GNN_Model, QL_MATCC_GNN_Model  # QL_MATCC_GNN_Model ä¸ºå…¼å®¹æ€§åˆ«å
    from models.base_model import GraphRWKV_Model, RNN_Model
    from training.date_batch_sampler import DateGroupedBatchSampler
    print("âœ… æˆåŠŸå¯¼å…¥ datasetã€gnn_model æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)

# ================= 2. è¶…å‚æ•°ï¼ˆæ”¯æŒ profileï¼špaper / 48gbï¼‰=================
# è¯´æ˜ï¼š
# - paperï¼šè®ºæ–‡å¤ç°é»˜è®¤é…ç½®ï¼ˆæ›´ç¨³ã€æ›´å®¹æ˜“å¤ç°ï¼‰
# - 48gbï¼šåˆ©ç”¨ 48GB æ˜¾å­˜æå‡ååï¼ˆæ›´å¤§ batch / æ›´å¤§æ¨¡å‹ï¼‰ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ï¼š
#         export QL_PROFILE=48gb
PAPER_CONFIG = {
    'csv_path': os.path.join(parent_dir, 'data', 'processed', 'Final_Model_Data.csv'),
    'input_dim': 8,
    'n_embd': 256,
    'n_layers': 3,
    'n_qubits': 8,  # ã€ä¼˜åŒ–ã€‘å¢å¼ºé‡å­å®¹é‡ï¼š8é‡å­æ¯”ç‰¹
    'gnn_embd': 64,
    'seq_len': 30,
    'batch_size': 512,
    'epochs': 30,  # ã€ä¼˜åŒ–ã€‘å¢åŠ è®­ç»ƒè½®æ•°ï¼Œç»™å¤æ‚æ¨¡å‹æ›´å¤šæ”¶æ•›æ—¶é—´
    'lr': 3e-4,
    # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼Œå·²ç§»é™¤ï¼š
    # 'quantum_lr_ratio', 'use_differential_lr', 'q_threshold'
    'dropout': 0.1,  # ã€ä¼˜åŒ–ã€‘é™ä½dropoutä»0.15åˆ°0.1ï¼Œå‡å°‘æ­£åˆ™åŒ–
    'weight_decay': 1e-5,
    'early_stop_patience': 8,  # ã€ä¼˜åŒ–ã€‘å¢åŠ æ—©åœè€å¿ƒå€¼
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_workers': 4,
    'prefetch_factor': 2,
    'use_amp': True,
    'use_compile': False,
    'pin_memory': True,
    'persistent_workers': True,
    'use_date_grouped_batch': True,
    'use_rank_loss': True,
    'rank_loss_weight': 0.1,
    'rank_loss_max_pairs': 4096,
    'rank_loss_type': 'rankic',  # pairwise | rankic
    'rankic_tau': 1.0,
    'rankic_max_items': 256,
    'feature_columns_path': os.path.join(parent_dir, 'data', 'processed', 'feature_columns.json'),
    # æ€§èƒ½/å¯å¤ç°å¼€å…³
    'enable_perf_flags': True,
    # è¿è¡Œé…ç½®
    'output_dir': OUTPUT_DIR,
    'graph_path': GRAPH_PATH,
    'graph_tickers_path': GRAPH_TICKERS_PATH,
    'use_graph': True,
    'experiment_name': 'full',
    'checkpoint_name': 'best_model.pth',
    # Walk-forward é…ç½®
    'use_walk_forward': False,
    'walk_forward_train_start': '2018-01-01',
    'walk_forward_train_end': '2020-12-31',
    'walk_forward_test_start': '2021-01-01',
    'walk_forward_test_end': '2023-12-31',
    'walk_forward_freq': 'Q',
    # è®­ç»ƒ/è¯„ä¼°æ—¥æœŸèŒƒå›´ï¼ˆç”± walk-forward è¦†ç›–ï¼‰
    'train_start': None,
    'train_end': None,
    'test_start': None,
    'test_end': None,
    'use_date_split': True,
    # æ—¶é—´ç¼–ç å™¨ç±»å‹
    'temporal_backend': 'rwkv',  # rwkv | lstm | gru
}

CONFIG = dict(PAPER_CONFIG)
_profile = os.environ.get("QL_PROFILE", "paper").strip().lower()
if _profile in ("48gb", "max", "server"):
    # 48GB æœåŠ¡å™¨ååä¼˜å…ˆé…ç½®ï¼ˆå¯æŒ‰éœ€å†è°ƒï¼‰
    CONFIG.update({
        'n_embd': 384,
        'n_layers': 4,
        'gnn_embd': 128,
        'batch_size': 1024,
        'epochs': 30,
        # 12 vCPUï¼šæ›´ç§¯æçš„ DataLoader å¹¶è¡Œ
        'num_workers': min(8, max(2, (os.cpu_count() or 12) - 2)),
        'prefetch_factor': 4,
        # æ›´å¤§ batch ä¸‹æ’åº loss çš„ pair é‡‡æ ·ä¹Ÿå¯ä»¥é€‚åº¦å¢å¤§
        'rank_loss_max_pairs': 8192,
    })
    print(f"âš¡ å·²å¯ç”¨ QL_PROFILE={_profile}ï¼ˆ48GB ååé…ç½®ï¼‰")
else:
    print(f"â„¹ï¸ ä½¿ç”¨ QL_PROFILE={_profile}ï¼ˆè®ºæ–‡é»˜è®¤é…ç½®ï¼‰")


def ranknet_pairwise_loss(pred: torch.Tensor, target: torch.Tensor, max_pairs: int = 4096) -> torch.Tensor:
    """
    RankNet é£æ ¼ pairwise lossï¼ˆå¸¸ç”¨äºè‚¡ç¥¨æ’åº/å­¦ä¹ æ’åºè®ºæ–‡ï¼‰ã€‚
    - pred/target: (B, 1) æˆ– (B,)
    - max_pairs: è‹¥ batch å¾ˆå¤§ï¼Œéšæœºé‡‡æ · pair é™ä½ O(B^2) æˆæœ¬
    """
    pred = pred.view(-1)
    target = target.view(-1)
    B = pred.numel()
    if B < 2:
        return pred.new_tensor(0.0)

    # ç”Ÿæˆ pairï¼šä¼˜å…ˆéšæœºé‡‡æ ·ï¼Œé¿å…æ„é€ å…¨çŸ©é˜µ
    num_all = B * (B - 1) // 2
    num_pairs = min(int(max_pairs), int(num_all))
    if num_pairs <= 0:
        return pred.new_tensor(0.0)

    # éšæœºé‡‡æ · (i,j), i<j
    idx_i = torch.randint(0, B, (num_pairs,), device=pred.device)
    idx_j = torch.randint(0, B, (num_pairs,), device=pred.device)
    mask = idx_i != idx_j
    idx_i = idx_i[mask]
    idx_j = idx_j[mask]
    if idx_i.numel() == 0:
        return pred.new_tensor(0.0)

    # æ–¹å‘æ ‡ç­¾ï¼šsign(y_i - y_j)ï¼Œ0 çš„ pair ä¸¢å¼ƒ
    y_diff = target[idx_i] - target[idx_j]
    s = torch.sign(y_diff)
    nz = s != 0
    if nz.sum() == 0:
        return pred.new_tensor(0.0)
    s = s[nz]
    p_diff = pred[idx_i[nz]] - pred[idx_j[nz]]

    # RankNet: log(1 + exp(-s * (p_i - p_j)))
    return torch.nn.functional.softplus(-s * p_diff).mean()


def _soft_rank(x: torch.Tensor, tau: float = 1.0) -> torch.Tensor:
    """
    å¯å¾®è¿‘ä¼¼æ’åºï¼šrank_i = sum_j sigmoid((x_i - x_j)/tau)
    """
    x = x.view(-1)
    diff = x.unsqueeze(0) - x.unsqueeze(1)
    P = torch.sigmoid(diff / max(tau, 1e-6))
    return P.sum(dim=1)


def rankic_soft_loss(
    pred: torch.Tensor,
    target: torch.Tensor,
    tau: float = 1.0,
    max_items: int = 256,
) -> torch.Tensor:
    """
    RankIC Lossï¼ˆå¯å¾®è¿‘ä¼¼ï¼‰ï¼šå¯¹ pred/target åš soft-rank åè®¡ç®— Pearson ç›¸å…³ï¼Œæœ€å¤§åŒ–ç›¸å…³æ€§ã€‚
    """
    pred = pred.view(-1)
    target = target.view(-1)
    B = pred.numel()
    if B < 2:
        return pred.new_tensor(0.0)

    if B > max_items:
        idx = torch.randperm(B, device=pred.device)[:max_items]
        pred = pred[idx]
        target = target[idx]

    r_pred = _soft_rank(pred, tau=tau)
    r_true = _soft_rank(target, tau=tau)

    r_pred = r_pred - r_pred.mean()
    r_true = r_true - r_true.mean()
    denom = (r_pred.std() * r_true.std()).clamp_min(1e-6)
    corr = (r_pred * r_true).mean() / denom
    return -corr


def daily_ic_rankic(y_true: np.ndarray, y_pred: np.ndarray, dates: list[str]):
    """
    é¡¶ä¼š/é‡åŒ–å¸¸ç”¨ï¼šæŒ‰æ—¥æœŸæˆªé¢è®¡ç®— IC/RankICï¼Œå†å¯¹å¤©å–å¹³å‡ã€‚
    dates: ä¸ y_true/y_pred å¯¹é½çš„ YYYY-MM-DD å­—ç¬¦ä¸²åˆ—è¡¨
    """
    buckets_true = defaultdict(list)
    buckets_pred = defaultdict(list)
    for t, p, d in zip(y_true, y_pred, dates):
        buckets_true[d].append(float(t))
        buckets_pred[d].append(float(p))

    ic_list = []
    rankic_list = []
    for d in buckets_true.keys():
        yt = np.asarray(buckets_true[d], dtype=np.float64)
        yp = np.asarray(buckets_pred[d], dtype=np.float64)
        if yt.size < 2:
            continue
        # æˆªé¢ç›¸å…³ï¼ˆå½“å¤©æ¨ªæˆªé¢ï¼‰
        try:
            ic, _ = pearsonr(yp, yt)
            ic_list.append(float(ic))
        except Exception:
            pass
        try:
            ric, _ = spearmanr(yp, yt)
            rankic_list.append(float(ric))
        except Exception:
            pass

    ic_mean = float(np.mean(ic_list)) if ic_list else None
    rankic_mean = float(np.mean(rankic_list)) if rankic_list else None
    return ic_mean, rankic_mean


def _train_once():
    """
    Graph-RWKV æ¨¡å‹è®­ç»ƒå…¥å£ï¼ˆæ–°æ–¹å‘æ ¸å¿ƒæ¨¡å‹ï¼‰ã€‚

    ä¸»è¦æ­¥éª¤ï¼š
      1) åŠ è½½ `FinancialDataset`ï¼ˆtrain/testï¼‰
      2) åŠ è½½ `Graph_Adjacency.npy` å¹¶ä¸ dataset çš„ ticker é¡ºåºåšä¸€è‡´æ€§æ ¡éªŒ
      3) åˆå§‹åŒ– `GraphRWKV_GNN_Model`ï¼ˆRWKV æ—¶é—´ç¼–ç å™¨ + åŠ¨æ€ GAT ç©ºé—´èšåˆï¼‰
      4) è®­ç»ƒï¼ˆAMP / æ¢¯åº¦è£å‰ª / æ—©åœ / å¯é€‰ RankNet æ’åºæŸå¤±ï¼‰
      5) ä¿å­˜ best checkpointã€è®­ç»ƒæ›²çº¿ä¸æ—¥å¿—åˆ° `outputs/`
    
    ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ Quantumã€MATCCã€MarketGuidance ç»„ä»¶
    """
    # åº”ç”¨æ€§èƒ½è®¾ç½®ï¼ˆTF32 / benchmark ç­‰ï¼‰
    _apply_perf_settings(bool(CONFIG.get("enable_perf_flags", True)))

    # è¾“å‡ºç›®å½•é…ç½®
    output_dir = str(CONFIG.get("output_dir", OUTPUT_DIR))
    checkpoint_dir = os.path.join(output_dir, "checkpoints")
    log_dir = os.path.join(output_dir, "logs")
    figure_dir = os.path.join(output_dir, "figures")
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(figure_dir, exist_ok=True)

    experiment_name = str(CONFIG.get("experiment_name", "full"))
    checkpoint_name = str(CONFIG.get("checkpoint_name", "best_model.pth"))
    if not checkpoint_name.endswith(".pth"):
        checkpoint_name = f"{checkpoint_name}.pth"

    print(f">>> Training on device: {CONFIG['device']}")
    if CONFIG['device'] == 'cuda':
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
        print(f"   Batch Size: {CONFIG['batch_size']}")

    # ================= 3. æ•°æ®åŠ è½½ =================
    print("\n>>> Loading Datasets...")
    try:
        use_date_split = bool(CONFIG.get('use_date_split', True))
        train_dataset = FinancialDataset(
            CONFIG['csv_path'],
            seq_len=CONFIG['seq_len'],
            mode='train',
            start_date=CONFIG.get('train_start'),
            end_date=CONFIG.get('train_end'),
            use_date_split=use_date_split,
            feature_columns_path=CONFIG.get('feature_columns_path'),
        )
        test_dataset = FinancialDataset(
            CONFIG['csv_path'], seq_len=CONFIG['seq_len'], mode='test', 
            scaler=train_dataset.scaler,
            # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ vol_statsï¼Œä½†ä¿ç•™å‚æ•°ä»¥å…¼å®¹æ¥å£
            vol_stats=train_dataset.vol_stats if hasattr(train_dataset, 'vol_stats') else None,
            start_date=CONFIG.get('test_start'),
            end_date=CONFIG.get('test_end'),
            use_date_split=use_date_split,
            feature_columns_path=CONFIG.get('feature_columns_path'),
        )
        print(f"   Train: {len(train_dataset)}, Test: {len(test_dataset)}")
        CONFIG['input_dim'] = len(train_dataset.feature_cols)
        print(f"   Input Dim: {CONFIG['input_dim']} (features)")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # ================= 3.5 ä¿å­˜â€œå¯å¤ç°/æ¶ˆèå¯¹æ¯”â€äº§ç‰©å¿«ç…§ =================
    artifacts_dir = None
    try:
        artifacts_dir = _save_run_artifacts(output_dir, experiment_name, train_dataset, CONFIG)
        print(f">>> Artifacts å·²ä¿å­˜: {artifacts_dir}")
    except Exception as e:
        print(f"[WARN] Artifacts ä¿å­˜å¤±è´¥: {e}")

    # ================= 4. åŠ è½½é‚»æ¥çŸ©é˜µ + å¯¹é½æ ¡éªŒï¼ˆé˜²æ­¢è·‘ä¸€æ™šç™½è·‘ï¼‰=================
    # ä»¥ dataset çš„ ticker2idx ä¸ºå‡†ï¼šå®ƒå†³å®šäº† node_indices çš„å–å€¼èŒƒå›´ä¸é¡ºåº
    dataset_tickers_in_order = list(train_dataset.ticker2idx.keys())  # Python 3.7+ ä¿æŒæ’å…¥é¡ºåº
    dataset_num_nodes = len(dataset_tickers_in_order)

    graph_path = str(CONFIG.get("graph_path", GRAPH_PATH))
    use_graph = bool(CONFIG.get("use_graph", True))
    if use_graph:
        if os.path.exists(graph_path):
            adj_matrix = np.load(graph_path)
            print(f">>> åŠ è½½å›¾è°±: {graph_path}, å½¢çŠ¶: {adj_matrix.shape}")
        else:
            adj_matrix = np.eye(dataset_num_nodes, dtype=np.float32)
            print(f">>> æœªæ‰¾åˆ°å›¾è°±ï¼Œä½¿ç”¨å•ä½é˜µ (num_nodes={dataset_num_nodes})")

        # --- 1) å½¢çŠ¶ç¡¬æ ¡éªŒ ---
        if adj_matrix.ndim != 2 or adj_matrix.shape[0] != adj_matrix.shape[1]:
            raise ValueError(f"å›¾è°±é‚»æ¥çŸ©é˜µå¿…é¡»ä¸ºæ–¹é˜µï¼Œä½†å¾—åˆ° shape={adj_matrix.shape}")
        if adj_matrix.shape[0] != dataset_num_nodes:
            raise ValueError(
                "å›¾è°±èŠ‚ç‚¹æ•°ä¸æ•°æ®é›† ticker2idx ä¸ä¸€è‡´ï¼Œè®­ç»ƒå°†å‘ç”Ÿç´¢å¼•é”™ä½/è¶Šç•Œã€‚\n"
                f"- Graph_Adjacency.npy nodes={adj_matrix.shape[0]}\n"
                f"- Dataset nodes={dataset_num_nodes}\n"
                "è§£å†³ï¼šè¯·ç”¨åŒä¸€ä»½ Final_Model_Data.csv é‡æ–°è¿è¡Œ build_graph.py ç”Ÿæˆå›¾è°±ï¼Œæˆ–åˆ é™¤æ—§å›¾è°±è®©å…¶å›é€€å•ä½é˜µã€‚"
            )

        # --- 2) èŠ‚ç‚¹é¡ºåºæ ¡éªŒï¼ˆå¼ºçƒˆæ¨èï¼‰---
        graph_tickers_path = str(CONFIG.get("graph_tickers_path", GRAPH_TICKERS_PATH))
        if not os.path.exists(graph_tickers_path) and os.path.exists(GRAPH_TICKERS_PATH_LEGACY):
            graph_tickers_path = GRAPH_TICKERS_PATH_LEGACY

        if os.path.exists(graph_tickers_path):
            try:
                with open(graph_tickers_path, "r", encoding="utf-8") as f:
                    graph_tickers = json.load(f).get("tickers", [])
                if graph_tickers != dataset_tickers_in_order:
                    # æ‰“å°å‰å‡ ä¸ªå·®å¼‚ä½ç½®ï¼Œå¸®åŠ©å®šä½
                    diffs = []
                    for i, (a, b) in enumerate(zip(graph_tickers, dataset_tickers_in_order)):
                        if a != b:
                            diffs.append((i, a, b))
                            if len(diffs) >= 5:
                                break
                    raise ValueError(
                        "å›¾è°± tickers é¡ºåºä¸è®­ç»ƒæ•°æ® tickers é¡ºåºä¸ä¸€è‡´ï¼šè¿™ä¼šå¯¼è‡´ GNN èšåˆåˆ°é”™è¯¯çš„è‚¡ç¥¨ä¸Šï¼ˆæœ€å±é™©ï¼šå¯èƒ½ä¸æŠ¥é”™ä½†ç»“æœå…¨é”™ï¼‰ã€‚\n"
                        f"ç¤ºä¾‹å·®å¼‚(æœ€å¤š5æ¡): {diffs}\n"
                        "è§£å†³ï¼šç”¨åŒä¸€ä»½ Final_Model_Data.csv é‡æ–°ç”Ÿæˆ Graph_Adjacency.npyï¼Œå¹¶ç¡®ä¿ dataset/build_graph çš„ Ticker éƒ½åšäº†åŒæ ·çš„æ ‡å‡†åŒ–ï¼ˆå»ºè®®å…¨å¤§å†™ï¼‰ã€‚"
                    )
                else:
                    print("âœ… å›¾è°± tickers é¡ºåºæ ¡éªŒé€šè¿‡ï¼ˆä¸ dataset.ticker2idx å¯¹é½ï¼‰")
            except Exception:
                raise
        else:
            print("âš ï¸ æœªæ‰¾åˆ° Graph_Tickers.jsonï¼Œæ— æ³•æ ¡éªŒèŠ‚ç‚¹é¡ºåºï¼ˆå»ºè®®ä¿ç•™è¯¥æ–‡ä»¶ä»¥é¿å…é™é»˜é”™ä½ï¼‰")
    else:
        adj_matrix = np.eye(dataset_num_nodes, dtype=np.float32)
        print(">>> [æ¶ˆè] w/o_graph æ¨¡å¼ï¼šè·³è¿‡å›¾è°±åŠ è½½ä¸èŠ‚ç‚¹æ ¡éªŒ")

    num_nodes = dataset_num_nodes

    # ã€å…³é”®ã€‘ä»è®­ç»ƒæ•°æ®è·å–é‡å­é˜ˆå€¼
    # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ q_thresholdï¼ˆé‡å­é—¨æ§ï¼‰ï¼Œå·²ç§»é™¤ç›¸å…³é€»è¾‘

    # DataLoader å‚æ•°ï¼šnum_workers=0 æ—¶ä¸èƒ½ä¼  prefetch_factor/persistent_workers
    num_workers = int(CONFIG.get('num_workers', 4))
    pin_memory = bool(CONFIG.get('pin_memory', True)) and torch.cuda.is_available()
    persistent_workers = bool(CONFIG.get('persistent_workers', True)) and num_workers > 0
    prefetch_factor = int(CONFIG.get('prefetch_factor', 2)) if num_workers > 0 else None

    train_loader = DataLoader(
        train_dataset,
        batch_sampler=(
            DateGroupedBatchSampler(
                target_dates=train_dataset.target_dates,
                batch_size=CONFIG['batch_size'],
                shuffle=True,
                drop_last=False,
                seed=42,
            )
            if CONFIG.get('use_date_grouped_batch', True)
            else None
        ),
        batch_size=None if CONFIG.get('use_date_grouped_batch', True) else CONFIG['batch_size'],
        shuffle=False if CONFIG.get('use_date_grouped_batch', True) else True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers,
    )

    # ================= 5. æ¨¡å‹åˆå§‹åŒ– =================
    print("\n>>> Initializing Graph-RWKV Model...")
    if CONFIG.get("use_graph", True):
        model = GraphRWKV_GNN_Model(
            input_dim=CONFIG['input_dim'],
            n_embd=CONFIG['n_embd'],
            n_layers=CONFIG['n_layers'],
            num_nodes=num_nodes,
            adj_matrix=adj_matrix,
            gnn_embd=CONFIG.get('gnn_embd', 64),
            dropout=CONFIG.get('dropout', 0.1),
            temporal_backend=CONFIG.get('temporal_backend', 'rwkv'),
        ).to(CONFIG['device'])
    else:
        backend = str(CONFIG.get('temporal_backend', 'rwkv')).lower()
        if backend in ("lstm", "gru"):
            model = RNN_Model(
                input_dim=CONFIG['input_dim'],
                n_embd=CONFIG['n_embd'],
                n_layers=CONFIG['n_layers'],
                dropout=CONFIG.get('dropout', 0.1),
                rnn_type=backend,
            ).to(CONFIG['device'])
        else:
            model = GraphRWKV_Model(
                input_dim=CONFIG['input_dim'],
                n_embd=CONFIG['n_embd'],
                n_layers=CONFIG['n_layers'],
                dropout=CONFIG.get('dropout', 0.1),
            ).to(CONFIG['device'])

    if CONFIG.get("use_compile", False):
        try:
            if hasattr(model, "compile"):
                model.compile()
            else:
                model = torch.compile(model, backend="inductor")
            print("âœ… å·²å¯ç”¨ torch.compile (inductor)")
        except Exception as e:
            print(f"âš ï¸ torch.compile å¤±è´¥ï¼Œå›é€€ eager: {e}")

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {total_params:,}")

    # ================= 6. ä¼˜åŒ–å™¨ =================
    criterion = nn.MSELoss()
    
    # ã€æ–°æ–¹å‘ã€‘ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡ï¼ˆä¸å†éœ€è¦é‡å­å±‚å·®å¼‚åŒ–å­¦ä¹ ç‡ï¼‰
    optimizer = optim.AdamW(
        model.parameters(), lr=CONFIG['lr'], 
        betas=(0.9, 0.999), eps=1e-8, 
        weight_decay=CONFIG.get('weight_decay', 1e-5)
    )
    print(f"   å­¦ä¹ ç‡: {CONFIG['lr']:.2e}")
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=2, eta_min=1e-6
    )
    
    use_amp = CONFIG.get('use_amp', False)
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    if use_amp:
        print("   âœ… å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")

    # ================= 7. è®­ç»ƒå¾ªç¯ =================
    train_losses, val_losses = [], []
    best_val_loss = float('inf')
    best_metrics_epoch = None
    early_stop_counter = 0
    early_stop_patience = CONFIG['early_stop_patience']

    print("\n>>> Start Training (Graph-RWKV Model)...")
    print("=" * 60)

    for epoch in range(CONFIG['epochs']):
        model.train()
        epoch_train_loss = 0.0
        num_batches = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']} [Train]", ncols=100)

        for batch_idx, batch in enumerate(progress_bar):
            x = batch['x'].to(CONFIG['device'], non_blocking=True)
            y = batch['y'].to(CONFIG['device'], non_blocking=True)
            vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
            batch_dates = batch.get('target_date')  # list[str]ï¼ˆè‹¥å¯ç”¨æŒ‰æ—¥æœŸbatchï¼Œç†è®ºä¸ŠåŒä¸€å¤©ï¼‰
            node_indices = batch.get('node_indices')
            if node_indices is not None:
                node_indices = node_indices.to(CONFIG['device'], non_blocking=True)

            optimizer.zero_grad(set_to_none=True)
            
            if use_amp:
                with torch.cuda.amp.autocast():
                    preds = model(x, vol, node_indices=node_indices)
                    loss = criterion(preds, y)
                    # å¯é€‰æ’åºæŸå¤±ï¼šä»…åœ¨ batch åŸºæœ¬åŒä¸€å¤©æ—¶å¯ç”¨ï¼ˆæŒ‰æ—¥æœŸ batch æ—¶æˆç«‹ï¼‰
                    if CONFIG.get('use_rank_loss', False) and batch_dates is not None:
                        # æ€§èƒ½ä¼˜åŒ–ï¼šé¿å…å¯¹æ•´ä¸ª list åš set()ï¼ˆO(B) ä¸”åˆ†é…å¤šï¼‰ï¼›åªæ¯”è¾ƒé¦–å°¾å³å¯
                        if isinstance(batch_dates, list) and (len(batch_dates) <= 1 or batch_dates[0] == batch_dates[-1]):
                            if CONFIG.get('rank_loss_type', 'pairwise') == 'rankic':
                                rank_loss = rankic_soft_loss(
                                    preds,
                                    y,
                                    tau=float(CONFIG.get('rankic_tau', 1.0)),
                                    max_items=int(CONFIG.get('rankic_max_items', 256)),
                                )
                            else:
                                rank_loss = ranknet_pairwise_loss(
                                    preds, y, max_pairs=CONFIG.get('rank_loss_max_pairs', 4096)
                                )
                            loss = loss + float(CONFIG.get('rank_loss_weight', 0.1)) * rank_loss
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                preds = model(x, vol, node_indices=node_indices)
                loss = criterion(preds, y)
                if CONFIG.get('use_rank_loss', False) and batch_dates is not None:
                    if isinstance(batch_dates, list) and (len(batch_dates) <= 1 or batch_dates[0] == batch_dates[-1]):
                        if CONFIG.get('rank_loss_type', 'pairwise') == 'rankic':
                            rank_loss = rankic_soft_loss(
                                preds,
                                y,
                                tau=float(CONFIG.get('rankic_tau', 1.0)),
                                max_items=int(CONFIG.get('rankic_max_items', 256)),
                            )
                        else:
                            rank_loss = ranknet_pairwise_loss(
                                preds, y, max_pairs=CONFIG.get('rank_loss_max_pairs', 4096)
                            )
                        loss = loss + float(CONFIG.get('rank_loss_weight', 0.1)) * rank_loss
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\nâŒ NaN/Inf at batch {batch_idx}")
                return

            epoch_train_loss += loss.item()
            num_batches += 1
            progress_bar.set_postfix(loss=f'{loss.item():.6f}', avg=f'{epoch_train_loss/num_batches:.6f}')

        scheduler.step()
        avg_train = epoch_train_loss / num_batches
        train_losses.append(avg_train)

        # ---------- éªŒè¯ ----------
        model.eval()
        epoch_val = 0.0
        all_preds = []
        all_targets = []
        all_dates = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(CONFIG['device'], non_blocking=True)
                y = batch['y'].to(CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(CONFIG['device'], non_blocking=True)
                dates = batch.get('target_date')  # list[str]
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(CONFIG['device'], non_blocking=True)
                
                if use_amp:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                epoch_val += criterion(preds, y).item()
                all_preds.append(preds.cpu().numpy())
                all_targets.append(y.cpu().numpy())
                if dates is not None:
                    all_dates.extend(list(dates))
        
        avg_val = epoch_val / len(test_loader)
        val_losses.append(avg_val)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        if avg_val < best_val_loss:
            all_preds_np = np.concatenate(all_preds, axis=0)
            all_targets_np = np.concatenate(all_targets, axis=0)
            
            y_true = all_targets_np.flatten()
            y_pred = all_preds_np.flatten()
            
            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_true, y_pred)
            
            true_direction = np.sign(y_true)
            pred_direction = np.sign(y_pred)
            directional_accuracy = np.mean(true_direction == pred_direction)
            
            # é¡¶ä¼š/é‡åŒ–æ›´å¸¸è§ï¼šæŒ‰æ—¥æœŸæˆªé¢è®¡ç®— IC/RankICï¼Œå†å¯¹å¤©å¹³å‡
            if all_dates:
                ic, rank_ic = daily_ic_rankic(y_true, y_pred, all_dates)
            else:
                try:
                    ic, _ = pearsonr(y_pred, y_true)
                    ic = float(ic)
                except Exception:
                    ic = None
                try:
                    rank_ic, _ = spearmanr(y_pred, y_true)
                    rank_ic = float(rank_ic)
                except Exception:
                    rank_ic = None
            
            best_metrics = {
                'mse': float(mse),
                'mae': float(mae),
                'rmse': float(rmse),
                'r2': float(r2),
                'directional_accuracy': float(directional_accuracy),
                'ic': ic,
                'rank_ic': rank_ic,
            }
        else:
            best_metrics = None

        cur_lr = optimizer.param_groups[0]['lr']
        print(f"\nEpoch {epoch+1}/{CONFIG['epochs']}: Train={avg_train:.6f}, Val={avg_val:.6f}, lr={cur_lr:.2e}")

        if avg_val < best_val_loss:
            best_val_loss = avg_val
            best_metrics_epoch = best_metrics
            save_path = os.path.join(checkpoint_dir, checkpoint_name)
            torch.save(model.state_dict(), save_path)
            if best_metrics:
                print(f"  ğŸŒŸ Best model saved!")
                ic_str = f"{best_metrics['ic']:.4f}" if best_metrics['ic'] is not None else "N/A"
                print(f"     RÂ²={best_metrics['r2']:.4f}, MAE={best_metrics['mae']:.6f}, "
                      f"DirAcc={best_metrics['directional_accuracy']:.2%}, IC={ic_str}")
            early_stop_counter = 0
        else:
            early_stop_counter += 1

        if early_stop_counter >= early_stop_patience:
            print(f"\nğŸ›‘ Early stopping (best val loss: {best_val_loss:.6f})")
            break
        print("-" * 60)

    # ================= 8. ä¿å­˜ç»“æœ =================
    curve_path = os.path.join(figure_dir, f"training_curve_{experiment_name}.png")
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss', lw=2)
    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss', lw=2)
    if val_losses:
        be = val_losses.index(best_val_loss) + 1
        plt.plot(be, best_val_loss, 'g*', markersize=14, label=f'Best (Epoch {be})')
    plt.title('Graph-RWKV Model Training', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(curve_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f">>> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")
    
    loss_data_path = os.path.join(log_dir, f"training_losses_{experiment_name}.json")
    loss_data = {
        'experiment_name': experiment_name,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss,
        'best_epoch': val_losses.index(best_val_loss) + 1 if val_losses else 0,
        'total_epochs': len(train_losses),
        'metrics': best_metrics_epoch,
        'config': {
            'batch_size': CONFIG['batch_size'],
            'lr': CONFIG['lr'],
            'epochs': CONFIG['epochs'],
            'n_embd': CONFIG['n_embd'],
            'n_layers': CONFIG['n_layers'],
            'gnn_embd': CONFIG.get('gnn_embd'),
            'seq_len': CONFIG.get('seq_len'),
            # ã€æ³¨æ„ã€‘æ–°æ–¹å‘ä¸ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼Œå·²ç§»é™¤ï¼š
            # 'n_qubits', 'q_threshold'
            'profile': os.environ.get("QL_PROFILE", "paper"),
            'output_dir': output_dir,
            'checkpoint_name': checkpoint_name,
            'use_graph': CONFIG.get("use_graph", True),
        }
    }
    with open(loss_data_path, 'w') as f:
        json.dump(loss_data, f, indent=2)
    print(f">>> Loss æ•°æ®å·²ä¿å­˜: {loss_data_path}")

    # ä¿å­˜æ‘˜è¦ï¼Œä¾¿äºæ¶ˆèå¯¹æ¯”
    results_dir = os.path.join(output_dir, "results")
    os.makedirs(results_dir, exist_ok=True)
    summary_path = os.path.join(results_dir, f"experiment_{experiment_name}.json")
    summary = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "experiment_name": experiment_name,
        "checkpoint_path": os.path.join(checkpoint_dir, checkpoint_name),
        "artifacts_dir": artifacts_dir,
        "best_val_loss": best_val_loss,
        "best_epoch": val_losses.index(best_val_loss) + 1 if val_losses else 0,
        "metrics": best_metrics_epoch,
        "config": loss_data.get("config", {}),
        "loss_log": loss_data_path,
    }
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f">>> è®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {summary_path}")

    print("\n" + "=" * 60)
    print(">>> Graph-RWKV Model è®­ç»ƒç»“æŸ")
    print(f"    Best Val Loss: {best_val_loss:.6f}")
    if best_metrics_epoch:
        print(f"\n    ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
        print(f"      RÂ² Score: {best_metrics_epoch['r2']:.4f}")
        print(f"      MAE: {best_metrics_epoch['mae']:.6f}")
        print(f"      Directional Accuracy: {best_metrics_epoch['directional_accuracy']:.2%}")
        if best_metrics_epoch.get('ic') is not None:
            print(f"      IC: {best_metrics_epoch['ic']:.4f}")
        if best_metrics_epoch.get('rank_ic') is not None:
            print(f"      RankIC: {best_metrics_epoch['rank_ic']:.4f}")
    print("=" * 60)
    return summary


def _generate_walk_forward_windows(
    train_start: str,
    train_end: str,
    test_start: str,
    test_end: str,
    freq: str = "Q",
):
    """ç”Ÿæˆæ»šåŠ¨çª—å£ï¼šè®­ç»ƒé›†é€æ­¥æ‰©å±•ï¼Œæµ‹è¯•é›†æŒ‰å­£åº¦æ»šåŠ¨ã€‚"""
    periods = pd.period_range(test_start, test_end, freq=freq)
    windows = []
    for p in periods:
        t_start = p.start_time.normalize()
        t_end = p.end_time.normalize()
        train_end_cur = t_start - pd.Timedelta(days=1)
        label = f"{p.year}Q{p.quarter}"
        windows.append({
            "train_start": train_start,
            "train_end": train_end_cur.strftime("%Y-%m-%d"),
            "test_start": t_start.strftime("%Y-%m-%d"),
            "test_end": t_end.strftime("%Y-%m-%d"),
            "label": label,
        })
    return windows


def main():
    if CONFIG.get("use_walk_forward", False):
        base_exp = str(CONFIG.get("experiment_name", "full"))
        base_ckpt = str(CONFIG.get("checkpoint_name", "best_model.pth"))
        base_ckpt_prefix = base_ckpt.replace(".pth", "")

        windows = _generate_walk_forward_windows(
            CONFIG.get("walk_forward_train_start", "2018-01-01"),
            CONFIG.get("walk_forward_train_end", "2020-12-31"),
            CONFIG.get("walk_forward_test_start", "2021-01-01"),
            CONFIG.get("walk_forward_test_end", "2023-12-31"),
            CONFIG.get("walk_forward_freq", "Q"),
        )

        summaries = []
        for w in windows:
            CONFIG.update({
                "train_start": w["train_start"],
                "train_end": w["train_end"],
                "test_start": w["test_start"],
                "test_end": w["test_end"],
                "use_date_split": False,
                "experiment_name": f"{base_exp}_wf_{w['label']}",
                "checkpoint_name": f"{base_ckpt_prefix}_wf_{w['label']}.pth",
            })
            print(f"\n>>> Walk-Forward Window: {w['train_start']}~{w['train_end']} -> {w['test_start']}~{w['test_end']}")
            summary = _train_once()
            summary["window"] = w
            summaries.append(summary)

        results_dir = os.path.join(str(CONFIG.get("output_dir", OUTPUT_DIR)), "results")
        os.makedirs(results_dir, exist_ok=True)
        wf_path = os.path.join(results_dir, "walk_forward_summary.json")
        with open(wf_path, "w", encoding="utf-8") as f:
            json.dump(summaries, f, indent=2, ensure_ascii=False)
        print(f">>> Walk-Forward æ±‡æ€»å·²ä¿å­˜: {wf_path}")
        return

    _train_once()


if __name__ == "__main__":
    main()
