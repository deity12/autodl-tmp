# -*- coding: utf-8 -*-
"""
å…¨è‡ªåŠ¨æ¶ˆèå®éªŒè„šæœ¬ (Run Ablation Studies)
========================================================================
ä¸ paper/newpaper.md å¯¹é½çš„æ¶ˆèå®šä¹‰ï¼š
    1) w/o Quantumï¼šç§»é™¤ VQC é‡å­æ¨¡å—
    2) w/o Decompositionï¼šç§»é™¤ MATCC è¶‹åŠ¿è§£è€¦
    3) w/o Graphï¼šç§»é™¤å›¾æ³¨æ„åŠ›èšåˆ

è¯´æ˜ï¼š
    - Full Model åŸºå‡†è¯·ç›´æ¥è¿è¡Œ `training/train_full.py`ï¼ˆä¿æŒåŒä¸€å¥—è¶…å‚ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼‰
    - æœ¬è„šæœ¬åªè·‘è®ºæ–‡ä¸­åˆ—å‡ºçš„ 3 ç»„æ¶ˆèï¼Œé¿å…â€œè®ºæ–‡æ²¡å†™ä½†ä»£ç å¤šè·‘â€çš„ä¸ä¸€è‡´
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import json
from datetime import datetime
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr, spearmanr
from collections import defaultdict

def _apply_perf_settings(enable: bool = True) -> None:
    """åŒ train_full.pyï¼šå¯ç”¨ TF32 / benchmark ç­‰ä»¥æå‡ååã€‚"""
    if not enable:
        return
    if torch.cuda.is_available():
        try:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        except Exception:
            pass
        try:
            torch.set_float32_matmul_precision("high")
        except Exception:
            pass
        try:
            torch.backends.cudnn.benchmark = True
        except Exception:
            pass


# ================= æ”¹è¿› Loss å‡½æ•°ï¼šæ–¹å‘æ€§ Loss =================
class HybridLoss(nn.Module):
    """
    æ··åˆæŸå¤±å‡½æ•°ï¼šç»“åˆ MSE å’Œ Sign Loss
    - MSE Loss: æ•°å€¼ç²¾å‡†åº¦
    - Sign Loss: æ–¹å‘å‡†ç¡®åº¦ï¼ˆå¼ºè¿«æ¨¡å‹å…³æ³¨æ¶¨è·Œæ–¹å‘ï¼‰
    
    å‚æ•°:
        alpha: Sign Loss çš„æƒé‡ï¼ˆé»˜è®¤ 0.1ï¼Œå¯è°ƒï¼‰
    """
    def __init__(self, alpha=0.1):
        super().__init__()
        self.mse = nn.MSELoss()
        self.alpha = alpha

    def forward(self, pred, target):
        # 1. MSE Loss (æ•°å€¼ç²¾å‡†)
        loss_mse = self.mse(pred, target)
        
        # 2. Sign Loss (æ–¹å‘ç²¾å‡†)
        # å¦‚æœç¬¦å·ç›¸åï¼Œpred*target < 0ï¼ŒæŸå¤±å˜å¤§
        # ä½¿ç”¨ ReLU(-pred*target) ä½œä¸ºä¸€ä¸ªè½¯æƒ©ç½š
        loss_sign = torch.mean(torch.relu(-pred * target))
        
        return loss_mse + self.alpha * loss_sign

# ================= 1. ç¯å¢ƒä¸è·¯å¾„é…ç½® =================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

# è·¯å¾„é…ç½®
GRAPH_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency.npy')
GRAPH_TICKERS_PATH = os.path.join(parent_dir, 'data', 'processed', 'Graph_Adjacency_tickers.json')
CSV_PATH = os.path.join(parent_dir, 'data', 'processed', 'Final_Model_Data.csv')
OUTPUT_DIR = os.path.join(parent_dir, 'outputs')
CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')
LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')
FIGURE_DIR = os.path.join(OUTPUT_DIR, 'figures')

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
if not os.path.exists(CSV_PATH):
    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {CSV_PATH}")
    exit(1)

try:
    from dataProcessed.dataset import FinancialDataset
    from models.gnn_model import QL_MATCC_GNN_Model
    print("âœ… æˆåŠŸå¯¼å…¥åŸºç¡€æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)

# ================= 2. ç»Ÿä¸€è¶…å‚æ•° (ä¸ Full Model å®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”) =================
# ã€å…³é”®ã€‘æ¶ˆèå®éªŒå¿…é¡»ä¸ Full Model ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°ï¼Œå¦åˆ™å¯¹æ¯”æ— æ„ä¹‰
BASE_CONFIG = {
    'input_dim': 8,

    # ã€æ¨¡å‹ç»´åº¦ã€‘ä¸ train_full.py / paper/newpaper.md å®Œå…¨ä¸€è‡´
    'n_embd': 256,
    'n_layers': 3,
    'n_qubits': 8,  # ã€åŒæ­¥ã€‘ä¸train_full.pyä¿æŒä¸€è‡´
    'gnn_embd': 64,
    'seq_len': 30,

    # ã€Batch Sizeã€‘ä¸ train_full.py ä¸€è‡´
    'batch_size': 512,

    # ã€Epochã€‘ä¸ train_full.py ä¸€è‡´
    'epochs': 10,  # ã€åŒæ­¥ã€‘ä¸train_full.pyä¿æŒä¸€è‡´

    # ã€å­¦ä¹ ç‡ã€‘ä¸ train_full.py ä¸€è‡´
    'lr': 3e-4,
    'quantum_lr_ratio': 0.1,
    'use_differential_lr': True,

    # ã€é‡å­é˜ˆå€¼ã€‘å°†åœ¨è¿è¡Œæ—¶ä»æ•°æ®è·å–
    'q_threshold': None,

    # ã€æ­£åˆ™åŒ–ã€‘ä¸ train_full.py ä¸€è‡´
    'dropout': 0.1,  # ã€åŒæ­¥ã€‘ä¸train_full.pyä¿æŒä¸€è‡´
    'weight_decay': 1e-5,

    'use_hybrid_loss': False,
    'hybrid_loss_alpha': 0.1,
    'early_stop_patience': 3,  # ã€åŒæ­¥ã€‘ä¸train_full.pyä¿æŒä¸€è‡´
    
    # ã€æ•°æ®åŠ è½½/ç¡¬ä»¶ä¼˜åŒ–ã€‘ä¸ train_full.py ä¿æŒä¸€è‡´
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'num_workers': 4,
    'prefetch_factor': 2,
    'use_amp': True,
    'pin_memory': True,
    'persistent_workers': True,
}

# å…è®¸ç”¨ç¯å¢ƒå˜é‡æŠŠ DataLoader å¹¶è¡Œä¸ batch/æ¨¡å‹ç»´åº¦åˆ‡åˆ°â€œååä¼˜å…ˆâ€
# ï¼ˆæ¶ˆèå®éªŒå»ºè®®ä¸ Full Model ä½¿ç”¨åŒä¸€ profileï¼Œä¿è¯å…¬å¹³å¯¹æ¯”ï¼‰
_profile = os.environ.get("QL_PROFILE", "paper").strip().lower()
if _profile in ("48gb", "max", "server"):
    BASE_CONFIG.update({
        'n_embd': 384,
        'n_layers': 4,
        'gnn_embd': 128,
        'batch_size': 1024,
        'epochs': 30,
        'num_workers': min(8, max(2, (os.cpu_count() or 12) - 2)),
        'prefetch_factor': 4,
    })
    print(f"âš¡ å·²å¯ç”¨ QL_PROFILE={_profile}ï¼ˆ48GB ååé…ç½®ï¼›æ¶ˆèå°†ä¸ Full Model ä¿æŒä¸€è‡´ï¼‰")
else:
    print(f"â„¹ï¸ ä½¿ç”¨ QL_PROFILE={_profile}ï¼ˆè®ºæ–‡é»˜è®¤é…ç½®ï¼‰")

# ================= 3. ç»“æœå­˜å‚¨ç›®å½• =================
# æ¶ˆèå®éªŒç»“æœç»Ÿä¸€ä¿å­˜åˆ° outputs ç›®å½•
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(FIGURE_DIR, exist_ok=True)
print(f"ğŸ“ æ¶ˆèå®éªŒç»“æœå°†ä¿å­˜åˆ°: {OUTPUT_DIR}")

RESULTS = []  # å­˜å‚¨æ‰€æœ‰å®éªŒçš„ç»“æœ


def calculate_metrics(y_true, y_pred):
    """
    è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼ˆåŒ…æ‹¬é‡åŒ–é‡‘èæ ‡å‡†æŒ‡æ ‡ï¼‰
    
    å‚æ•°:
        y_true: çœŸå®å€¼ (numpy array)
        y_pred: é¢„æµ‹å€¼ (numpy array)
    
    è¿”å›:
        dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()
    
    # ========== 1. ç»Ÿè®¡è¯¯å·®ç±» ==========
    # MSE (å‡æ–¹è¯¯å·®) - è®­ç»ƒä¸»æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)
    
    # MAE (å¹³å‡ç»å¯¹è¯¯å·®) - å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
    mae = mean_absolute_error(y_true, y_pred)
    
    # RMSE (å‡æ–¹æ ¹è¯¯å·®) - ä¸ç›®æ ‡å˜é‡åŒå•ä½ï¼Œæ›´ç›´è§‚
    rmse = np.sqrt(mse)
    
    # RÂ² (å†³å®šç³»æ•°) - æ¨¡å‹æ‹Ÿåˆä¼˜åº¦
    r2 = r2_score(y_true, y_pred)
    
    # MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®) - ç›¸å¯¹è¯¯å·®
    mask = np.abs(y_true) > 1e-8
    if np.sum(mask) > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = None
    
    # ========== 2. æ–¹å‘é¢„æµ‹ç±» ==========
    # Directional Accuracy (æ–¹å‘å‡†ç¡®ç‡) - é‡‘èé¢„æµ‹æ ¸å¿ƒæŒ‡æ ‡
    # é¢„æµ‹æ¶¨è·Œæ–¹å‘çš„å‡†ç¡®ç‡ï¼Œå¯¹é‡‘èé¢„æµ‹éå¸¸é‡è¦
    true_direction = np.sign(y_true)
    pred_direction = np.sign(y_pred)
    directional_accuracy = np.mean(true_direction == pred_direction)
    
    # ========== 3. é‡åŒ–æŠ•èµ„ç±» ==========
    # IC (Information Coefficient) - ä¿¡æ¯ç³»æ•°
    # Pearson ç›¸å…³ç³»æ•°ï¼Œè¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„çº¿æ€§ç›¸å…³æ€§
    # è¿™æ˜¯é‡åŒ–é‡‘èé¢†åŸŸçš„é»„é‡‘æ ‡å‡†æŒ‡æ ‡
    try:
        ic, ic_pvalue = pearsonr(y_pred, y_true)
        ic = float(ic)
    except:
        ic = None
        ic_pvalue = None
    
    # RankIC (Rank Information Coefficient) - ç§©ä¿¡æ¯ç³»æ•°
    # Spearman ç§©ç›¸å…³ç³»æ•°ï¼Œè¡¡é‡é¢„æµ‹æ’åä¸çœŸå®æ’åçš„ç›¸å…³æ€§
    # æ¯” IC æ›´ç¨³å¥ï¼Œä¸å—å¼‚å¸¸å€¼å½±å“ï¼Œæ˜¯åŸºé‡‘å…¬å¸æœ€çœ‹é‡çš„æŒ‡æ ‡
    try:
        rank_ic, rank_ic_pvalue = spearmanr(y_pred, y_true)
        rank_ic = float(rank_ic)
    except:
        rank_ic = None
        rank_ic_pvalue = None
    
    # ä¼ ç»Ÿç›¸å…³ç³»æ•°ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
    try:
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        correlation = float(correlation)
    except:
        correlation = None
    
    return {
        # ç»Ÿè®¡è¯¯å·®ç±»
        'mse': float(mse),
        'mae': float(mae),
        'rmse': float(rmse),
        'r2': float(r2),
        'mape': float(mape) if mape is not None and not np.isnan(mape) else None,
        
        # æ–¹å‘é¢„æµ‹ç±»
        'directional_accuracy': float(directional_accuracy),
        
        # é‡åŒ–æŠ•èµ„ç±»
        'ic': ic,  # Information Coefficient (Pearson)
        'ic_pvalue': float(ic_pvalue) if ic_pvalue is not None else None,
        'rank_ic': rank_ic,  # Rank Information Coefficient (Spearman)
        'rank_ic_pvalue': float(rank_ic_pvalue) if rank_ic_pvalue is not None else None,
        
        # å…¼å®¹æ€§æŒ‡æ ‡
        'correlation': correlation,
    }


def calculate_daily_ic_rankic(y_true, y_pred, dates):
    """æŒ‰æ—¥æœŸæˆªé¢è®¡ç®— IC/RankICï¼Œå†å¯¹å¤©å–å¹³å‡ï¼ˆé¡¶ä¼š/é‡åŒ–å¸¸ç”¨å£å¾„ï¼‰ã€‚"""
    buckets_true = defaultdict(list)
    buckets_pred = defaultdict(list)
    for t, p, d in zip(np.array(y_true).flatten(), np.array(y_pred).flatten(), dates):
        buckets_true[str(d)].append(float(t))
        buckets_pred[str(d)].append(float(p))
    ic_list = []
    rankic_list = []
    for d in buckets_true.keys():
        yt = np.asarray(buckets_true[d], dtype=np.float64)
        yp = np.asarray(buckets_pred[d], dtype=np.float64)
        if yt.size < 2:
            continue
        try:
            ic, _ = pearsonr(yp, yt)
            ic_list.append(float(ic))
        except Exception:
            pass
        try:
            ric, _ = spearmanr(yp, yt)
            rankic_list.append(float(ric))
        except Exception:
            pass
    return (float(np.mean(ic_list)) if ic_list else None), (float(np.mean(rankic_list)) if rankic_list else None)


def run_experiment(exp_name, use_quantum=True, use_graph=True, use_matcc=True, use_market_guidance=True):
    """
    è¿è¡Œå•ä¸ªå®éªŒçš„æ ¸å¿ƒå‡½æ•°
    
    å‚æ•°:
        exp_name: å®éªŒåç§°ï¼ˆå¦‚ "no_quantum"ï¼‰
        use_quantum: æ˜¯å¦ä½¿ç”¨é‡å­æ¨¡å—
        use_graph: æ˜¯å¦ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œ
        use_matcc: æ˜¯å¦ä½¿ç”¨MATCCè¶‹åŠ¿è§£è€¦
        use_market_guidance: æ˜¯å¦ä½¿ç”¨å¸‚åœºå¼•å¯¼
    """
    print("\n" + "="*70)
    print(f"ğŸ§ª å¼€å§‹è¿è¡Œå®éªŒ: {exp_name}")
    print(f"   é…ç½®: Quantum={use_quantum}, Graph={use_graph}, MATCC={use_matcc}, MarketGuidance={use_market_guidance}")
    print("="*70)

    # ---------------- A. å‡†å¤‡æ•°æ® ----------------
    # æ¯æ¬¡é‡æ–°åŠ è½½æ•°æ®ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ï¼›ä»¥ dataset çš„ ticker2idx ä½œä¸ºâ€œå”¯ä¸€çœŸç›¸â€
    train_dataset = FinancialDataset(CSV_PATH, seq_len=BASE_CONFIG['seq_len'], mode='train')
    test_dataset = FinancialDataset(
        CSV_PATH, seq_len=BASE_CONFIG['seq_len'], mode='test', 
        scaler=train_dataset.scaler,
        vol_stats=train_dataset.vol_stats  # ã€æ–°å¢ã€‘ä¼ å…¥æ³¢åŠ¨ç‡ç»Ÿè®¡
    )
    dataset_tickers_in_order = list(train_dataset.ticker2idx.keys())
    dataset_num_nodes = len(dataset_tickers_in_order)

    # ---------------- B. å‡†å¤‡å›¾è°±ï¼ˆå¹¶åšå¯¹é½æ ¡éªŒï¼‰----------------
    if not use_graph:
        print("   âš ï¸ [æ¶ˆèè®¾ç½®] ç¦ç”¨å›¾ç¥ç»ç½‘ç»œ (ä½¿ç”¨å•ä½é˜µ)")
        adj_matrix = np.eye(dataset_num_nodes, dtype=np.float32)
    else:
        if GRAPH_PATH and os.path.exists(GRAPH_PATH):
            adj_matrix = np.load(GRAPH_PATH)
            print(f"   âœ… åŠ è½½å›¾è°±: {GRAPH_PATH}, å½¢çŠ¶: {adj_matrix.shape}")
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°å›¾è°±æ–‡ä»¶ï¼Œä½¿ç”¨å•ä½é˜µ")
            adj_matrix = np.eye(dataset_num_nodes, dtype=np.float32)

        # å½¢çŠ¶ç¡¬æ ¡éªŒ
        if adj_matrix.ndim != 2 or adj_matrix.shape[0] != adj_matrix.shape[1]:
            raise ValueError(f"å›¾è°±é‚»æ¥çŸ©é˜µå¿…é¡»ä¸ºæ–¹é˜µï¼Œä½†å¾—åˆ° shape={adj_matrix.shape}")
        if adj_matrix.shape[0] != dataset_num_nodes:
            raise ValueError(
                "å›¾è°±èŠ‚ç‚¹æ•°ä¸æ•°æ®é›† ticker2idx ä¸ä¸€è‡´ï¼Œæ¶ˆèå®éªŒå°†å‘ç”Ÿç´¢å¼•é”™ä½/è¶Šç•Œã€‚\n"
                f"- Graph_Adjacency.npy nodes={adj_matrix.shape[0]}\n"
                f"- Dataset nodes={dataset_num_nodes}\n"
                "è§£å†³ï¼šç”¨åŒä¸€ä»½ Final_Model_Data.csv é‡æ–°è¿è¡Œ build_graph.py ç”Ÿæˆå›¾è°±ã€‚"
            )

        # èŠ‚ç‚¹é¡ºåºæ ¡éªŒï¼ˆå¼ºçƒˆæ¨èï¼‰
        if os.path.exists(GRAPH_TICKERS_PATH):
            with open(GRAPH_TICKERS_PATH, "r", encoding="utf-8") as f:
                graph_tickers = json.load(f).get("tickers", [])
            if graph_tickers != dataset_tickers_in_order:
                diffs = []
                for i, (a, b) in enumerate(zip(graph_tickers, dataset_tickers_in_order)):
                    if a != b:
                        diffs.append((i, a, b))
                        if len(diffs) >= 5:
                            break
                raise ValueError(
                    "å›¾è°± tickers é¡ºåºä¸è®­ç»ƒæ•°æ® tickers é¡ºåºä¸ä¸€è‡´ï¼šä¼šå¯¼è‡´ GNN èšåˆåˆ°é”™è¯¯è‚¡ç¥¨ä¸Šã€‚\n"
                    f"ç¤ºä¾‹å·®å¼‚(æœ€å¤š5æ¡): {diffs}\n"
                    "è§£å†³ï¼šé‡æ–°ç”Ÿæˆå›¾è°±å¹¶ç¡®ä¿ Ticker æ ‡å‡†åŒ–ä¸€è‡´ï¼ˆå»ºè®®å…¨å¤§å†™ï¼‰ã€‚"
                )
            else:
                print("   âœ… å›¾è°± tickers é¡ºåºæ ¡éªŒé€šè¿‡ï¼ˆä¸ dataset.ticker2idx å¯¹é½ï¼‰")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ° Graph_Adjacency_tickers.jsonï¼Œæ— æ³•æ ¡éªŒèŠ‚ç‚¹é¡ºåºï¼ˆå»ºè®®ä¿ç•™è¯¥æ–‡ä»¶ï¼‰")

    num_nodes = dataset_num_nodes
    
    # ã€å…³é”®ã€‘ä»è®­ç»ƒæ•°æ®è·å–é‡å­é˜ˆå€¼
    q_threshold = BASE_CONFIG['q_threshold']
    if q_threshold is None:
        q_threshold = train_dataset.vol_stats.get('p70', 0.5)
        print(f"   >>> ä»æ•°æ®è·å–é‡å­é˜ˆå€¼: q_threshold = {q_threshold:.4f}")
    
    train_loader = DataLoader(train_dataset, batch_size=BASE_CONFIG['batch_size'], shuffle=True, 
                              num_workers=BASE_CONFIG['num_workers'], pin_memory=True, 
                              prefetch_factor=BASE_CONFIG['prefetch_factor'],
                              persistent_workers=True if BASE_CONFIG['num_workers'] > 0 else False)
    test_loader = DataLoader(test_dataset, batch_size=BASE_CONFIG['batch_size'], shuffle=False, 
                             num_workers=BASE_CONFIG['num_workers'], pin_memory=True, 
                             prefetch_factor=BASE_CONFIG['prefetch_factor'],
                             persistent_workers=True if BASE_CONFIG['num_workers'] > 0 else False)

    # ---------------- C. åˆå§‹åŒ–æ¨¡å‹ ----------------
    model = QL_MATCC_GNN_Model(
        input_dim=BASE_CONFIG['input_dim'],
        n_embd=BASE_CONFIG['n_embd'],
        n_layers=BASE_CONFIG['n_layers'],
        n_qubits=BASE_CONFIG['n_qubits'],
        num_nodes=num_nodes,
        adj_matrix=adj_matrix,
        gnn_embd=BASE_CONFIG['gnn_embd'],
        # === å…³é”®ï¼šè¿™é‡Œä¼ å…¥æ¶ˆèå¼€å…³ ===
        use_quantum=use_quantum,
        use_matcc=use_matcc,
        use_market_guidance=use_market_guidance,
        # ã€æ–°å¢ã€‘ä¼ å…¥åŠ¨æ€é‡å­é˜ˆå€¼å’Œ dropout
        q_threshold=q_threshold,
        dropout=BASE_CONFIG.get('dropout', 0.1),
    ).to(BASE_CONFIG['device'])

    # é€‰æ‹©æŸå¤±å‡½æ•°
    if BASE_CONFIG.get('use_hybrid_loss', False):
        criterion = HybridLoss(alpha=BASE_CONFIG.get('hybrid_loss_alpha', 0.1))
        print(f"   âœ… ä½¿ç”¨æ··åˆLoss (MSE + Sign Loss, alpha={BASE_CONFIG.get('hybrid_loss_alpha', 0.1)})")
    else:
        criterion = nn.MSELoss()
        print(f"   âœ… ä½¿ç”¨æ ‡å‡†MSE Loss")
    
    # åˆ†å±‚å­¦ä¹ ç‡ï¼šåŒºåˆ†é‡å­å±‚å’Œç»å…¸å±‚
    quantum_params = []
    classic_params = []
    for name, param in model.named_parameters():
        # é‡å­å±‚å‚æ•°é€šå¸¸åŒ…å« 'vqc' æˆ– 'weights'ï¼ˆæ¥è‡ª PennyLane çš„ TorchLayerï¼‰
        if 'vqc' in name.lower() or 'weights' in name.lower():
            quantum_params.append(param)
        else:
            classic_params.append(param)
    
    if BASE_CONFIG.get('use_differential_lr', True) and len(quantum_params) > 0 and use_quantum:
        quantum_lr = BASE_CONFIG['lr'] * BASE_CONFIG.get('quantum_lr_ratio', 0.1)
        optimizer = optim.AdamW([
            {'params': classic_params, 'lr': BASE_CONFIG['lr']},
            {'params': quantum_params, 'lr': quantum_lr}
        ], betas=(0.9, 0.999), eps=1e-8, weight_decay=BASE_CONFIG.get('weight_decay', 1e-5))
        print(f"   âœ… ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡: ç»å…¸å±‚={BASE_CONFIG['lr']:.2e}, é‡å­å±‚={quantum_lr:.2e}")
    else:
        optimizer = optim.AdamW(
            model.parameters(), lr=BASE_CONFIG['lr'], 
            betas=(0.9, 0.999), eps=1e-8,
            weight_decay=BASE_CONFIG.get('weight_decay', 1e-5)
        )
        if BASE_CONFIG.get('use_differential_lr', True) and len(quantum_params) == 0 and use_quantum:
            print(f"   âš ï¸ æœªæ£€æµ‹åˆ°é‡å­å±‚å‚æ•°ï¼Œä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡")
        elif not BASE_CONFIG.get('use_differential_lr', True):
            print(f"   âœ… ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡={BASE_CONFIG['lr']:.2e}")
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6)
    scaler = torch.cuda.amp.GradScaler() if BASE_CONFIG['use_amp'] else None

    # ---------------- D. è®­ç»ƒå¾ªç¯ ----------------
    train_losses, val_losses = [], []
    best_val = float('inf')
    best_epoch = 0
    counter = 0

    for epoch in range(BASE_CONFIG['epochs']):
        model.train()
        train_loss = 0.0
        steps = 0
        
        # è®­ç»ƒ
        pbar = tqdm(train_loader, desc=f"[{exp_name}] Ep {epoch+1}/{BASE_CONFIG['epochs']}", ncols=100)
        for batch in pbar:
            x = batch['x'].to(BASE_CONFIG['device'], non_blocking=True)
            y = batch['y'].to(BASE_CONFIG['device'], non_blocking=True)
            vol = batch['vol'].to(BASE_CONFIG['device'], non_blocking=True)
            node_indices = batch.get('node_indices')
            if node_indices is not None:
                node_indices = node_indices.to(BASE_CONFIG['device'], non_blocking=True)

            optimizer.zero_grad()
            if scaler:
                with torch.cuda.amp.autocast():
                    preds = model(x, vol, node_indices=node_indices)
                    loss = criterion(preds, y)
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                preds = model(x, vol, node_indices=node_indices)
                loss = criterion(preds, y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
            
            train_loss += loss.item()
            steps += 1
            pbar.set_postfix(loss=f"{loss.item():.6f}")

        avg_train = train_loss / steps
        train_losses.append(avg_train)

        # éªŒè¯ï¼ˆæ”¶é›†æ‰€æœ‰é¢„æµ‹å€¼å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰
        model.eval()
        val_loss = 0.0
        all_preds = []
        all_targets = []
        all_dates = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(BASE_CONFIG['device'], non_blocking=True)
                y = batch['y'].to(BASE_CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(BASE_CONFIG['device'], non_blocking=True)
                dates = batch.get('target_date')
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(BASE_CONFIG['device'], non_blocking=True)
                
                if scaler:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                val_loss += criterion(preds, y).item()
                
                # æ”¶é›†é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼ˆç”¨äºè®¡ç®—å®Œæ•´æŒ‡æ ‡ï¼‰
                all_preds.append(preds.cpu().numpy())
                all_targets.append(y.cpu().numpy())
                if dates is not None:
                    # DataLoader ä¼šæŠŠ str collate æˆ list[str]
                    if isinstance(dates, list):
                        all_dates.extend(dates)
        
        avg_val = val_loss / len(test_loader)
        val_losses.append(avg_val)
        
        # è®¡ç®—å®Œæ•´è¯„ä¼°æŒ‡æ ‡ï¼ˆä»…åœ¨æœ€ä½³epochæ—¶è®¡ç®—ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        metrics = None
        if avg_val < best_val:
            all_preds_np = np.concatenate(all_preds, axis=0)
            all_targets_np = np.concatenate(all_targets, axis=0)
            metrics = calculate_metrics(all_targets_np, all_preds_np)
            # è¦†ç›– ic/rank_ic ä¸ºâ€œæŒ‰æ—¥æˆªé¢å‡å€¼â€å£å¾„ï¼ˆæ›´ç¬¦åˆè‚¡ç¥¨æ’åºç±»è®ºæ–‡ï¼‰
            if all_dates:
                ic_d, ric_d = calculate_daily_ic_rankic(all_targets_np, all_preds_np, all_dates)
                metrics["ic"] = ic_d
                metrics["rank_ic"] = ric_d
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val)
        cur_lr = optimizer.param_groups[0]['lr']
        
        print(f"   Ep {epoch+1}: Train={avg_train:.6f}, Val={avg_val:.6f}, lr={cur_lr:.2e}")

        # æ—©åœå’Œä¿å­˜æœ€ä½³æ¨¡å‹
        best_metrics = None
        if avg_val < best_val:
            best_val = avg_val
            best_epoch = epoch + 1
            best_metrics = metrics  # ä¿å­˜æœ€ä½³epochçš„æŒ‡æ ‡
            counter = 0
            # ä¿å­˜æ¶ˆèå®éªŒçš„æœ€ä½³æ¨¡å‹åˆ° ablation ç›®å½•
            model_save_path = os.path.join(CHECKPOINT_DIR, f'best_model_{exp_name}.pth')
            torch.save(model.state_dict(), model_save_path)
            
            # æ‰“å°å…³é”®æŒ‡æ ‡
            if metrics:
                print(f"   ğŸ’¾ Best model saved (Ep {best_epoch}, Val={best_val:.6f})")
                print(f"      Metrics: RÂ²={metrics['r2']:.4f}, MAE={metrics['mae']:.6f}, "
                      f"DirAcc={metrics['directional_accuracy']:.2%}", end="")
                if metrics.get('ic') is not None:
                    print(f", IC={metrics['ic']:.4f}", end="")
                if metrics.get('rank_ic') is not None:
                    print(f", RankIC={metrics['rank_ic']:.4f}", end="")
                print()
            else:
                print(f"   ğŸ’¾ Best model saved (Ep {best_epoch}, Val={best_val:.6f})")
        else:
            counter += 1
            if counter >= BASE_CONFIG['early_stop_patience']:
                print("   ğŸ›‘ Early stopping")
                break

    # ---------------- E. ç”»å›¾å¹¶ä¿å­˜ ----------------
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss', lw=2)
    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Val Loss', lw=2)
    if val_losses:
        plt.plot(best_epoch, best_val, 'g*', markersize=14, label=f'Best (Ep {best_epoch})')
    plt.title(f'Ablation Study: {exp_name}', fontsize=14)
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(FIGURE_DIR, f'curve_{exp_name}.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ç»“æœå·²ä¿å­˜: {save_path}")
    
    # ---------------- F. æœ€ç»ˆè¯„ä¼°ï¼ˆåœ¨æœ€ä½³æ¨¡å‹ä¸Šè®¡ç®—å®Œæ•´æŒ‡æ ‡ï¼‰---------------
    # é‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è®¡ç®—å®Œæ•´æŒ‡æ ‡
    if best_metrics is None:
        print("   âš ï¸ é‡æ–°è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡...")
        model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, f'best_model_{exp_name}.pth')))
        model.eval()
        all_preds_final = []
        all_targets_final = []
        all_dates_final = []
        
        with torch.no_grad():
            for batch in test_loader:
                x = batch['x'].to(BASE_CONFIG['device'], non_blocking=True)
                y = batch['y'].to(BASE_CONFIG['device'], non_blocking=True)
                vol = batch['vol'].to(BASE_CONFIG['device'], non_blocking=True)
                dates = batch.get('target_date')
                node_indices = batch.get('node_indices')
                if node_indices is not None:
                    node_indices = node_indices.to(BASE_CONFIG['device'], non_blocking=True)
                
                if scaler:
                    with torch.cuda.amp.autocast():
                        preds = model(x, vol, node_indices=node_indices)
                else:
                    preds = model(x, vol, node_indices=node_indices)
                
                all_preds_final.append(preds.cpu().numpy())
                all_targets_final.append(y.cpu().numpy())
                if dates is not None and isinstance(dates, list):
                    all_dates_final.extend(dates)
        
        all_preds_final_np = np.concatenate(all_preds_final, axis=0)
        all_targets_final_np = np.concatenate(all_targets_final, axis=0)
        best_metrics = calculate_metrics(all_targets_final_np, all_preds_final_np)
        if all_dates_final:
            ic_d, ric_d = calculate_daily_ic_rankic(all_targets_final_np, all_preds_final_np, all_dates_final)
            best_metrics["ic"] = ic_d
            best_metrics["rank_ic"] = ric_d
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        if best_metrics:
            print(f"   ğŸ“Š Final Metrics: RÂ²={best_metrics['r2']:.4f}, "
                  f"DirAcc={best_metrics['directional_accuracy']:.2%}", end="")
            if best_metrics.get('ic') is not None:
                print(f", IC={best_metrics['ic']:.4f}", end="")
            if best_metrics.get('rank_ic') is not None:
                print(f", RankIC={best_metrics['rank_ic']:.4f}", end="")
            print()
    
    # ---------------- G. ä¿å­˜ Loss æ•°å€¼åˆ—è¡¨å’Œè¯„ä¼°æŒ‡æ ‡ ----------------
    loss_data_path = os.path.join(LOG_DIR, f'losses_{exp_name}.json')
    loss_data = {
        'experiment': exp_name,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val,
        'best_epoch': best_epoch,
        'total_epochs': len(train_losses),
        'use_quantum': use_quantum,
        'use_graph': use_graph,
        'use_matcc': use_matcc,
        'use_market_guidance': use_market_guidance,
        'metrics': best_metrics,  # æ·»åŠ å®Œæ•´è¯„ä¼°æŒ‡æ ‡
        'config': {
            'batch_size': BASE_CONFIG['batch_size'],
            'lr': BASE_CONFIG['lr'],
            'epochs': BASE_CONFIG['epochs'],
        }
    }
    with open(loss_data_path, 'w') as f:
        json.dump(loss_data, f, indent=2)
    print(f"âœ… Loss æ•°å€¼åˆ—è¡¨å’Œè¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {loss_data_path}")
    
    # ---------------- H. è®°å½•ç»“æœåˆ°æ±‡æ€»åˆ—è¡¨ ----------------
    final_train = train_losses[-1] if train_losses else float('nan')
    final_val = val_losses[-1] if val_losses else float('nan')
    
    result = {
        'experiment': exp_name,
        'best_val_loss': best_val,
        'best_epoch': best_epoch,
        'final_train_loss': final_train,
        'final_val_loss': final_val,
        'total_epochs': len(train_losses),
        'use_quantum': use_quantum,
        'use_graph': use_graph,
        'use_matcc': use_matcc,
        'use_market_guidance': use_market_guidance,
        # æ·»åŠ è¯„ä¼°æŒ‡æ ‡
        # ç»Ÿè®¡è¯¯å·®ç±»
        'mse': best_metrics['mse'] if best_metrics else None,
        'mae': best_metrics['mae'] if best_metrics else None,
        'rmse': best_metrics['rmse'] if best_metrics else None,
        'r2': best_metrics['r2'] if best_metrics else None,
        'mape': best_metrics['mape'] if best_metrics and best_metrics['mape'] is not None else None,
        
        # æ–¹å‘é¢„æµ‹ç±»
        'directional_accuracy': best_metrics['directional_accuracy'] if best_metrics else None,
        
        # é‡åŒ–æŠ•èµ„ç±»
        'ic': best_metrics['ic'] if best_metrics and best_metrics.get('ic') is not None else None,
        'ic_pvalue': best_metrics['ic_pvalue'] if best_metrics and best_metrics.get('ic_pvalue') is not None else None,
        'rank_ic': best_metrics['rank_ic'] if best_metrics and best_metrics.get('rank_ic') is not None else None,
        'rank_ic_pvalue': best_metrics['rank_ic_pvalue'] if best_metrics and best_metrics.get('rank_ic_pvalue') is not None else None,
        
        # å…¼å®¹æ€§æŒ‡æ ‡
        'correlation': best_metrics['correlation'] if best_metrics and best_metrics.get('correlation') is not None else None,
    }
    RESULTS.append(result)
    
    # æ¸…ç†æ˜¾å­˜
    del model, optimizer, scheduler
    if scaler:
        del scaler
    torch.cuda.empty_cache()


def save_summary_results():
    """ä¿å­˜æ±‡æ€»ç»“æœåˆ°CSVå’Œç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
    if not RESULTS:
        print("âš ï¸ æ²¡æœ‰å®éªŒç»“æœå¯ä¿å­˜")
        return
    
    # ä¿å­˜CSVåˆ° ablation ç›®å½•ï¼ˆåŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼‰
    df_results = pd.DataFrame(RESULTS)
    
    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼Œè®©é‡è¦æŒ‡æ ‡åœ¨å‰é¢
    column_order = [
        'experiment', 'best_val_loss', 
        # ç»Ÿè®¡è¯¯å·®ç±»
        'mse', 'mae', 'rmse', 'r2', 'mape',
        # æ–¹å‘é¢„æµ‹ç±»
        'directional_accuracy',
        # é‡åŒ–æŠ•èµ„ç±»
        'ic', 'ic_pvalue', 'rank_ic', 'rank_ic_pvalue',
        # å…¶ä»–
        'correlation', 'best_epoch', 'final_train_loss', 'final_val_loss', 
        'total_epochs', 'use_quantum', 'use_graph', 'use_matcc', 'use_market_guidance'
    ]
    # åªä¿ç•™å­˜åœ¨çš„åˆ—
    column_order = [col for col in column_order if col in df_results.columns]
    df_results = df_results[column_order]
    
    csv_path = os.path.join(OUTPUT_DIR, 'results', 'ablation_results_summary.csv')
    os.makedirs(os.path.join(OUTPUT_DIR, 'results'), exist_ok=True)
    df_results.to_csv(csv_path, index=False, float_format='%.6f')
    print(f"\nâœ… æ±‡æ€»ç»“æœå·²ä¿å­˜: {csv_path}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å·¦å›¾ï¼šæœ€ä½³éªŒè¯æŸå¤±å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    exp_names = [r['experiment'] for r in RESULTS]
    best_vals = [r['best_val_loss'] for r in RESULTS]
    
    # ä¸º Full Model ä½¿ç”¨ç‰¹æ®Šé¢œè‰²ï¼ˆé‡‘è‰²ï¼‰ï¼Œå…¶ä»–ç”¨ä¸åŒé¢œè‰²
    colors = []
    ablation_colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    ablation_idx = 0
    
    for name in exp_names:
        if name == 'full_model':
            colors.append('#FFD700')  # é‡‘è‰²ï¼Œçªå‡ºåŸºå‡†çº¿
        else:
            # ä¸ºæ¶ˆèå®éªŒåˆ†é…ä¸åŒé¢œè‰²
            if ablation_idx < len(ablation_colors):
                colors.append(ablation_colors[ablation_idx])
                ablation_idx += 1
            else:
                colors.append('#1f77b4')  # é»˜è®¤è“è‰²
    
    axes[0].bar(exp_names, best_vals, color=colors)
    axes[0].set_ylabel('Best Val Loss (MSE)', fontsize=12)
    axes[0].set_title('Ablation Study: Best Validation Loss', fontsize=14, fontweight='bold')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(best_vals):
        axes[0].text(i, v, f'{v:.6f}', ha='center', va='bottom', fontsize=9)
    
    # å³å›¾ï¼šæœ€ä½³éªŒè¯æŸå¤±å¯¹æ¯”ï¼ˆæ›´æ¸…æ™°çš„å±•ç¤ºï¼‰
    # æŒ‰æŸå¤±å€¼æ’åºï¼ŒFull Model åº”è¯¥æ˜¯æœ€ä½çš„
    sorted_results = sorted(RESULTS, key=lambda x: x['best_val_loss'])
    sorted_names = [r['experiment'] for r in sorted_results]
    sorted_vals = [r['best_val_loss'] for r in sorted_results]
    
    # ä¸º Full Model ä½¿ç”¨ç‰¹æ®Šé¢œè‰²
    bar_colors = []
    ablation_colors_h = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    ablation_idx_h = 0
    
    for name in sorted_names:
        if name == 'full_model':
            bar_colors.append('#FFD700')  # é‡‘è‰²
        else:
            # ä¸ºæ¶ˆèå®éªŒåˆ†é…ä¸åŒé¢œè‰²
            if ablation_idx_h < len(ablation_colors_h):
                bar_colors.append(ablation_colors_h[ablation_idx_h])
                ablation_idx_h += 1
            else:
                bar_colors.append('#1f77b4')  # é»˜è®¤è“è‰²
    
    axes[1].barh(sorted_names, sorted_vals, color=bar_colors)
    axes[1].set_xlabel('Best Val Loss (MSE)', fontsize=12)
    axes[1].set_title('Ablation Study: Loss Ranking', fontsize=14, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='x')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(sorted_vals):
        axes[1].text(v, i, f' {v:.6f}', va='center', fontsize=9)
    
    axes[1].set_ylabel('Experiment', fontsize=12)
    # è¿™é‡Œä¸éœ€è¦ legendï¼ˆbar æ²¡æœ‰ labelï¼Œä¼šå¯¼è‡´ç©º legend/è­¦å‘Šï¼‰
    
    plt.tight_layout()
    comparison_path = os.path.join(FIGURE_DIR, 'ablation_results_comparison.png')
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {comparison_path}")
    
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print("\n" + "="*70)
    print("ğŸ“Š æ¶ˆèå®éªŒæ±‡æ€»ç»“æœ")
    print("="*70)
    print(df_results.to_string(index=False))
    print("="*70)


# ================= 4. ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    _apply_perf_settings(True)
    print("ğŸš€ å¯åŠ¨æ¶ˆèå®éªŒæµç¨‹...")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {current_dir}")
    print(f"ğŸ“ ç»“æœç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {CSV_PATH}")
    print(f"ğŸ”— å›¾è°±æ–‡ä»¶: {GRAPH_PATH if GRAPH_PATH else 'æœªæ‰¾åˆ°ï¼ˆå°†ä½¿ç”¨å•ä½é˜µï¼‰'}")
    print(f"ğŸ’» è®¾å¤‡: {BASE_CONFIG['device']}")
    
    print("\n" + "="*70)
    print("ğŸ“Œ è¯´æ˜ï¼šFull Model åŸºå‡†è¯·è¿è¡Œ training/train_full.py")
    print("   æœ¬è„šæœ¬ä»…è¿è¡Œ 3 ç»„æ¶ˆèå®éªŒï¼ˆä¸ paper/newpaper.md å¯¹é½ï¼‰")
    print("="*70)
    
    start_time = datetime.now()
    
    # å®éªŒ 1: w/o Quantum
    run_experiment(exp_name="no_quantum", 
                   use_quantum=False, use_graph=True, use_matcc=True, use_market_guidance=True)
    
    # å®éªŒ 2: w/o Decomposition (MATCC)
    run_experiment(exp_name="no_matcc", 
                   use_quantum=True, use_graph=True, use_matcc=False, use_market_guidance=True)
    
    # å®éªŒ 3: w/o Graph
    run_experiment(exp_name="no_graph", 
                   use_quantum=True, use_graph=False, use_matcc=True, use_market_guidance=True)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    save_summary_results()
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60  # åˆ†é’Ÿ
    
    print("\n" + "="*70)
    print("ğŸ‰ æ¶ˆèå®éªŒå·²å®Œæˆï¼ï¼ˆå…± 3 ç»„æ¶ˆèå®éªŒï¼‰")
    print(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f} åˆ†é’Ÿ")
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - ablation/curve_no_quantum.png")
    print("   - ablation/curve_no_matcc.png")
    print("   - ablation/curve_no_graph.png")
    print("   - ablation/losses_*.json (æ¯ä¸ªå®éªŒçš„ Loss æ•°å€¼åˆ—è¡¨ï¼Œå…±3ä¸ª)")
    print("   - ablation/ablation_results_summary.csv")
    print("   - ablation/ablation_results_comparison.png")
    print("   - ablation/best_model_*.pth (æ¯ä¸ªå®éªŒçš„æœ€ä½³æ¨¡å‹ï¼Œå…±3ä¸ª)")
    print("\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼šå¯¹æ¯” Full Model è¯·æŸ¥çœ‹ outputs/logs ä¸ outputs/results")
    print("="*70)
