# 数据处理流程技术报告

## 1. 研究背景与数据概况

本研究采用2019-2023年期间的美股股票数据，包括：
- **个股价格数据**：数千个CSV文件，每个文件对应一只股票的历史价格数据
- **新闻文本数据**：23GB的NASDQ外部新闻数据（nasdaq_exteral_data.csv）
- **市场指数数据**：S&P 500指数数据

数据处理的目标是将多源异构数据整合为可供深度学习模型使用的统一格式数据集。

---

## 2. 数据处理架构

数据处理采用ETL（Extract, Transform, Load）架构，分为三个阶段：

### 2.1 数据提取与初步清洗（ETL阶段）

**文件：`etl.py`**

#### 2.1.1 股价数据合并流程

**问题**：原始数据以单文件形式存储，每只股票对应一个CSV文件，共数千个文件，存在以下问题：
- 列名大小写不一致（Date/date）
- 时间范围需要统一过滤
- 数据格式不统一

**解决方案**：
```python
# 核心处理流程
1. 批量文件扫描与过滤（排除系统隐藏文件）
2. 列名标准化：统一转换为小写后规范化
3. 股票代码提取：从文件名解析Ticker，统一大写格式
4. 时间窗口过滤：仅保留2019-01-01至2023-12-31期间数据
5. 列选择与验证：动态检查必需列的存在性，防止列缺失错误
6. 批量合并：使用pd.concat()高效合并所有DataFrame
```

**输出**：`Stock_Prices.csv`，包含列：`Date, Ticker, Open, Close, High, Low, Volume`

**技术要点**：
- 使用`tqdm`显示处理进度，提升用户体验
- 异常文件自动跳过，保证处理鲁棒性
- 内存高效：逐个文件处理，避免一次性加载所有文件

#### 2.1.2 新闻数据流式处理

**问题**：23GB超大文件无法一次性加载至内存，需要高效处理策略。

**解决方案：分块读取（Chunk Processing）**
```python
# 核心技术：pandas.read_csv(chunksize=100000)
1. 分块读取：每次处理100,000行，避免内存溢出
2. 时间过滤：在每个chunk中应用日期范围过滤
3. 股票代码过滤：仅保留在股价数据中存在的股票新闻（保证数据对齐）
4. 列降维：仅保留关键列（Date, Ticker, Article_title→Headline, Publisher），丢弃全文内容
5. 增量合并：逐块处理后将清洗结果合并
```

**输出**：`Stock_News.csv`

**技术优势**：
- 内存复杂度：O(chunksize)而非O(file_size)，可处理任意大小文件
- 时间复杂度：线性扫描，O(n)
- 数据一致性：仅保留有对应股价数据的股票新闻，避免后续对齐问题

---

### 2.2 数据对齐与特征工程（数据融合阶段）

**文件：`align.py`**

#### 2.2.1 多源数据对齐策略

**数据源**：
1. 股价数据（`Stock_Prices.csv`）
2. 市场指数数据（`SP500_Index.csv`）
3. 新闻文本数据（`Stock_News.csv`）

**对齐方法**：
```python
# 采用多级Left Join策略
1. 主表：股价数据（确保每个股票的每个交易日都被保留）
2. 第一级合并：按Date字段合并市场指数（Left Join）
3. 第二级合并：按(Date, Ticker)字段合并新闻数据（Left Join）
```

**新闻数据聚合**：
- **问题**：同一股票同一日期可能存在多条新闻
- **解决方案**：使用`groupby(['Date', 'Ticker']).apply()`将多条新闻标题合并，用`|`分隔
- **结果**：每个`(Date, Ticker)`组合对应一条聚合后的新闻文本

#### 2.2.2 缺失值处理策略

```python
# 缺失值填充策略
1. 市场指数缺失：前向填充（ffill）+ 首行缺失补0
   - 原因：市场数据在节假日可能缺失，但个股数据存在
   - 方法：使用前一交易日数据填充
2. 新闻缺失：填充空字符串（fillna("")）
   - 原因：不是所有交易日都有新闻
   - 方法：保留日期记录，新闻字段为空字符串
```

#### 2.2.3 波动率特征计算

**方法**：20日滚动历史波动率（Rolling Historical Volatility）

```python
# 计算流程
1. 对数收益率计算：
   Log_Ret_t = log(Close_t / Close_{t-1}) = log(Close_t) - log(Close_{t-1})
   
2. 20日滚动波动率：
   Volatility_20d_t = std(Log_Ret_{t-19}, ..., Log_Ret_t)
   
# 技术实现：使用pandas groupby().transform()进行向量化计算
merged_df['Log_Close'] = np.log(merged_df['Close'])
merged_df['Log_Ret'] = merged_df.groupby('Ticker')['Log_Close'].diff()
merged_df['Volatility_20d'] = merged_df.groupby('Ticker')['Log_Ret'].transform(
    lambda x: x.rolling(20).std()
)
```

**技术优势**：
- 向量化计算：比循环快数个数量级
- 分组计算：确保不跨股票计算波动率（每个Ticker独立计算）
- 缺失值处理：首20行无足够历史数据，填充0

**输出**：`Final_Model_Data.csv`，包含完整特征集

---

### 2.3 数据预处理与模型输入准备（训练前处理）

**文件：`dataset.py`**

#### 2.3.1 异常值检测与处理（鲁棒性保证）

**问题识别**：通过`check_data.py`发现数据中存在极端异常值（如Log_Ret=14.92，对应收益率>3000%），可能导致：
- 梯度爆炸
- 模型训练不稳定
- 数值溢出

**解决方案：多层防护机制**
```python
# 1. 裁剪（Clipping）：限制收益率范围
Log_Ret = Log_Ret.clip(-1.0, 1.0)  # 限制在±100%内

# 2. 无穷大值处理
df = df.replace([np.inf, -np.inf], np.nan)

# 3. 缺失值处理：前向填充 + 删除
df[numeric_cols] = df[numeric_cols].fillna(method='ffill')
df = df.dropna(subset=numeric_cols)

# 4. 波动率上界限制
Volatility_20d = Volatility_20d.clip(0, 5.0)  # 防止后续计算溢出
```

#### 2.3.2 训练/测试集划分策略

```python
# 时间序列数据划分：按时间顺序划分，避免数据泄露
split_idx = int(len(dates) * 0.8)
split_date = dates[split_idx]

train_data = df[df['Date'] < split_date]
test_data = df[df['Date'] >= split_date]
```

**关键原则**：时间序列数据必须按时间划分，不能随机划分，否则会产生"未来信息泄露"问题。

#### 2.3.3 特征标准化

**方法**：Z-score标准化（StandardScaler）

```python
# 公式：z = (x - μ) / σ
# μ: 训练集均值，σ: 训练集标准差

# 训练集：fit + transform（计算统计量并转换）
scaler = StandardScaler()
train_features = scaler.fit_transform(train_df[feature_cols])

# 测试集：仅transform（使用训练集统计量）
test_features = scaler.transform(test_df[feature_cols])
```

**重要性**：
- 消除量纲影响（股价与成交量的尺度差异）
- 加速模型收敛
- 测试集必须使用训练集的统计量，保证分布一致性

#### 2.3.4 滑动窗口序列构建

**方法**：时间序列滑动窗口

```python
# 参数设置
seq_len = 30  # 输入序列长度：过去30个交易日
pred_len = 1  # 预测步长：预测未来1个交易日

# 构建方法
for each_ticker:
    for i in range(len(ticker_data) - seq_len - pred_len + 1):
        样本 = {
            'x': ticker_data[i : i+seq_len],      # 输入：30天特征
            'y': ticker_data[i+seq_len+pred_len-1], # 目标：第31天收益率
            'vol': ticker_data[i+seq_len-1]['Volatility_20d']  # 波动率特征
        }
```

**关键约束**：每个序列必须来自同一只股票（不跨Ticker），保证时间连续性和金融逻辑一致性。

---

## 3. 数据处理质量评估

### 3.1 处理方法的合理性分析

**优点**：
1. ✅ **内存效率**：分块处理23GB大文件，避免了内存溢出问题
2. ✅ **数据一致性**：通过Ticker过滤确保股价与新闻数据对齐
3. ✅ **鲁棒性**：多层异常值检测与处理机制
4. ✅ **时间序列规范性**：按时间划分训练/测试集，避免数据泄露
5. ✅ **特征工程合理**：对数收益率和滚动波动率是金融领域的标准特征

**潜在问题与建议**：
1. ⚠️ **新闻聚合方式**：目前使用简单的`|`分隔符合并多条新闻，可能丢失时序信息。建议：考虑使用时间戳或保留新闻数量特征。
2. ⚠️ **波动率计算窗口**：固定20日窗口可能不适应不同市场周期。建议：可尝试多尺度波动率（5日、10日、20日）。
3. ⚠️ **缺失值处理**：前向填充可能引入滞后偏差。建议：可考虑插值方法或引入缺失值指示特征。
4. ⚠️ **异常值阈值**：±100%的裁剪阈值较为宽松。建议：基于统计方法（如3σ原则）动态确定阈值。

### 3.2 代码质量评估

**优点**：
- 代码结构清晰，模块化设计
- 注释详细，便于维护
- 错误处理完善（try-except、文件存在性检查）
- 性能优化（向量化计算、类型转换float32）

**可改进点**：
- 可引入配置文件管理路径和参数
- 可增加日志系统替代print语句
- 可添加数据质量报告的自动生成

---

## 4. 数据流程总结

```
原始数据
  ├─ 数千个股价CSV文件
  └─ 23GB新闻文件 + S&P500指数
  
    ↓ [etl.py]
    
中间数据
  ├─ Stock_Prices.csv (合并后的股价数据)
  └─ Stock_News.csv (清洗后的新闻数据)
  
    ↓ [align.py]
    
最终数据
  └─ Final_Model_Data.csv (对齐后的完整数据集)
  
    ↓ [dataset.py]
    
模型输入
  └─ 标准化后的滑动窗口序列（可直接用于训练）
```

---

## 5. 结论

本数据处理流程采用了标准的ETL架构，通过分块处理、数据对齐、特征工程和异常值处理等步骤，成功将多源异构数据整合为适合深度学习模型的高质量数据集。整体流程设计合理，具有良好的可扩展性和鲁棒性，能够支撑后续的模型训练与评估工作。
