# 本地模型使用说明：Qwen2.5-14B-Instruct（免费方案）

## 🎯 为什么推荐本地模型？

**Qwen2.5-14B-Instruct** 是阿里云通义千问的开源模型，**完全免费**，无需 API Key，无调用次数限制！

### 优势对比

| 特性 | 本地模型（Qwen） | API 模式（DeepSeek/OpenAI） |
|------|-----------------|---------------------------|
| **费用** | ✅ 完全免费 | ❌ 按量付费（约 1.5-5 元/万条） |
| **速度** | ⚠️ 较慢（需本地推理） | ✅ 快（云端并行） |
| **限制** | ✅ 无 API 限流 | ⚠️ 有速率限制 |
| **隐私** | ✅ 数据不上传 | ⚠️ 数据发送到云端 |
| **显存需求** | ⚠️ 需要 GPU（推荐）或大内存 | ✅ 无需本地资源 |

**建议**：
- **预算有限 / 大量数据**：用本地模型（Qwen）
- **快速测试 / 少量数据**：用 API 模式（DeepSeek）

---

## 📦 安装步骤

### 1. 安装依赖

```bash
pip install transformers torch accelerate
```

**注意**：
- 如果有 GPU，确保安装了对应版本的 `torch`（带 CUDA 支持）
- 检查 GPU：`python -c "import torch; print(torch.cuda.is_available())"`

### 2. 下载模型（首次运行自动下载）

模型会自动从 HuggingFace 下载，约 **14GB**，首次运行需要时间。

**如果下载慢**，可以：
- 使用镜像站（如 `huggingface.co` 的国内镜像）
- 或手动下载后放到 `~/.cache/huggingface/hub/` 目录

### 3. 启用本地模型

**方法一：环境变量（推荐）**
```bash
# Linux/Mac
export USE_LOCAL_MODEL=true
python dataProcessed/build_graph.py

# Windows
$env:USE_LOCAL_MODEL="true"
python dataProcessed/build_graph.py
```

**方法二：修改代码**
编辑 `dataProcessed/build_graph.py` 第 44 行：
```python
USE_LOCAL_MODEL = True  # 改为 True
```

**方法三：修改主函数调用**
编辑 `dataProcessed/build_graph.py` 最后一行（第 217 行）：
```python
build_dynamic_graph(use_llm=True, max_news=2000)  # use_llm=True
```

---

## 🚀 运行

### 完整流程

```bash
# 1. 设置环境变量
export USE_LOCAL_MODEL=true

# 2. 运行建图（会自动下载模型，首次较慢）
python dataProcessed/build_graph.py
```

**首次运行**：
- 会下载模型（约 14GB，需等待）
- 下载完成后会加载到内存/显存
- 之后运行会直接使用已下载的模型

**后续运行**：
- 直接使用已下载的模型，无需重新下载

---

## 💻 硬件要求

### 推荐配置（GPU）

- **GPU**：至少 16GB 显存（如 RTX 3090, A100）
- **内存**：32GB+（如果 GPU 不够，会用到内存）
- **存储**：至少 20GB 空闲（存放模型）

### 最低配置（CPU）

- **内存**：32GB+（模型会加载到内存）
- **存储**：至少 20GB 空闲
- **速度**：较慢（每条新闻约 1-3 秒）

**注意**：CPU 模式非常慢，建议用 GPU 或改用 API 模式。

---

## ⚙️ 优化建议

### 1. 使用 GPU 加速

确保 PyTorch 安装了 CUDA 版本：
```bash
# 检查 GPU 是否可用
python -c "import torch; print(torch.cuda.is_available())"

# 如果显示 False，需要重新安装带 CUDA 的 PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. 批量处理（提高效率）

本地模型推理较慢，建议：
- 先用少量数据测试（如 100 条）
- 确认效果后再处理全部
- 可以后台运行，不占用终端

### 3. 使用量化模型（节省显存）

如果显存不足，可以使用 4-bit 量化版本：
```python
# 在 build_graph.py 中修改加载方式
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
local_model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_NAME,
    quantization_config=quantization_config,
    ...
)
```

**需要额外安装**：`pip install bitsandbytes`

---

## ✅ 验证是否生效

运行 `build_graph.py` 时，如果看到：
```
[加载中] 正在加载本地模型 Qwen/Qwen2.5-14B-Instruct（首次运行需下载，约 14GB）...
[OK] 本地模型加载完成（设备: cuda）。
```

说明本地模型已启用。如果看到：
```
[WARN] 未配置本地模型或 API_KEY，将使用规则模拟。
```

说明还在用规则模式，检查环境变量或代码设置。

---

## 🆚 三种模式对比

| 模式 | 费用 | 速度 | 准确度 | 适用场景 |
|------|------|------|--------|---------|
| **规则模式** | 免费 | 快（秒级） | 低 | 快速测试 |
| **本地模型** | 免费 | 慢（分钟到小时） | 高 | 大量数据、无预算 |
| **API 模式** | 付费 | 快（分钟级） | 高 | 快速实验、有预算 |

**建议选择**：
- 开发/测试：规则模式
- 正式实验（预算有限）：本地模型
- 正式实验（预算充足）：API 模式

---

## 🐛 常见问题

### Q1: 下载模型很慢怎么办？

**A**: 使用 HuggingFace 镜像或手动下载：
```bash
# 设置镜像（Linux/Mac）
export HF_ENDPOINT=https://hf-mirror.com

# 或手动下载到 ~/.cache/huggingface/hub/
```

### Q2: 显存不足（OOM）怎么办？

**A**: 
1. 使用量化模型（4-bit）
2. 减少 `max_news` 参数，分批处理
3. 改用 API 模式

### Q3: CPU 模式太慢怎么办？

**A**: 
- 建议使用 GPU 服务器
- 或改用 API 模式（DeepSeek 性价比高）

### Q4: 模型加载失败？

**A**: 
- 检查网络连接（需要访问 HuggingFace）
- 检查磁盘空间（至少 20GB）
- 检查 transformers 版本：`pip install --upgrade transformers`

---

## 📝 总结

**本地模型（Qwen）适合**：
- ✅ 预算有限
- ✅ 数据量大（10万+ 条新闻）
- ✅ 对隐私要求高
- ✅ 有 GPU 服务器

**API 模式适合**：
- ✅ 快速实验
- ✅ 数据量中等（< 5万条）
- ✅ 无 GPU 或显存不足
- ✅ 有预算（约 1.5 元/万条）

**规则模式适合**：
- ✅ 快速测试代码流程
- ✅ 无预算且无 GPU
- ✅ 对准确度要求不高
